88232b4cee,skip,Untopiced,Fix ENABLE_RECORD_KERNEL_FUNCTION_DTYPE build (#65370)
04a5e45aeb,jit,Untopiced,[PyTorch] Compare Type pointers before calling operator== in EqualNode (#65352)
6d7bc34b67,cpp_frontend,Untopiced,Make new_empty/new_ones/new_zeros/new_full respect subclass (#65169)
c0eb266c02,jit,Untopiced,[Static runtime] Micro-optimization pass on GetLivenessMap (#65175)
00b732e98b,nn_frontend,Untopiced,Remove orphan from cuDNN persistent note (#65160)
9afdf017dc,releng,Untopiced,Add force_on_cpu test to win cuda10.2 on GHA (#65094)
28bfdbb066,python_frontend,Untopiced,OpInfo for `nn.functional.batch_norm` (#63218)
c245632e2e,distributed,Untopiced,Use higher timeout for TSAN tests. (#65391)
cd813f16bf,distributed,Untopiced,Add functional api for `nn.Module` (#61447)
8bab468943,autograd_frontend,Untopiced,Reduce test size for max_pool (#65336)
1fec9cd76b,cpp_frontend,Untopiced,"[Fixed] Enable Half, BFloat16, and Complex dtypes for coo-coo sparse matmul [CUDA] (#59980)"
73c4bfc30a,onnx,Untopiced,[ONNX] Add log10 symbolic (#63418) (#64374)
a1216061c1,dataloader_frontend,Untopiced,[DataPipe] Fix deepcopy filehandle for Mapper and in-place modification for IterableWrapper (#65220)
5aa33770f5,releng,Untopiced,.circleci: Remove Windows workflows from Circle (#64959)
27faa7a560,onnx,Untopiced,[ONNX] Support torch.isfinite export (#64759)
158b8bdc8a,distributed,Untopiced,Cleaning up DDP SPMD in reducer.cpp (#64113)
3f5f721ab3,quantization,Untopiced,Pass through allow-list from prepare_qat into propagate_qconfig_ to allow custom mapping and custom QAT module (#65119)
64d3c7388f,distributed,Untopiced,[RELAND] Enable ncclAvg for reductions (#62835)
feefc94573,fx,Untopiced,[fx2trt] Use itensor_to_tensor_meta to track the TensorMeta info for ITensor node (#65427)
127c9402d0,jit,Untopiced,"Revert ""Revert D30752939: [pytorch][PR] nvfuser update"" (#65137)"
e331beef20,releng,Untopiced,Delete code coverage jobs from CI (#65362)
2c7df1360a,releng,Untopiced,Bump torch version to 1.11 (#65435)
3c6d9fd124,skip,Untopiced,Eagerly populate python_error::what() when TORCH_SHOW_CPP_STACKTRACES=1 (#65376)
9d17f21e46,dataloader_frontend,Untopiced,Added PandasDataframeWrapper (#65411)
f90d9b48db,complex_frontend,Untopiced,test_neg_view: preseve sign of sample input (#63010)
9324d682fd,autograd_frontend,Untopiced,Fix autograd engine checks and update InputMetadata (#65235)
bcc6e3ab5e,dispatcher,Untopiced,add python API to print all operators that have kernels registered to a particular DispatchKey (#63575)
762c2276e1,fx,Untopiced,feed model merge net lower benchmark (#65191)
508845f2b5,quantization,Untopiced,[quant] AO migration of the `torch/quantization/quantize_fx.py` and `torch/quantization/fx/*` (#65033)
7c9a278895,skip,Untopiced,fix trailing newlines (#65474)
152f0236c3,autograd_frontend,Untopiced,Revert D31082693: Fix autograd engine checks and update InputMetadata
b3ec88f41f,skip,Untopiced,ugh (#65477)
db4b68b3ac,skip,Untopiced,"Back out ""Eagerly populate python_error::what() when TORCH_SHOW_CPP_STACKTRACES=1"""
158393e1a1,autograd_frontend,Untopiced,Fix autograd engine checks and update InputMetadata (#65235)
11ca641491,nn_frontend,Untopiced,[docs] Add images to some activation functions (#65415)
70a545b21e,python_frontend,Untopiced,Add Tensor._make_wrapper_subclass (#65340)
a0dea074b2,distributed,Untopiced,Remove `.data` from benchmarks and tensorboard (#65389)
0fe86ac6c6,python_frontend,Untopiced,Fix torch.any documentation (#65310)
f24bd43375,distributed,Untopiced,Changing type and name of local_used_maps to reflect that it is only one map (#65380)
72fc53ff27,releng,Untopiced,.github: Add timeout for test step (#65486)
cbc3db8274,performance_as_product,Untopiced,Create test for builtin tensorrt module in torch deploy (#63819)
ce5981e431,distributed,Untopiced,[DDP] Custom buffer reduction (#64513)
5739f77775,distributed,Untopiced,[DDP] Refactor and remove sync_params (#64514)
2898ef7549,cuda,Untopiced,Minor ScanKernels.cu cleanup (#65350)
1c20b98b4b,jit,Untopiced,[iOS][CoreML] Check backend availability at runtime. (#65315)
31584d065e,jit,Untopiced,[Static Runtime] Added NNC implementation for signed log1p kernel. (#65387)
b80bdcc73b,nn_frontend,Untopiced,Add register_module alias to nn.Module (#65174)
32f0387ee8,nn_frontend,Untopiced,Bug in CosineAnnealingWarmRestarts in optim/lr_scheduler.py (#64758)
0eaf081018,jit,Untopiced,[JIT] canonicalize aten::rsub (#65014)
2f67579864,distributed,Untopiced,[ddp] use named_params and named_buffers explicitly (#65181)
fccaa4a3c8,fx,Untopiced,[fx2trt] fix transpose unittest (#65481)
0ca1102609,fx,Untopiced,[fx2trt] fuse permute + matmul using a pass instead of hardcoding it as a leaf module (#65482)
228141f939,quantization,Untopiced,[pytorch] more informative error msg from fbgemm embedding spmdm call (#65186)
5525e9a591,jit,Untopiced,Lock unpickling of source ranges
14949d2922,fx,Untopiced,Add nn.function.hardsigmoid in acc_tracer (#65422)
7e7be526c9,cpp_frontend,Untopiced,Add TORCH_SHOW_CPP_STACKTRACES to Contributing.md (#64052)
9668a8a82d,dataloader_frontend,Untopiced,[DataPipe] Update Docstrings for Tar and ZipArchiveReader (#65500)
2a4d5e4c6d,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
a012216b96,cpp_frontend,Untopiced,[nn] Fold : no batch dim (#64909)
767a104698,quantization,Untopiced,[quant] change observer FQNs generated in prepare step (#65420)
14307f7a56,jit,Untopiced,[Static Runtime] Added logging to dump the model graphs (#65509)
eb949464d6,jit,Untopiced,[PyTorch] Fix missing moves in SchemaParser::parseArgument (#64839)
97b535dabd,dispatcher,Untopiced,[PyTorch] add fastToString for infer_schema (#64823)
da166d4f12,distributed,Untopiced,Add a timeout argument to RPC shutdown() (#65425)
c731be8066,dispatcher,Untopiced,[BE] Use `DispatchKeySet` in `check_base_legacy_new` (#65535)
cac7c1a192,releng,Untopiced,[ci] remove auto-label-rocm workflow (#65558)
7e772e7685,nn_frontend,Untopiced,Update link to tutorial on defining NN modules (#65534)
65fbd2c12b,releng,Untopiced,[ci] do not continue through error on trunk (#65503)
1f0f246fe2,skip,Untopiced,Automated submodule update: FBGEMM (#65360)
36485d36b6,nn_frontend,Untopiced,Docathon: Add docs for nn.functional.*d_max_pool (#63264)
71704349aa,distributed,Untopiced,[DDP] Allow await of custom buffer reduction in backward (#64515)
d07b2cb4ec,fx,Untopiced,[fx2trt] update the oss fx2trt exmaple (#65544)
c015cbabf9,caffe2,Untopiced,[codemod][fbcode/caffe2] Apply all buildifier fixes
2a0208f4dc,distributed,Untopiced,fixed comments referring fairscale master branch (#65531)
f850d7ef2e,releng,Untopiced,[CoreML][OSS] Add Simulator tests (#65076)
7dbc21bc2b,skip,Untopiced,Enable CUPTI for kineto by default on windows. (#62175)
a2e631b874,releng,Untopiced,Windows GHA: Only upload artifacts if prev steps pass (#65561)
b77c979102,quantization,Untopiced,[quant][fx][graphmode] Make FixedQParam ops work for dtypes other than quint8 (#65484)
d78b3909e8,distributed,Untopiced,Explicitly destory ProcessGroup in allgather_coalesced_async test (#65513)
c73f0e457e,cpp_frontend,Untopiced,Tensor and device is_hpu methods (#65408)
963ae25e41,cpp_frontend,Untopiced,Migrate THCAtomics to ATen (#65470)
8c7caedbb8,performance_as_product,Untopiced,avoid re-allocation of view_shape for every tensor in `torch.meshgrid` (#62908)
bc02255d5e,releng,Untopiced,Revert D30721329: [pytorch][PR] Enable CUPTI for kineto by default on windows.
9965163751,onnx,Untopiced,[ONNX] Add supplementary tests and description for custom_opsets param from torch.onnx.export() function. (#62085) (#64372)
f83250fd4e,mobile,Untopiced,Revert logic in `mobile/type_parser.cpp` (#65556)
01720d6a23,jit,Untopiced,[JIT] constant object compilation unit ref fix (#65442)
cc4db35205,jit,Untopiced,[TensorExpr] Break circular dependency of shared pointers in MemDependencyChecker. (#65600)
ca66698202,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
d85e12a5bf,python_frontend,Untopiced,add OpInfo for `torch.argsort` (#65454)
fd24e1b61f,python_frontend,Untopiced,add `OpInfo` for `torch.repeat_interleave` (#65455)
b003b2a9c0,jit,Untopiced,[Static Runtime] Add record functions (#64698)
ef9e560796,jit,Untopiced,[Static Runtime] Add aten::remainder out variant (#64967)
e742839f0e,autograd_frontend,Untopiced,Fix autograd engine test in python_dispatch (#65567)
b858993c97,autograd_frontend,Untopiced,Fix engine check for case where grad is a subclass (#65568)
5b2a7eaa03,caffe2,Untopiced,[codemod][fbcode/caffe2] Apply all buildifier fixes
f3587f6bfa,cuda,Untopiced,Remove THC ScalarConvert (#65471)
1682722152,jit,Untopiced,keep output type after calling SubgraphRewriter (#65453)
c3b09e977a,fx,Untopiced,[fx2trt] Refresh execution context across save/load for TRTModule. (#65592)
760aefd34d,jit,Untopiced,Fix nullptr addition (#65548)
7f25c3e666,distributed,Untopiced,Update distributed.rst to show that CUDA send/recv on GPU is supported (#65601)
eca4f14b6c,cpp_frontend,Untopiced,[PyTorch] Add C10_ prefix to MPARK_* macros in variant.h (#65589)
6b60884f12,skip,Untopiced,Enable CUPTI for kineto by default on windows (#65608)
57e066e188,skip,Untopiced,TST Adds gradcheck and gradgradcheck to module info (#64444)
640a615150,mobile,Untopiced,[easy] [PyTorch Edge] Remove double pragma once directive in the generated code (#65620)
71d1d16acb,jit,Untopiced,Moving the constant parameter check to a more common file (#64251)
a839cec0ad,releng,Untopiced,.github: GHA retry docker pull (#65103)
15724bcc03,jit,Untopiced,[TensorExpr] Re-enable a float16 test. (#65632)
af7238f214,releng,Untopiced,Rocm4.3.1 nightly (#65624)
ece25c453f,jit,Untopiced,[PyTorch] Store Argument::alias_info_ on the heap (#64824)
ab5eb56983,mobile,Untopiced,add qmul (#63913)
1de8976e85,mobile,Untopiced,Add quantized::convtranspose2d (#63914)
cda2ee9016,fx,Untopiced,Add nn.function.hardswish in acc_tracer (#65590)
399214efd6,releng,Untopiced,Revert D31172530: [pytorch][PR] Enable CUPTI for kineto by default on windows
0256c3be50,jit,Untopiced,[TensorExpr] Delete dtype_ field from Let - it should use its var's dtype. (#65634)
146817c9d0,package,Untopiced,Add all_paths utility function (#65602)
4fe66d962d,skip,Untopiced,[Codemod][FBSourceBlackLinter] Daily `arc lint --take BLACK`
f70147b426,distributed,Untopiced,[BE] Enable ZeRO test on windows (#65385)
1c8949c51a,distributed,Untopiced,[BE] Run Zero test internally (#65519)
cd80bbe5f5,dataloader_frontend,Untopiced,Bug fixes in dataframe_wrapper (#65629)
facff2ec65,distributed,Untopiced,Update ProcessGroup collective C++ APIs to be non-pure virtual functions (#64943)
aa5d2a8d86,releng,Untopiced,Remove confusing SHARD_NUMBER resetting logic (#65701)
10d0dbc6d9,cpp_frontend,Untopiced,Avoid storage access for HPU tensors (#65409)
cd2656a2e5,package,Untopiced,[package] add some docs describing how to debug dependencies (#65704)
b91375f741,releng,Untopiced,upgrade windows cuda installer: cu11.1.0 to cu11.1.1 (#65669)
ea546e20fd,nn_frontend,Untopiced,[Reland] nn.functional.linear OpInfo (#65498)
811601e19a,releng,Untopiced,Upload sccache stats (#65582)
82e0bf44c0,jit,Untopiced,Apply linter suggestions to #65137 (#65459)
fea32be964,cpp_frontend,Untopiced,Add HPU type for check_base_legacy_new (#65410)
e1340d4282,releng,Untopiced,[GHA] Small refactors (#65647)
f5b4e369f6,sparse_frontend,Untopiced,Sparse SoftMax: Remove unused variables (#65539)
7c62b6e973,python_frontend,bc_breaking,add deepcopy support to subclasses (#65584)
6a6ee92e36,quantization,Untopiced,[quant] Add op benchmark for CPU FakeQuantizePerChannel with float zero_points (#65241)
e9327ed2ce,fx,Untopiced,Add nn.function.hardtanh in acc_tracer (#65639)
63bb7c6dba,mobile,Untopiced,Refactor AotCompile to return a pair (#65707)
002ff19836,fx,Untopiced,[acc_utils] Fix off by one for model info getter (#65708)
46b3fc032a,quantization,Untopiced,Migrate remainder of THCDeviceUtils.cuh to ATen (#65472)
c28e3ffb4b,jit,Untopiced,"[jit] Shape propagation batch_norm, dropout, quantize, hardswidh (#64740)"
31ea4358d8,jit,Untopiced,[tensorexpr] Add Op handling for mobilenetv3 large (#64741)
43d47bdcca,jit,Untopiced,[tensorexpr] conv2d handle optional bias (#64750)
6a99053515,sparse_frontend,Untopiced,Added sparse-tensor copy logic to dispatcher (#65304)
3324bae5f1,cuda,Untopiced,Remove THCTensor.cu and THCTensorCopy.cu copy (#65491)
26e31f76b0,autograd_frontend,Untopiced,`*_solve` methods: implements forward AD (#65546)
e155e7520f,cpp_frontend,Untopiced,MaxUnpooling: parallel_for not always backed by OMP (#65655)
57e5ae5306,vulkan,Untopiced,[vulkan] Use push constants instead of SSBOs (#65716)
87cd658c27,cpp_frontend,Untopiced,Add override to virtual destructor in derived class (#65476)
50edc2679d,onnx,Untopiced,onnx/test.sh: Run test/onnx in only shard 1 (#65722)
145202c45b,dataloader_frontend,Untopiced,Define timeout in TestIndividualWorkerQueue (#65742)
278edb5626,releng,Untopiced,.circleci: Only generate docker configs we need (#65728)
4752453d27,linalg_frontend,Untopiced,[Structured Kernels] Port for `baddbmm` and `bmm` (#64805)
57529d48c4,quantization,Untopiced,[quant] Fix applying non-zero offset 1 to null pointer in quantized interpolation (#65570)
93852bb2d4,quantization,Untopiced,Port `sort` kernel to structured kernels. (#62391)
51f1569c77,linalg_frontend,Untopiced,Add checks for structured in-place operations. (#65686)
c2252b3aa6,python_frontend,Untopiced,Port `max` kernel to structured kernels. (#61449)
c829cb6840,python_frontend,Untopiced,Port `min` kernel to structured kernels. (#61450)
a90912ecc5,sparse_frontend,Untopiced,[sparsity] Remove the pack_param from the sparsifier state_dict (#65292)
92ee5cc2e2,sparse_frontend,Untopiced,[sparsity] Fix for accumulation bug in WeightNormSparsifier (#65293)
609384c056,sparse_frontend,Untopiced,[sparsity][doc] Docstring for WeightNormSparsifier (#65294)
b3c32ad32f,releng,Untopiced,.github: Move calculate-docker-image into build (#65789)
72b27bde83,releng,Untopiced,[CIFlow] Modify workflow trigger logic (#65733)
0d7036fdaf,package,Untopiced,don't leak build time path name to runtime for frozen python modules (#65715)
ed4491be6f,releng,Untopiced,Fix error code checking for Windows build scripts (#57331)
d528c7f3c0,releng,Untopiced,.github: Move windows back to default directory (#64962)
4a7a0ea42e,dataloader_frontend,Untopiced,Skip flaky ASAN tests (#65792)
8a247fb418,cpp_frontend,Untopiced,LLVM-12 fix for shm_mutex (#65781)
f9c2dc860d,skip,Untopiced,make layout check optional in torch.testing.assert_close() (#65419)
0a0564a347,skip,Untopiced,Revert D31206837: [pytorch][PR] `*_solve` methods: implements forward AD
07d5d7b5cc,skip,Untopiced,move kernel launch checks from `torch.testing` to `torch.testing._internal.check_kernel_launches` (#60862)
aebde1bc2b,onnx,Untopiced,deprecate device getter from `torch.testing` namespace (#63844)
f63150fd1d,mobile,Untopiced,[PyTorch Edge] Reduce the cost of computing isIncludedInAlias() (#65735)
5f7ab7be6f,jit,Untopiced,[Static Runtime] concat_add_mul_replacenan_clip retains axis arg (#65741)
971c57f1d0,skip,Untopiced,CMake: Limit python include directories to only python libraries (#65654)
1d681c1ab2,cpp_frontend,bc_breaking,Migrate THCThrustAllocator to ATen (#65492)
09eb3e661c,jit,Untopiced,don't check 0 elements for cat symbolic diff (#65751)
2670cacfc2,skip,Untopiced,LLVM-12 fix for tensor_new.cpp (#65785)
9b40eaaaab,skip,Untopiced,Revert D31193205: [pytorch][PR] CMake: Limit python include directories to only python libraries
085e2f7bdd,distributed,Untopiced,[ROCm] Changes not to rely on CUDA_VERSION or HIP_VERSION (#65610)
911d01c1de,fx,Untopiced,type annotate operator_support (#65136)
6c2f235d36,build_frontend,Untopiced,common_utils.py: Add ASAN as a platform for which you can disable tests (#65791)
2f712c452e,releng,Untopiced,.github: Remove confusing on_pull_request variable (#65731)
c975ca4337,jit,Untopiced,[Static Runtime] Simplify out variant overload implementations (#65384)
7b5d676fa1,releng,Untopiced,.github: Bump linux gpu max limit to 100 (#65831)
a84feeeade,mobile,Untopiced,[PyTorch Edge] Conditionally trim dispatch key set to save heap memory at runtime (#65732)
0dd1b74a5b,cpp_frontend,Untopiced,Migrate THCScanUtils to ATen (#65743)
8bf0ba546e,quantization,Untopiced,ns for fx: add basic testing on cuda (#65593)
7191dd2613,nn_frontend,Untopiced,Update Module docstring for Python 3 (#65748)
20374c991b,cpp_frontend,Untopiced,slow_conv2d_forward: avoid calling dispatcher in parallel region (#65724)
ad85b582da,cuda,Untopiced,Remove THCDeviceTensor (#65744)
2b22a5dde2,distributed,Untopiced,[NCCL] Init dummy NCCL comms in constructor (#65173)
5950240bdf,releng,Untopiced,Stop Win+CUDA-10.2 builds (#65649)
91611fe1d1,autograd_frontend,Untopiced,Decouple forward AD checks from backward AD in OpInfo tests and gradcheck (#65040)
9f97c66a7a,skip,Untopiced,Make JIT Aliasing Test Less Brittle (#65493)
768cfaa8f8,distributed,Untopiced,fix typo in _sharded_tensor (#65511)
fb412bdd80,skip,Untopiced,Avoid saving self for`softmax` and `log_softmax` (#65242)
c7ef620a14,quantization,Untopiced,[quant] Add imports to the torch/ao/quantization/__init__.py (#64911)
3900509b7d,distributed,Untopiced,(torchelastic) make --max_restarts explicit in the quickstart and runner docs (#65838)
5349ea921b,cuda,Untopiced,Migrate THCIntegerDivider.cuh to ATen (#65745)
91f8755b0e,distributed,Untopiced,Revert D31005792: [NCCL] Init dummy NCCL comms in constructor
2c29ec2a41,distributed,Untopiced,"Remove ""SciPioneer"" from PT Distributed code owners (#65862)"
dd354117ef,linalg_frontend,Untopiced,Skip failing tests when LAPACK and MAGMA are not available (#64930)
cd458fe092,jit,Untopiced,[JIT] Make output of prim::TupleConstruct alias only with its inputs (#64879)
4176afc4a0,jit,Untopiced,[Static Runtime] Disable SigridTransform + ListUnpack fusion when outputs reachable from graph output (#62697)
de80aff72d,jit,Untopiced,Revert D31132861: Make JIT Aliasing Test Less Brittle
6d4b93bd96,quantization,Untopiced,[quant] adding memoryless observers for embeddingbag QAT work (#65699)
4666e3f192,skip,Untopiced,[quant] update fused_obs_fake_quant op to accept output_fake_quant argument (#65621)
b777d790ea,dataloader_frontend,Untopiced,Convert Sampler back to lazily construction (#63646)
ea776fa034,nn_frontend,Untopiced,Update CODEOWNERS for optim (#65773)
7f87ff183d,jit,Untopiced,[RFC] [Modular] Include less headers in vararg_functions.cpp (#65672)
928a4bbafb,jit,Untopiced,[JIT] Fix compilation unit reference link in constant object upon load (#65784)
cdbfb2b689,releng,Untopiced,.github: Bump linux and windows gpu max available (#65923)
6a30d83596,releng,Untopiced,Move ASAN to GHA (#65846)
be00f0207a,releng,Untopiced,Update git version for CentOS base dockers (#65703)
541eb1db63,cuda,Untopiced,Add cuSPARSE descriptors and update CSR addmm (#60838)
9ae63bd87c,jit,Untopiced,Revert D31238123: [pytorch][PR] Avoid saving self for`softmax` and `log_softmax`
d4d3bb91f9,fx,Untopiced,Refactor `OperatorSupport` related code and fix TRT not supporting int64 dtype (#65848)
24f59fa20b,skip,Untopiced,[ci] fix softmax bc check (#65952)
8f3983254b,performance_as_product,Untopiced,[MicroBench] Added a micro benchmark for prefix sum (#65790)
38c77539e8,mobile,Untopiced,[PyTorch][Edge] Fix inefficiency in objLoaderMobile (#65710)
70f9f58a71,python_frontend,Untopiced,Add __module__ to torch.dtype.__dict__ (#65182)
6285348f06,linalg_frontend,Untopiced,Implement n-dimensional hermitian FFTs (#63890)
22f36353dc,linalg_frontend,Untopiced,Revert D31137652: [pytorch][PR] Skip failing tests when LAPACK and MAGMA are not available
b3da2afebe,python_frontend,Untopiced,Clarified difference in behavior of `empty_strided` and `as_strided` (#64568)
207fefc988,releng,Untopiced,Delete rouge cu102 windows builds (#65961)
3d6d4f4322,fx,Untopiced,[fx2trt][quant] Add lowering support for per channel quantization in fx2trt (#64787)
84190dafa8,onnx,Untopiced,[ONNX] Update instance_norm implementation and support training (#60538) (#64375)
f17ee368b3,onnx,Untopiced,Fix empty size constant creation (#63607) (#64376)
2d61009f4a,onnx,Untopiced,[ONNX] Fix input sequence for pad op (#60554) (#64377)
41bdfe3919,onnx,Untopiced,[ONNX] Fix cuda test case (#63597) (#64378)
7e15f2ddaa,onnx,Untopiced,[ONNX] Fix gather squeeze axis in constant folding (#63588) (#64379)
0f0ef4fe64,onnx,Untopiced,Add onnx test for batched_nms (#53175) (#64381)
d4ff344fae,onnx,Untopiced,[ONNX] Fix remainder export (#64230) (#64578)
89cbe6229d,onnx,Untopiced,[ONNX] Update doc and error message for indexing export (#64290) (#64579)
e598ba2ef3,onnx,Untopiced,[ONNX] Fix inplace fill_ dtype export mismatch (#64233) (#64580)
d39790340d,onnx,Untopiced,[ONNX] Enable export of __xor_ (#64042) (#64581)
f6dfac6974,cpp_frontend,Untopiced,Migrate THCCachingHostAllocator to ATen (#65746)
2481c06496,caffe2,Untopiced,[caffe2] fix LLVM-12 nullptr-with-nonzero-offset UBSAN error (#65506)
4f5ea5983a,mobile,Untopiced,[QPL] move metadata logging to markerEnd for model run QPL (#65451)
6502fb89dd,skip,Untopiced,Make JIT Aliasing Test Less Brittle (#65493)
08df4c2b3c,cpp_frontend,Untopiced,slow_conv2d grad_input: avoid dispatch in parallel region (#65725)
a6ad2b41ac,jit,Untopiced,[Static Runtime] Make module_ optional in StaticModule (#65882)
d84191fcc6,jit,Untopiced,[TensorExpr] Kernel: make prim::ConstantChunk handled like other ops. (#65549)
eee9ad0fdd,jit,Untopiced,[TensorExpr] Add a skeleton for a registry of NNC lowerings. (#65550)
3a0165da49,jit,Untopiced,[TensorExpr] Port NNC lowerings to the new registry mechanism. (#65551)
015e0079e3,jit,Untopiced,[TensorExpr] Move 'compute*' functions to operators/... (#65552)
765b6a90f3,jit,Untopiced,[TensorExpr] Move lowerings registration from kernel.cpp to lowerings.cpp. (#65553)
33c03cb61a,cpp_frontend,Untopiced,[deploy][1/n] Make deploy code conform to PyTorch style. (#65861)
2828ce53fd,jit,Untopiced,Added jit log stream changing function and some refactor (#65768)
ea0de37d2e,mobile,Untopiced,[PyTorch] Avoid string construction from const char* and speedup empty string creation if error messages are suppressed (#65939)
8b1aa85388,sparse_frontend,Untopiced,[sparsity] Change API to take FQNs as configuration (#65296)
c27b427cd9,sparse_frontend,Untopiced,[sparsity] Add m-out-of-n support in the WeightNormSparsifier (#65295)
c1447f06a8,cpp_frontend,Untopiced,[special] special alias for softmax (#62251)
e3af4be963,caffe2,Untopiced,pytorch quantization ao migration phase 2: caffe2/benchmark (#65833)
dac35b3592,quantization,Untopiced,pytorch quantization ao migration phase 2: torch/jit (#65829)
227e37dd39,caffe2,Untopiced,pytorch quantization ao migration phase 2: caffe2/test (#65832)
982ef8837b,jit,Untopiced,[Static Runtime] Fuse ListUnpack + gather_ranges_to_dense (#65116)
fc52f1293e,dataloader_frontend,Untopiced,"Improve pytorch type hints (Dataloader, trig functions)"
8595b6eeed,python_frontend,Untopiced,Avoid UB when indexing into size-0 tensors (#65878)
8cf047afac,jit,Untopiced,[nnc] Add call_with_numel interface for fast CUDA calls (#65213)
bff8d8fd28,jit,Untopiced,[nnc] Add BufHandle.store to python API (#65213)
53c0d91db9,autograd_frontend,Untopiced,Make autograd codegen for differentiable outputs safer to use (#65823)
383c0a3858,python_frontend,Untopiced,Fix internal assert failure for torch.all and torch.any with requires_grad=True (#65714)
5f7cadc7aa,skip,Untopiced,Avoid saving self for`softmax` and `log_softmax` (#65242)
eac218dbc6,quantization,Untopiced,"Revert ""Port `sort` kernel to structured kernels. (#62391)"" (#65876)"
21da6ae9ce,dataloader_frontend,Untopiced,suppress mypy error (#66003)
ccf8d48f16,jit,Untopiced,Revert D31317680: [pytorch][PR] Avoid saving self for`softmax` and `log_softmax`
592481a5cc,fx,Untopiced,"[fx][const_fold] Refactor to use base split module to simplify, and correctly handle non-single-Tensor outputs (#65933)"
21eebc9fd6,skip,Untopiced,[PyTorch][easy] Use copy-and-move instead of copy-and-swap in IValue::operator= (#65826)
f14e5e636d,fx,Untopiced,[fx2trt]fix slice tensor converter (#65960)
176d3c6fb4,quantization,Untopiced,[PyTorch] Fix many Tuple::elements() callsites (#64065)
7b10a76e05,jit,Untopiced,[PyTorch] Try removing Android strtod implementation (#65713)
ffede499b2,jit,Untopiced,[PyTorch][Static Runtime] Fast path for contiguous to_copy (#65499)
6e8ffd191e,skip,Untopiced,Fix typo in name of LayerNormBackwardCUDAKernel (#66000)
894d296bae,releng,Untopiced,Remove usage of GitHub's artifact store in linux jobs (#65875)
8e8695285f,releng,Untopiced,Re-generate workflows (#66027)
d937473709,skip,Untopiced,Fix typo in tensor docs (#64160)
7d22007902,fx,Untopiced,[fx-acc] add acc_op optimization flags and decorator (#65928)
ad889d0b5e,skip,Untopiced,Revert D30634700: [pytorch][PR] Fix typo in tensor docs
060e41eafa,dataloader_frontend,Untopiced,Forward fix type hint for DataLoader (#66001)
f85d7422bb,fx,Untopiced,[fx2trt]add support for torch.tile (#66016)
d9a95e66f0,build_frontend,Untopiced,Upload test failures to RDS (#65873)
23caeb3f71,mobile,Untopiced,model_dump: Add a helper to produce html with a single call (#66005)
e1d963e8fc,mobile,Untopiced,model_dump: Fix memory computation when both constants and data tensors are present (#66006)
10f6294281,caffe2,Untopiced,"Fix shape inference dim_type for Clip, Mean, Div (#65996)"
a6280ab653,skip,Untopiced,Compile without -Wno-unused-variable (#65954)
363ccb257d,fx,Untopiced,GELU acc OP (#65957)
eb3b9fe719,mobile,Untopiced,[XROS][ML] System specific adjustments for UTs to work. (#65245)
ca76e193a3,nn_frontend,Untopiced,Fix nll_backward for negative weights (#64572)
c269f471f4,skip,Untopiced,Fix cang-tidy regressions caused by #65954 (#66040)
5ef350d7cc,skip,Untopiced,Revert D31359010: [pytorch][PR] Fix cang-tidy regressions caused by #65954
e4ee5ca698,cpp_frontend,Untopiced,Revert D31326599: [pytorch][PR] Compile without -Wno-unused-variable
7941590a51,jit,Untopiced,[JIT] Selectively enable precise alias analysis for TupleConstruct (#66025)
8b8012a165,mobile,Untopiced,[PyTorch Edge] Skip writing version during backport (#65842)
8a307640db,fx,Untopiced,selective trt import based whether we have gpu or not (#66045)
7c52963350,quantization,Untopiced,[WIP] skip constant folding dequant node (#63991)
29c0725e8a,caffe2,Untopiced,"Back out ""[caffe2] fix LLVM-12 nullptr-with-nonzero-offset UBSAN error"" (#66055)"
0fc6bd2e47,cuda,Untopiced,[gpu ne eval] disable adam decay unit test for gpu (#66056)
c7748fc172,nn_frontend,Untopiced,Added validation of mode parameter in AveragedModel (#65921)
40948a935d,jit,Untopiced,Fix LLVM-12 UB in generate_proposals_op.cc (#66009)
89ed9bdaee,cpp_frontend,Untopiced,[Static Runtime] Fix bug of creating output aliases in aten::embedding_bag (#65516)
b6d5f1ee70,vmap_frontend,Untopiced,Allow None to pass through for vmap (#65565)
73901b099d,autograd_frontend,Untopiced,Add batched_grad parameter to `autograd.grad` (#65564)
8f5631b859,autograd_frontend,Untopiced,Refactor functional api vectorized jacobian to use batched grad parameter (#65566)
4cdfceddd2,jit,Untopiced,[Reland] Avoid saving self for `softmax` and `log_softmax` (#66018)
69da4b4381,releng,Untopiced,GHA: make obvious when we are running smoke tests to user (#66011)
7c758759e3,mobile,Untopiced,[PyTorch Edge] Avoid string copying in TypeParser (#64278)
6eb3a1c831,releng,Untopiced,Run master clang-tidy on PRs (#66104)
6ea4902cf4,caffe2,Untopiced,[ao_migration] torch.quantization --> torch.ao.quantization in caffe2/torch/fx (#66096)
6b0aa2958d,fx,Untopiced,[FX] Support torch.layout as arg (#66048)
4c4525fa5c,cpp_frontend,Untopiced,Compile without -Wno-unused-variable (take 2) (#66041)
ed50fa2513,jit,Untopiced,[Static Runtime] Test isOptimizableContainerType and getAlwaysAliveValues (#65849)
d609957c95,jit,Untopiced,patching graph_for (#55139)
2a5116e159,fx,Untopiced,[quant][fx2trt] Add quantize_per_channel in acc_ops and acc_ops_converter (#65287)
df475aa1dc,vulkan,Untopiced,Update Vulkan runner in benchmark binary to handle non-tensor inputs (#66123)
1932bc69e9,releng,Untopiced,Move GHA to ONNX (#65975)
aa80f05d2d,cuda,Untopiced,Remove sync in Embedding caused by unique (#66091)
9c3eb50b7b,jit,Untopiced,[PyTorch] Use std::move() in a couple places in function_schema_parser.cpp (#66114)
1db78c30c9,caffe2,Untopiced,Fix LLVM-12 concat_split_op.h error (#66060)
bda3230b62,cpp_frontend,Untopiced,slow_conv2d grad_weight: call gemm directly (#65726)
2b39b80971,quantization,Untopiced,[quantized] Replace conv_p with convolution_op in qnnpack (#65783)
458a00bacb,quantization,Untopiced,"Back out ""[quant] update fused_obs_fake_quant op to accept output_fake_quant argument"" (#66063)"
92d0b7e99c,package,Untopiced,[deploy] fix typo in `registerModuleSource` (#66107)
727576e501,quantization,Untopiced,[quant] Fixing the hypothesis test for topk (#66057)
0d020effab,quantization,Untopiced,[quant] Fix the parts that were missing after initial migration (#66058)
da0e29edd4,skip,Untopiced,fixing sorting in stride indices (#63940)
588c1787ba,cpp_frontend,Untopiced,Update link to example pytorch/examples (#66095)
5883523c1d,package,Untopiced,Remove dtype from torch.Storage and use only torch.ByteStorage (#62030)
89b56d630d,releng,Untopiced,Create CI sev template (#66163)
a3bbaf227c,skip,Untopiced,Revert D31227448: [pytorch][PR] fixing sorting in stride indices
e7747795c9,mobile,Untopiced,[PyTorch Edge] Reduce dispatch table size further for a trimmed build (#66112)
a5e6b2b2e3,jit,Untopiced,[Static Runtime] Add variadic sigrid_transforms_torch_bind (#63960)
6e06cb76ff,jit,Untopiced,[JIT] Initialize CUDA context before launching fused kernel (#65064)
7452b65144,skip,Untopiced,Remove unused `dump` method from VSX vec256 methods (#66085)
d2021e5e68,releng,Untopiced,ci: Migrate vulkan builds to GHA (#66044)
68555339d7,build_frontend,Untopiced,test_utils.py: Add another retry to test_download_url_to_file (#66159)
143c957c2d,jit,Untopiced,[nnc] Reduced memory usage of LLVMCodeGen object after code generation is complete (#65373)
e94fea08d0,skip,Untopiced,Add hash and int128 utils for Lazy Tensor Core (#65635)
5e6347ca64,releng,Untopiced,.circleci: Remove migrated distributed configs (#66174)
f062def486,skip,Untopiced,Revert D31260343: [pytorch][PR] Add hash and int128 utils for Lazy Tensor Core
83bac89d64,quantization,Untopiced,[quant] Add fp32/fp16 zero_point support for GPU fakeQuant (#65836)
dc26f5eb65,fx,Untopiced,[FX] Specifies a default value when possible for placeholders created from concrete_args (#59569)
eeabab03e7,distributed,Untopiced,[DataParallel] Log API Usage for tracking (#66038)
3bd26792c0,distributed,Untopiced,Skip test_multiple_groups on windows (#66154)
43e26d0086,package,Untopiced,[deploy] Improve error messaging for create_movable (#65955)
6cdea8239e,jit,Untopiced,Precomputing Transposes for frozen linear layers (#65631)
cbc29acca3,skip,Untopiced,[Codemod][FBSourceBlackLinter] Daily `arc lint --take BLACK`
8d435877d5,onnx,Untopiced,Fix typos at ONNX docs (#66090)
c0b1965f7c,vulkan,Untopiced,"Back out ""[vulkan] Use push constants instead of SSBOs"" (#66169)"
931352c68d,cpp_frontend,Untopiced,Make handle_torch_function_no_python_arg_parser public (#66054)
c75210face,mobile,Untopiced,[PyTorch Edge][type] Move TypeParser class definition to header file (#65976)
a5895f85be,mobile,Untopiced,[PyTorch Edge][type] Add type check in compatibility api (#63129)
ab25516054,mobile,Untopiced,[PyTorch] Remove unused function in import (#65865)
747a5782e3,quantization,Untopiced,[quant][fx] Don't assume bias is a keyword argument (#61647)
9de9733390,jit,Untopiced,Add 1d to 2d conv transform during mobile optimization (#65850)
b8e1999253,cuda,Untopiced,[quant] Add op benchmark for GPU FakeQuantizePerChannel with float zero_points (#66183)
bfaaac6392,build_frontend,Untopiced,Ignore register_rds errors (#66185)
c4ea447eb5,jit,Untopiced,Use src size for memcpy in order to avoid fortify complaints (#65222)
6d4d636d66,releng,Untopiced,[GHA] Rectify `trigger_action_only` flag (#66209)
9a0b2acd76,quantization,Untopiced,[quant] Remove hypothesis from qtopk (#66158)
623ac7eabb,cpp_frontend,Untopiced,slow_conv3d: Avoid dispatch in parallel region (#65737)
8548928950,fx,Untopiced,Cumsum: acc_ops (#66189)
9285981de1,quantization,Untopiced,Clean up unused model instantiation (#65487)
6d7fab5929,skip,Untopiced,[Static Runtime][easy] Clone scripts do not use aten::add (#66161)
90db214d4b,caffe2,Untopiced,support counter-based fused rowwise adagrad (#66177)
252b6f2cba,jit,Untopiced,[PyTorch][easy] Remove dead std::set in parseAliasAnnotation (#65712)
e2be087207,quantization,Untopiced,[oss][pytorch] Add quint2x4 dtype (#65545)
84c5970a77,releng,Untopiced,ci: Migrate slow_gradcheck to GHA (#65730)
722f1ccfb8,distributed,Untopiced,[DDP][Instrumentation] Profiling range for bucket copy (#65769)
480a1a88d6,distributed,Untopiced,[DDP] Log iteration in debug mode (#65770)
7cc121dbcd,cpp_frontend,Untopiced,slow_conv3d grad_input: Avoid dispatch in parallel region (#65757)
fc4836f400,mobile,Untopiced,[Fix] Use full name to look for the promoted prim operator table (#66081)
05e1476d49,jit,Untopiced,[jit] Fix list copy in MemoryDAG (#65176)
78209b93b3,build_frontend,Untopiced,Don't build shared library for AOT Compiler (#66227)
1d586e78c6,autograd_frontend,Untopiced,`*_solve` methods: implements forward AD (#65546)
e8837d741e,vulkan,Untopiced,[Vulkan] cat operator for height dimension (#66103)
4937218611,distributed,Untopiced,[torch][launch] Add ability to override sys.executable for `torch.distributed.run` (#66179)
213c3f45da,dataloader_frontend,Untopiced,[oss/ci] skip TestDataLoaderPersistentWorkers on ASAN (#66236)
6c54971cd9,package,Untopiced,Open Registration for torch::deploy Builtins (#65953)
67970e8c9b,releng,Untopiced,Add CI tests for AOT Compile (#65441)
300613dc60,fx,Untopiced,make FX symbolic tracing reuse buffers if they're the same (#66211)
0cab25468d,mobile,Untopiced,[Pytorch Edge][tracing-based] reorganize model tracer dependency (#63421)
eeaf527feb,skip,Untopiced,[Pytorch Edge][tracing-based] build tracer in OSS (#64087)
3f30526ff2,cpp_frontend,Untopiced,Remove THCAllocator (#65942)
ac0dbd6eec,mobile,Untopiced,Promote missing ops for delegated models (#66052)
115526cc88,fx,Untopiced,GELU Converter (#66008)
8a974a482c,quantization,Untopiced,[quant] Add support for quantization of Embedding{Bag} in dynamic quant APIs (#65674)
dcf39f9bb9,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
057a01556c,jit,Untopiced,[Static Runtime] Do not use variadic_sigrid_transforms_torch_bind if out variant is disabled (#66221)
1e4bcbdddb,complex_frontend,Untopiced,[Bootcamp][Pytorch Core] Add test for complex numbers for vanilla SGD (#66230)
2f1ab477f1,caffe2,Untopiced,Speed up DataTypeToTypeMeta (#66113)
88f8944ef1,mobile,Untopiced,Revert D30599136: [Pytorch Edge][tracing-based] build tracer in OSS
61fca037d6,distributed,Untopiced,"[Part 1] upstreaming fairscale fsdp to PyTorch -- sharding, core data flow and hooks (#63881)"
a8c0b362ce,skip,Untopiced,"[pytorch][PR] Add hash and int128 utils for Lazy Tensor Core"" (#66181)"
d5f64afc38,jit,Untopiced,[Static Runtime] Support aten::to.prim_dtype overload (#64928)
9fb6ba24e7,fx,Untopiced,Update `torch.fx.passes.split_module` docstring (#65542)
361b34eb81,fx,Untopiced,Chunk: acc_ops (#66010)
2e4e5b0264,python_frontend,Untopiced,Add inplace_variant for resize_ OpInfo (#66135)
e6a4f746c2,cpp_frontend,Untopiced,slow_conv3d: Use at::sum for grad_bias accumulation (#65758)
2213c463ba,cpp_frontend,Untopiced,C++ API and docs for hfftn (#66127)
f445ed19b2,python_frontend,Untopiced,OpInfo for 2d fft functions (#66128)
3cc40253d9,distributed,Untopiced,add gather to ShardedTensor (#65671)
c30dc52739,skip,Untopiced,[nnc] Use given kernel function name while emitting code (#66216)
7e5ef5e517,skip,Untopiced,"[nnc] Added a cache to use singleton instances of PytorchLLVMJIT for every triple,cpu,attrs combination (#66217)"
40dd2711b6,skip,Untopiced,[Static Runtime] Cleanup LLVMCodeGen memory after code gen completes (#66218)
5e7d8ec846,package,Untopiced,Support Registering a Variable Length List of Builtin Modules for torch::deploy Builtin Libraries (#66021)
0e2d1b221a,nn_frontend,Untopiced,[Bootcamp][Pytorch Core] Add testing for complex non-vanilla SGD
416f593080,jit,Untopiced,[Static Runtime] Group graph nodes into input aliases & output aliases (#65517)
86de09e49a,build_frontend,Untopiced,Upgrade to ubuntu:trusty-20190515 (#63468)
20f2e55d4f,skip,Untopiced,Rename cuda/Resize.cu to cuda/Resize.cpp (#65943)
38f5144eae,distributed,Untopiced,Fix https://github.com/pytorch/pytorch/issues/61982 (#66015)
ca363d1e22,releng,Untopiced,docker: Ensure libgnutls30 for all docker builds (#66258)
e1817d895f,skip,Untopiced,[BE] Cleanup python_function.cpp (#66296)
d5033410b1,skip,Untopiced,Parallel: Deduplicate parallel functions in different backends (#65326)
bd9eee4e65,cpp_frontend,Untopiced,TBB: Use static partitioner to match OpenMP scheduling (#65327)
ebe530a9cd,releng,Untopiced,Periodic jobs should not have CIFLOW_DEFAULT label (#66300)
64caee1356,mobile,Untopiced,[PyTorch Edge] Leave out field for debug_handle if not being built with eager symbolication support (#66131)
a58ff186e8,quantization,Untopiced,[quant][embedding qat] Add basic EmbeddingBag QAT fakeQuant workflow (#65443)
227f91e72d,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
74477ba243,fx,Untopiced,[fx2trt] More controls over output dtypes (#65959)
321345d7c9,jit,Untopiced,"Revert ""Revert D31227448: [pytorch][PR] fixing sorting in stride indices"" (#66176)"
60fe854f9f,fx,Untopiced,[fx2trt] save and load TRTModule for OSS (#65958)
61f0bb70c1,skip,Untopiced,Allow external CUDA streams to be set as current (#65914)
4af913a7cf,python_frontend,Untopiced,fixed minor issues for index_add in docs (#65806)
8d6d448238,autograd_frontend,Untopiced,Add HPU for Autograd Fallback (#65605)
dc37547c44,python_frontend,Untopiced,Opinfos for avg_pooling (#64214)
4af47eb3a7,onnx,Untopiced,[ONNX] Update slice process shape to support rank only inference (#65782) (#66149)
53fefaa916,onnx,Untopiced,[ONNX] Fix duplicated output same name case (#64190) (#66137)
9b09a5f7ba,onnx,Untopiced,[ONNX] Enable scripting tests (#64780) (#66138)
11bc435622,onnx,Untopiced,Allow registration of custom symbolics for prim namespace (#64460) (#66139)
ece0221854,skip,Untopiced,"Rename int to long, add more C++ types. (#66108)"
81660c08f0,quantization,Untopiced,quantized add: enable broadcasting (#66049)
c9aba3b128,quantization,Untopiced,make error message when trying to quantize non floats more specific (#66050)
a7cc07f109,quantization,Untopiced,quantized embedding: make error message clearer (#66051)
e808e3d3d6,dataloader_frontend,Untopiced,[DataPipe] adding SequenceWrapperMapDataPipe (#66275)
ed17851642,dataloader_frontend,Untopiced,[DataPipe] adding test for IterableWrapperIterDataPipe (#66276)
8ebe1a924d,dataloader_frontend,Untopiced,[DataPipe] moving mux IterDataPipe test to the right location (#66277)
1ae468a484,jit,Untopiced,[jit] Refcounting spot fixes (#65346)
2d885ab73d,distributed,Untopiced,[jit] Reduce refcounting of Types (#65345)
0cad2c0615,cpp_frontend,Untopiced,Move intraop_launch_future from Parallel.h (#64166)
b5b1d49a66,distributed,Untopiced,[PG Wrapper][BE] Make some methods private (#66166)
b72a1782d8,distributed,Untopiced,[PG Wrapper][BE] Add collective information when monitored barrier error is (#66167)
201174cb91,skip,Untopiced,Revert D31389480: [pytorch][PR] Allow external CUDA streams to be set as current
dfb64b3287,distributed,Untopiced,log API usage for fsdp API in PyTorch (#64964)
0020a151c6,cpp_frontend,Untopiced,slow_conv3d grad_weight: call gemm directly (#65759)
3ef69a4598,jit,Untopiced,[static runtime] Pre-allocate hash tables (#65343)
c80693f7e6,jit,Untopiced,[jit] Add cache for MemoryDAG::collectAllContainedMemoryLocations (#65122)
94845fc44e,jit,Untopiced,[jit] Implement one-argument AliasDb::mayContainAlias more efficiently (#65177)
3bad54069b,jit,Untopiced,Concatting multiple linear layers with same input Tensor (different weight/bias) (#63198)
bc1dec9b81,skip,Untopiced,Migrate THCStorage_resizeBytes to ATen (#65944)
0be36d798b,skip,Untopiced,Remove Tensor.h include from TensorIterator.h (#64167)
097fdcdf0c,jit,Untopiced,Revert D31445798: [Static Runtime] Cleanup LLVMCodeGen memory after code gen completes
2e6fa0261f,jit,Untopiced,"Revert D31445797: [nnc] Added a cache to use singleton instances of PytorchLLVMJIT for every triple,cpu,attrs combination"
92ce188510,jit,Untopiced,Revert D31445799: [nnc] Use given kernel function name while emitting code
731cf494f2,quantization,Untopiced,Remove cuda/Loops.cuh dependency on native_functions.yaml (#64168)
8a02d3e5d0,cpp_frontend,Untopiced,Wextra fix for Tensorshape.cpp (#66320)
c1343ff706,skip,Untopiced,[Pytorch Edge] Support profiling kineto events from external source (#64397)
f1f3bd8c36,skip,Untopiced,"Back out ""Revert D31005792: [NCCL] Init dummy NCCL comms in constructor"" (#65883)"
b23709df03,quantization,Untopiced,[ao_migration] torch/nn/quantized: torch.quantization -> torch.ao.quantization (#65900)
1a6482ee2a,quantization,Untopiced,[ao_migration] torch/nn/quantizable: torch.quantization -> torch.ao.quantization (#65901)
2daae532bd,quantization,Untopiced,[ao_migration] torch/nn/qat: torch.quantization -> torch.ao.quantization (#65902)
a28b038af4,quantization,Untopiced,[ao_migration] torch/nn/intrinsic: torch.quantization -> torch.ao.quantization (#65903)
51835bec07,caffe2,Untopiced,Wextra fix 1 for caffe2 (#66272)
c957d9fdf6,linalg_frontend,Untopiced,Replace _baddbmm_mkl_ with cpublas::gemm_batched (#66165)
c62ed96496,mobile,Untopiced,Revert D30710710: [Pytorch Edge] Support profiling kineto events from external source
0a48f56318,distributed,Untopiced,"Revert D31299350: Back out ""Revert D31005792: [NCCL] Init dummy NCCL comms in constructor"""
4a302a3074,cuda,Untopiced,Wextra fix for CUDAApplyUtils.cuh (#66323)
566922bbcd,jit,Untopiced,clean up mypy nit in torch/jit/_recursive.py (#66253)
5a67ffe0ad,jit,Untopiced,"[PyTorch][Static Runtime] Combine ProcessedNode::{native_,}fn_ (#65414)"
904fbadaff,skip,Untopiced,Fix merge conflict in bc tests (#66356)
e5f6f356da,fx,Untopiced,[hpc infer] fix bench perf number
85b562dd2b,fx,Untopiced,Fix type checking errors in fx/utils.py (#66311)
fb5a80ffd8,performance_as_product,Untopiced,[jit] Don't force refcount bumps from getTypePtr (#66282)
1763c25414,jit,Untopiced,[PyTorch][jit] Fix excess refcounting in TupleType::compare (#66286)
3fe5895a00,mobile,Untopiced,"Back out ""Revert D30599136: [Pytorch Edge][tracing-based] build tracer in OSS"" (#66267)"
4cb4d11e0b,skip,Untopiced,"Disable ""-Wignored-qualifiers"" for vec256_bfloat16.h (#66279)"
109aa135e6,skip,Untopiced,Remove apparently unnecessary std::remove_cv_t (#66254)
b96c7aea73,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
a89ac3138e,skip,Untopiced,Create a documentation page for FX graph mode quantization APIs (#66122)
f0fa3d1110,skip,Untopiced,Create separate documentation pages for quantization observers and fake_quants (#66125)
7332ed13ed,skip,Untopiced,Create a documentation page for `torch.ao.quantization.QConfig` (#66129)
fe86f0e068,skip,Untopiced,Quantization docs: consilidate all API references on a single page (#66198)
7d2526ab20,skip,Untopiced,Quantization docs: rewrite API reference to be more automated (#66201)
309a8cf46c,skip,Untopiced,Quantization documentation: move backend section down (#66210)
9539e6216b,skip,Untopiced,Quantization docs: add pages for Numeric Suite (Eager and FX) (#66222)
84326ef059,build_frontend,Untopiced,Remove native_functions.yaml dependency from binary ops (#64169)
27f193af64,skip,Untopiced,Automated submodule update: kineto (#59674)
150b7c7410,fx,Untopiced,open source engine_layer_visualize.py (#66301)
0e0c98077f,quantization,Untopiced,[quantized] Implement 3d convolution in qnnpack (#66350)
291d463cf9,fx,Untopiced,Revert D31495086: open source engine_layer_visualize.py
ad0accdecd,skip,Untopiced,Revert D31447610: Quantization docs: add pages for Numeric Suite (Eager and FX)
df1858bea5,skip,Untopiced,Revert D31447611: Quantization documentation: move backend section down
09c3e6002b,quantization,Untopiced,Revert D31447615: Quantization docs: rewrite API reference to be more automated
037ac2330e,skip,Untopiced,Revert D31447616: Quantization docs: consilidate all API references on a single page
10633460ce,quantization,Untopiced,Revert D31447614: Create a documentation page for `torch.ao.quantization.QConfig`
b85fd4c54f,skip,Untopiced,Revert D31447613: Create separate documentation pages for quantization observers and fake_quants
9971113340,skip,Untopiced,Revert D31447612: Create a documentation page for FX graph mode quantization APIs
355acfdebc,mobile,Untopiced,[PyTorch Edge][tracing-based] use operator.yaml to build libtorch library (#66237)
bc06eefebe,cuda,Untopiced,[reland] Allow external CUDA streams to be set as current (#66324)
58fefa6516,distributed,Untopiced,Add pybind trampoline for ProcessGroup and Work (#66338)
0348148725,quantization,Untopiced,Update link to qnnpack in quantization doc. (#66226)
1d9a6862cd,quantization,Untopiced,fx quant: add a BC test for loading old torch.package models (#65538)
7c2f53b363,onnx,Untopiced,[BE] set pretrained=False for onnx tests (#66312)
1b40daac74,autograd_frontend,Untopiced,pinv: forward/backward AD which is Frechet-defined in a rank-preserving neighborhood. (#66092)
4775419850,skip,Untopiced,[BE] Address feedback from #66296 (#66315)
d3b29afbb6,skip,Untopiced,Remove old code that is unused in test/ (#66331)
c373387709,releng,Untopiced,Update CMake and use native CUDA language support (#62445)
c66847afbe,cuda,Untopiced,Add workaround for nvcc header dependecies bug (#62550)
3b4b1b2d23,releng,Untopiced,.github: Remove confusing ciflow_config.enabled variable (#66260)
736fa09a9a,jit,Untopiced,[Static Runtime] Manage output tensors (#65515)
221c308389,cuda,Untopiced,Wextra fix for LossCTC.cpp (#66381)
901df0cc22,distributed,Untopiced,Skip test_nccl_errors_nonblocking (#66394)
acb0157a3d,skip,Untopiced,Specialization for `c10::util:get_type_index<std::string>` (#66290)
998cb98844,jit,Untopiced,[PyTorch][jit] Cache TupleType objects in getTypePtr (#66340)
8c468ce00b,jit,Untopiced,[PyTorch][JIT] Return a reference from caching specializations of getTypePtr (#66342)
08fab7ae13,skip,Untopiced,Wextra fix for Integration.cpp (#66321)
1d14fbdad7,jit,Untopiced,[TensorExpr] Adding missing python binding for operators (#66336)
c03f851750,distributed,Untopiced,[torchelastic] Fix failing tests (#66440)
49f1605392,nn_frontend,Untopiced,[RFC] Reduce logging noise from AdagradOptimizer (#66443)
cc24e4e5d0,jit,Untopiced,[NNC] Normalize loops in SplitWithTail (#66242)
7baf4f6b12,fx,Untopiced,Chunk: Converter (#66028)
ae5a9a451f,package,Untopiced,Do not enforce unused vars rule for torch_deploy (#66447)
25965619dd,fx,Untopiced,"Back out ""Revert D31495086: open source engine_layer_visualize.py"" (#66431)"
88ed93c2ca,quantization,Untopiced,Fix type checking errors in torch/quantization/fx/qconfig_utils.py (#66428)
8b1258698e,quantization,Untopiced,Improve quantization API docs (#66379)
565cf47abf,quantization,Untopiced,Quantization docs: add pages for Numeric Suite (Eager and FX) (#66380)
a6774d6e1f,skip,Untopiced,Migrate THCState to ATen (#65948)
3eb9443619,fx,Untopiced,[FX] Fix issue where GraphModule.delete_all_unused_submodules deletes submodules from called leaf modules (#66430)
9a85167d22,distributed,Untopiced,Fix batch_isend_irecv tests for err case (#63112)
07ec250fd7,releng,Untopiced,[deploy] fix oss build (#66347)
d8532e3524,skip,Untopiced,[PyTorch] Split c10 Type.cpp into two files to allow targets to include one of them (#66445)
2d1552824a,skip,Untopiced,Revert D31386275: Migrate THCState to ATen
18e4688199,mobile,Untopiced,[Pytorch Edge] Improve bundled inputs name error handling (#65856)
24b9b304d9,jit,Untopiced,[TensorExpr] Nuke TE shape inference. (#65554)
60a2a295ce,jit,Untopiced,[TensorExpr] Use schema instead of op name in NNC lowerings. (#65843)
6864146f2b,jit,Untopiced,[TensorExpr] Fix lowerings for aten::view and aten::reshape. (#65852)
5f1518609b,jit,Untopiced,[TensorExpr] Fix lowering for aten::t. (#65859)
71e17d9827,dataloader_frontend,Untopiced,[DataPipe] Fix HttpReader IterDataPipe Issue with streaming (#66432)
1841f76cc0,build_frontend,Untopiced,Remove native_functions.yaml dependency from unary ops (#64170)
8674a3c6e3,build_frontend,Untopiced,Remove native_functions.yaml dependency from PowKernel (#64171)
213ac4e59c,build_frontend,Untopiced,Remove native_functions.yaml dependency from PointwiseOps (#64172)
8818dda237,linalg_frontend,Untopiced,Fix lstsq to work with inputs that require grad (#66426)
e6261083f9,fx,Untopiced,[FX] fuse permute021 linear pass for trt lowering (#66362)
a6eec0c60f,onnx,Untopiced,Upgrade onnx submodule to 85546f8c44e627f8ff1181725d03cc49f675e44f (#66427)
702fb1de72,fx,Untopiced,[fx2trt] open source tests for acc tracer (#66302)
17e79bc76c,quantization,Untopiced,remove is_reference from all is_output_quantized (#66456)
47c531b6e8,jit,Untopiced,[jit] Compare object identity first in ClassType::operator== (#65347)
d7916e3734,jit,Untopiced,[jit] Eliminate malloc & recursive refcount bumps in HashType::operator() (#65348)
c6216b2a43,mobile,Untopiced,"Back out ""Revert D30710710: [Pytorch Edge] Support profiling kineto events from external source"" (#66421)"
8b0eae5aa8,skip,Untopiced,Use shared CUPTI by default (#65401)
9984f4bb8b,build_frontend,Untopiced,Remove native_functions.yaml dependency from some reduction operators (#64173)
3ac2c74896,skip,Untopiced,Revert D31082208: Use shared CUPTI by default
d921891f57,releng,Untopiced,GHA: Stop skipping periodic jobs (#66264)
d32736e317,build_frontend,Untopiced,Make permission errors more human readable (#66492)
40794dbb25,fx,Untopiced,add backend_config_dict to checkGraphModeFxOp (#66499)
93d326c868,cpp_frontend,Untopiced,Add InplaceOrView boxed kernel (#63878)
06a156efc7,skip,Untopiced,.github: Enable onnx tests (#66513)
8492e6bc6a,releng,Untopiced,".github: scheduled -> schedule, fix periodic (#66531)"
08f3823647,cuda,Untopiced,Sparse CSR CUDA: add `addmv_out` (#61407)
2d3b23190c,releng,Untopiced,Revert D31591512: .github: Enable onnx tests
8eb85b5027,skip,Untopiced,Remove THCNumerics (#66388)
bc6935ddf5,distributed,Untopiced,[PyTorch][Distributed][Easy] Make ShardedTensor.size() equivalent to torch.Tensor.size() (#65087) (#66012)
80a3619823,skip,Untopiced,Remove THCTensorMathReduce.cuh (#66389)
9918fd8305,fx,Untopiced,[fx2trt] open source tests for converters (#66361)
ecb7b38c00,profiler,Untopiced,[PyTorch] Support additional arguments in Python record function (#65736)
87df043f63,complex_frontend,Untopiced,[Bootcamp][Pytorch]Add testing for complex parameters in Adagrad optimizer (#66501)
82a216c45b,fx,Untopiced,"Add tensor.{adjoint(),H,mT,mH} methods and properties (#64179)"
d85948896c,jit,Untopiced,Add softplus support to autodiff (#63942)
6401658b08,amd,Untopiced,fix type error in hipify_python.py (#66164)
84385c40e4,python_frontend,Untopiced,Add output_mask (#66068)
f8d98b5a6d,autograd_frontend,Untopiced,Compute input gradient only if required (CPU) (#66069)
8a40bb62f9,autograd_frontend,Untopiced,Compute input gradient only if required (CUDA) (#66070)
713e025c9f,nn_frontend,Untopiced,Add no-input-grad-needed cases to test_grid_sample (#66071)
9d13ae450a,dataloader_frontend,Untopiced,[oss/ci] skip all dataloader tests with asan (#66561)
5e34ac6c43,fx,Untopiced,[FX] Fix cases when we should not fuse due to more than one users of intermediate node (#66472)
cebaf21c5a,vulkan,Untopiced,[vulkan] Release GPU resources when vTensor::View is destroyed (#66477)
a8815d557a,vulkan,Untopiced,[vulkan] Remove the persistent resource pool (#66478)
a453ebc8ac,package,Untopiced,Use interactive_embedded_interpreter to dynamicly loading various third-party libraries (#66512)
fdd9f49cf5,python_frontend,Untopiced,add a note on numerical accuracy (#65947)
f48f20e154,skip,Untopiced,Make ContainerHash compatible with const& types (#66497)
09b90612c4,skip,Untopiced,.github: Enable onnx tests (#66513)
b792a77895,skip,Untopiced,Skip `interactive_embedded_interpreter.cpp` for clang-tidy (#66569)
c04bcde245,python_frontend,Untopiced,Make empty* and *_like factory functions respect tensor subclasses (#65677)
86cf22cb1c,python_frontend,Untopiced,Add OpInfo for torch.bucketize (#65821)
675ba6cd53,quantization,Untopiced,[qnnpack] Remove usage of conv_param_t in deconv-run.cc (#66465)
82986a17a6,skip,Untopiced,fix lint (#66572)
37db650c9c,skip,Untopiced,[Static Runtime] Clone test does not use uninitialized memory (#66557)
9e8281fd2f,fx,Untopiced,[fx2trt][code quality] Add type annotation and docstring to utils functions in acc_ops_converters.py (#66496)
75d98fa0ae,jit,Untopiced,[jit] Implement one-element MemoryDAG::mayContainAlias more efficiently (#65178)
9767282643,jit,Untopiced,[jit] Add MutableTypePtrHelper::mapTypeToBorrowedAliasTypeSet (#65344)
0aab34c26c,jit,Untopiced,[jit] Refcounting spot fixes in alias_analysis (#66295)
42328090cb,releng,Untopiced,[GHA] Hardcode doc build target to `master` (#66567)
5f45927d15,autograd_frontend,Untopiced,Autograd: Delay warnings until the end of backward execution (#66235)
eb8138d886,skip,Untopiced,dropout update in autodiff (#66273)
24202f7fb4,build_frontend,Untopiced,Remove native_functions.yaml dependency from Activation.cu (#64499)
4e1c075542,nn_frontend,Untopiced,log_sigmoid: Use log1p for improved precision (#66441)
e75de4f307,skip,Untopiced,remove a few unused THCTensor/Storage methods (#66555)
30d9fd9cf3,skip,Untopiced,Migrate USE_MAGMA config macro to ATen (#66390)
160946e3f3,nn_frontend,Untopiced,Use `torch.empty()` instead of `torch.tensor()` in `torch.nn.Parameter` (#66486)
c6f0dde3ca,fx,Untopiced,Cumsum Converter (#66376)
d30397d42a,jit,Untopiced,[PyTorch][Static Runtime] Don't use vector in ProcessedNode (#65429)
6634570aef,jit,Untopiced,[SR] Fix bug in ValueGroup (#66470)
c1c985a282,jit,Untopiced,Rename tensorexpr::Value so that it can coexist with torch::jit::Value (#66467)
7d9bbd3596,jit,Untopiced,Revert D31580382: [pytorch][PR] dropout update in autodiff
74849d9188,fx,Untopiced,[acc_shape_inference] add shape inference for quantize_per_channel (#66562)
e1348973ac,fx,Untopiced,Add common_fx2trt.py (#66579)
6310eb30d1,jit,Untopiced,[SR] Clean up GetLivenessMap (#66606)
a40812de53,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
d802877dfa,quantization,Untopiced,speed up quantized interpolate for channels last (#66525)
fe41df3601,python_frontend,bc_breaking,Deprecate x.T on tensors of dimension other than 0 or 2 (#64180)
77f98ea5e0,python_frontend,Untopiced,assert no duplicate yaml keys in codegen (#66238)
5d4452937d,fx,Untopiced,OpInfos for some Tensor dtype conversion methods (#64282)
d810e738b9,fx,Untopiced,OpInfo for `*_like` functions (#65941)
49a1d7bfcb,python_frontend,Untopiced,"[opinfo] elemwise parcel : isfinite, isinf, isposinf, isneginf, isnan, isreal (#66400)"
51b67f2bca,quantization,Untopiced,[qat]Removed outdated context manager in unit test. (#66274)
543b7fb942,onnx,Untopiced,[JIT] Fix type annotations of pooling modules (#65847)
016362e2d7,distributed,Untopiced,Run sparse tests only for TensorPipe agent. (#66600)
76f3b07caf,quantization,Untopiced,quantization docs: remove erroneous rebase artifact (#66577)
833ede33ed,caffe2,Untopiced,Fix ubsan in concat_split_op.h (#66283)
914796a69c,jit,Untopiced,Fix for prim::BroadcastMKLDNNTensors issue (#66628)
3097755e7a,nn_frontend,Untopiced,[DOC] Fix typo in KLDivLoss (#66583)
1d90f29f14,nn_frontend,Untopiced,[DOC] Improve Transformer documentation (#66574)
f8348ce9c8,fx,Untopiced,graceful failure for draw_graph() in acc_utils.py (#66631)
871a31b9c4,jit,Untopiced,[TensorExpr] Add missing schemas for lshift/rshift lowerings. (#66653)
20aa417e38,quantization,Untopiced,[PyTorch] [Quantization] Speed up PackedEmbeddingBagWeight::prepack() (#66632)
22ec625028,fx,Untopiced,fx2trt example: run all submodules (#66590)
a7b79033ea,jit,Untopiced,Clean up `ListLiteral` and `ListComprehension` emission logic (#64952)
a1084401b0,jit,Untopiced,Clean up `DictLiteral` and `DictComprehension` emission logic (#64953)
583217fe37,jit,Untopiced,changes for pytorch issue 55577 (#66571)
23710e2d80,skip,Untopiced,[PyTorch] Implement improved version of gather_ranges_to_dense (#66664)
b60050e96a,quantization,Untopiced,[qat]Make sure the bn statistics are the same in the unit test. (#66244)
70fc60b9d1,jit,Untopiced,Revert D31325860: [PyTorch] Implement improved version of gather_ranges_to_dense
8dcf84069e,jit,Untopiced,[PyTorch] Implement improved version of gather_ranges_to_dense (#66677)
59b28063b4,jit,Untopiced,[NNC] Adding more python bindings for missing operators (#66612)
06fa6c15c0,distributed,Untopiced,"Back out ""Revert D31299350: Back out ""Revert D31005792: [NCCL] Init dummy NCCL comms in constructor"""" (#66393)"
3740a06712,distributed,Untopiced,[MonitoredBarrier] Fix some logging (#65771)
1e47181c47,distributed,Untopiced,[DDP Logging] Add iteration in error reporting (#65772)
76efbccc3b,mobile,Untopiced,[PyTorch Edge][tracing-based] Unify tracer between internal and external (#64152)
a3d12bcdf9,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
8a01bbd64a,distributed,Untopiced,add flatten parameter module (#66578)
65e25256c3,releng,Untopiced,[ROCm] enable test_distributed() in test.sh (#66657)
06c37876b8,linalg_frontend,Untopiced,`torch.linalg.householder_product` faster backward (#63880)
3a9259f6cf,jit,Untopiced,[TensorExpr] Add missing schema for aten::where and aten::pow lowerings. (#66688)
2506baf9c2,onnx,Untopiced,[ONNX] move CheckerError from torch.onnx.utils to torch.onnx (#66644)
6436bd3d5d,python_frontend,Untopiced,Clarify topk doc (#65938)
7400f34b8e,skip,Untopiced,Add meta support to tensor range factories (#66630)
f8f9a47b02,quantization,Untopiced,PR3: add a workaround for reference path (#66535)
e88d1c4f10,distributed,Untopiced,[PyTorch] Add tuple inline storage (#64066)
e48a4cbf64,jit,Untopiced,Make several methods of SharedParserData private (#66670)
a58852fd44,fx,Untopiced,Fix fx2trt broken unit test (#66696)
0b8dc0f04a,python_frontend,Untopiced,"add BFloat16 operators on CPU: logaddexp, logaddexp2, remainder (#63621)"
abc022f9c8,linalg_frontend,Untopiced,Fix torch.cholesky deprecation warning (#66645)
bd25f92e81,skip,Untopiced,Fix Wextra issues in Half.h (#66643)
b5b7d6a3a6,cuda,Untopiced,EmbeddingBackward exclusive_scan thrust->cub (#66566)
687c2267d4,skip,Untopiced,use irange for loops (#66234)
a25648953c,python_frontend,Untopiced,Add `warn_only` kwarg to `use_deterministic_algorithms` (#66233)
d1b6121935,skip,Untopiced,Revert D31656999: Add meta support to tensor range factories
67e003f09b,jit,Untopiced,[Static Runtime] Determine function for `ProcessedNode::run()` statically (#66692)
06cfdfae0e,python_frontend,Untopiced,Promote integral inputs to floating for `torch.logsumexp` (#63393)
2c761caaaa,vulkan,Untopiced,[Vulkan] cat operator for channel dimension (#66669)
9ba39d2008,releng,Untopiced,Clean up test running scripts (#65508)
1e2b2ee5ff,cuda,Untopiced,sort_out_cuda: Use custom kernels to fill index tensors (#66668)
2f099c7555,linalg_frontend,Untopiced,Revert D30652629: use irange for loops
65adf1dfa2,skip,Untopiced,Migrate THCState to ATen (#66480)
32ac001e4d,skip,Untopiced,Suppress deprecated copy in vec256_qint.h (#66646)
42f138469a,jit,Untopiced,[TS] Return early if device doesn't match (#66694)
8c5928bd78,package,Untopiced,add frozen_numpy as a builtin library to torch::deploy (#66297)
aa0c31876b,skip,Untopiced,Remove THCGeneral.cpp (#66391)
962c6476da,mobile,Untopiced,"Refactor: move method to func compilation work to compileMethod, add option to specify method name (#66726)"
9e3a2babfa,mobile,Untopiced,Make aotCompile support multiple input sizes (#66727)
8854817f44,python_frontend,Untopiced,Implement Python Array API `asarray` function. (#60627)
719d43a2a2,skip,Untopiced,Revert D31547709: Remove THCGeneral.cpp
3b4cb9ddca,skip,Untopiced,Revert D31577488: Migrate THCState to ATen
53aac4b6f3,skip,Untopiced,[PyTorch] Allow override for macro `HAS_DEMANGLE` (#66540)
0d203a16fe,linalg_frontend,Untopiced,"Add relative and absolute tolerances for matrix_rank, pinv (#63102)"
811f5a2b94,dataloader_frontend,Untopiced,Adding StreamWrapper to ensure file object will be closed (#66715)
e4a9ee8d42,jit,Untopiced,Deduplicate codegenOutputQuery to query maximum CUDA compute capabilities (#55901)
909694fd88,fx,Untopiced,Fix `nn.functional.max_poolNd` dispatch (for arg: `return_indices`) (#62544)
aa7da7b09c,quantization,Untopiced,[quant][embedding qat] Enable quint4 in EmbeddingBag QAT workflow (#66348)
9d287d0b63,fx,Untopiced,[fx2trt]Add support for negative dim in softmax (#66760)
d549c8de78,quantization,Untopiced,fx quant: enable linear-bn1d fusion for PTQ (#66484)
8532061bce,distributed,Untopiced,[sharded_tensor] support gloo/mpi backend in tests (#65855)
f4a7273b5c,build_frontend,Untopiced,Set test owners for module: ci (#66796)
62e89f692f,autograd_frontend,Untopiced,[doc] typo (#66754)
09c4e73c95,skip,Untopiced,[PyTorch] Fix unnecessary shared_ptr copies in FutureType (#66704)
eb1eefc399,skip,Untopiced,[PyTorch] Fix unnecessary shared_ptr copies in DictType (#66702)
d777e490a5,quantization,Untopiced,[bc-breaking][quant][graphmode][fx] Produce reference patterns for GeneralShapeOps (#66647)
472a6f2787,fx,Untopiced,"Strided masked reductions: sum, amax. Testing of masked reductions. (#65990)"
c373e188d8,skip,Untopiced,[PyTorch] Fix extra refcount bumps in unifyTypes (#66718)
6bde474066,skip,Untopiced,[PyTorch] Fix extra refcount bumps in matchTypeVariables (#66719)
7fad47e522,autograd_frontend,Untopiced,`torch.linalg.lstsq`: forward/backward AD support (#65054)
624ce95201,distributed,Untopiced,Run sparse tests only for TensorPipe agent. (#66661)
1cf317b85f,onnx,Untopiced,[ONNX] Support exporting with Apex O2 (#65374) (#66700)
9b729ebc88,quantization,Untopiced,[jit] shape propagation for quantization (#66343)
d5a25faf7a,skip,Untopiced,[PyTorch] Fix unnecessary shared_ptr copies in EnumType (#66714)
393299b124,skip,Untopiced,[PyTorch] Fix unnecessary shared_ptr copies in RRefType (#66706)
1fcbd8fa15,skip,Untopiced,[PyTorch] Fix extra refcount bumps in tryEvalTypeVariables (#66722)
8637556d23,skip,Untopiced,Migrate THCState to ATen (#66765)
9ea3424747,fx,Untopiced,Set test owner for fx (#66807)
6a7296be9c,skip,Untopiced,[PyTorch] Use castRaw in InterfaceType (#66728)
622e19b859,jit,Untopiced,[PyTorch] Take const Type& in TensorType::fromNumberType (#66716)
44fd312604,dispatcher,Untopiced,[PyTorch] Use intrusive_ptr to save space in KernelFunction (#65618)
0974215c4d,linalg_frontend,Untopiced,"Prefer mT and mH over transpose(-2, -1) and transpose(-2, -1).conj() (#64181)"
c994a7fc2d,cpp_frontend,Untopiced,Update documentation of torch.nn.Upsample (#66756)
06e49ea088,quantization,Untopiced,[not4land][quant][fx][graphmode] lower reference linear module example (#65723)
d024f1134d,releng,Untopiced,ci: Move bazel download from github -> s3 (#66815)
8173d4df69,distributed,Untopiced,move get_cycles_per_ms() to common_utils (#66798)
1db50505d5,cpp_frontend,Untopiced,[nn] MultiLabelSoftMarginLoss : no batch dim support (#65690)
f65b4b7a4c,jit,Untopiced,[PyTorch] Avoid refcount bump in UnionType::canHoldType (#66693)
d0a63c978b,jit,Untopiced,[PyTorch][easy] Don't copy string in TensorType::repr_str unnecessarily (#66699)
c9c447f4be,jit,Untopiced,[PyTorch] Fix missing moves in ListType (#66701)
a17a4e93ce,jit,Untopiced,[PyTorch][easy] Fix missing move in UnionType::createWithContained (#66691)
d05c1ec007,skip,Untopiced,Add lazy Node base and associated infra (#66601)
c9c52b760b,linalg_frontend,Untopiced,test addr type promotion in a single test (#66812)
08a464a9f3,jit,Untopiced,[PyTorch] Pass c10::optional<bool> to Stride ctor by value (#66698)
061baf02bf,linalg_frontend,Untopiced,Skip failing tests when LAPACK and MAGMA are not available (#64930)
bceb1db885,mobile,Untopiced,use irange for loops 3 (#66747)
e0643fa3fc,cpp_frontend,Untopiced,use irange for loops 5 (#66744)
bff64e84cd,distributed,Untopiced,[DDP] Track models with sync bn (#66680)
147f7559b1,jit,Untopiced,Add `SourceView` which doesn't own source text as base class of `Source` (#65309)
72803dbcfd,caffe2,Untopiced,[caffe2] Fix invalid vector accesses and polar() call (#66757)
0a07488ed2,linalg_frontend,Untopiced,use irange for loops 1 (#66741)
0036e41143,quantization,Untopiced,[quant][embedding qat] Add eager QAT test for EmbeddingBag+Linear model (#66334)
8f09292c5e,python_frontend,Untopiced,add `OpInfo` for `torch.nn.functional.pairwise_distance` (#65460)
1164118fc2,python_frontend,Untopiced,add `OpInfo` for `torch.nn.pixel_shuffle` (#65467)
9f782f8b35,python_frontend,Untopiced,add `OpInfo` for `torch.nn.pixel_unshuffle` (#65468)
05b6dc9d75,caffe2,Untopiced,Fix BatchMatMul test and shape inference (#66733)
9eab6da887,nn_frontend,Untopiced,[skip ci] Set test owner for nn tests (#66850)
39215ddf84,dataloader_frontend,Untopiced,[skip ci] Set test owners for dataloader tests (#66839)
299a6a65b2,build_frontend,Untopiced,[skip ci] Set test owners for autograd tests (#66834)
c806bb1022,build_frontend,Untopiced,[skip ci] Set test owner for test_complex.py (#66835)
fd608cd313,build_frontend,Untopiced,[skip ci] Set test owners for optim tests (#66861)
a2e94b80fa,linalg_frontend,Untopiced,Create linalg.matrix_exp (#62715)
17f07c310b,quantization,Untopiced,Fix type checking errors in torch/ao/quantization/quantize_fx.py (#66804)
94afbd158c,build_frontend,Untopiced,[skip ci] Set test owner for test_numpy_interop.py (#66851)
34051d74da,releng,Untopiced,Add test owner to distributed files starting with test_ (#66797)
c9d9244166,build_frontend,Untopiced,[skip ci] Set test owner for test_spectral_ops.py (#66843)
c37f413e75,quantization,Untopiced,[skip ci] Change pretrained to false for quantization tests (#66795)
50f5689d60,build_frontend,Untopiced,Set test owner for distributions tests (#66842)
7e81a89e13,jit,Untopiced,[PyTorch] Fix performance-no-automatic-move clang tidy warnings in matchTypeVariables (#66720)
552af8bdef,jit,Untopiced,[PyTorch] Fix missing move in OptionalType::createWithContained (#66697)
690c2a7076,cuda,Untopiced,masked_scatter: fuse mask count check into one kernel (#66871)
3488a85a76,sparse_frontend,Untopiced,Sparse CSR CUDA: fix input checks for `addmm` and `mm` (#66485)
57c596eb9e,package,Untopiced,add interactive_embedded_interpreter.cpp to the OSS build (#66352)
9a00910bf3,linalg_frontend,Untopiced,[skip ci] Set test owner for test_linalg.py (#66844)
66f241230d,mobile,Untopiced,"[PyTorch] Take const Type& in {tryS,s}calarTypeFromJitType (#66717)"
a1afb692f3,mobile,Untopiced,Fix metal issues with irange (#66877)
15f21eef5e,fx,Untopiced,[fx2trt]fix softmax test (#66885)
cbd7bac914,releng,Untopiced,Migrate clang5-mobile build to GHA (#66673)
e70b5d64f4,python_frontend,Untopiced,Change README getting started link to explicit instructions (#66828)
b1a6129e09,dataloader_frontend,Untopiced,Add repr to StreamWrapper (#66880)
bd4d5cb14c,sparse_frontend,Untopiced,Sparse CSR: Add torch.empty (#63509)
b3bb234e16,skip,Untopiced,Remove THCGeneral.cpp (#66766)
16d0896b69,skip,Untopiced,[JIT][Easy] Shape cleanups (#65148)
5db7db667f,skip,Untopiced,[JIT] Add partial evaluation graph stitching logic (#65377)
853fc25fb0,skip,Untopiced,Fix bug preventing optimization from firing (#65573)
66543f88de,skip,Untopiced,Add x + 0 optimization (#65574)
cc7de1df3b,skip,Untopiced,Add Handling of Cat in Shape Analysis (#65575)
0fdc9b77a3,skip,Untopiced,Add support for multi output nodes in partial eval graph stitching (#66097)
b4db5174fe,skip,Untopiced,Add support for cat in output stitching (#66098)
de4fe7a38c,skip,Untopiced,Add Initial NNC Dynamic Shapes Flow (#66136)
32e3003726,distributed,Untopiced,"Have test classes extend from common_utils.TestCase, not unittest.TestCase (#66900)"
deb6989880,fx,Untopiced,[fx-acc] add optimize_quantization to FX graph opts (#65929)
9c4d7d96db,releng,Untopiced,Address feedback from #66673 (#66905)
1bf0e1acb4,jit,Untopiced,Revert D31732414: Add Initial NNC Dynamic Shapes Flow
4187d870df,jit,Untopiced,Revert D31732415: Add support for cat in output stitching
57fcea9e88,jit,Untopiced,Revert D31732418: Add support for multi output nodes in partial eval graph stitching
e730752610,jit,Untopiced,Revert D31732416: Add Handling of Cat in Shape Analysis
b8d58129bb,jit,Untopiced,Revert D31732420: Add x + 0 optimization
90b42452e2,jit,Untopiced,Revert D31732417: Fix bug preventing optimization from firing
70c9eb130d,jit,Untopiced,Revert D31732419: [JIT] Add partial evaluation graph stitching logic
ef15691a1e,jit,Untopiced,Revert D31732421: [JIT][Easy] Shape cleanups
14ee608791,distributed,Untopiced,[PyTorch] Make rearragement in sharded linear work as expected. (#66603)
79803b199f,jit,Untopiced,[Static Runtime] Make sure ProcessedNode::function_kind_ is copied over (#66917)
ed5633d0c5,skip,Untopiced,opinfo : nn.functional.embedding (#66622)
c69e33bb11,python_frontend,Untopiced,Fix doc string for torch.acosh (#66814)
867ccc9987,sparse_frontend,Untopiced,Strided masked reduction: amin. (#66385)
62ca5a81c0,nn_frontend,Untopiced,Exposed `recompute_scale_factor` into nn.Upsample (#66419)
3fe2ff800c,nn_frontend,Untopiced,Module docs update (#66909)
f95fef7897,jit,Untopiced,Add prim::TensorExprDynamicGuard to bc allowlist (#66939)
94f4b22df9,fx,Untopiced,Revert D31761594: [pytorch][PR] opinfo : nn.functional.embedding
8a65047acc,build_frontend,Untopiced,[skip ci] Set test owners for everything considered with module: tests (#66865)
452b359c3f,build_frontend,Untopiced,[skip ci] Set test owners for tensor creation tests (#66864)
409364e597,build_frontend,Untopiced,[skip ci] Set test owners for test_typing.py (#66869)
822277f302,build_frontend,Untopiced,[skip ci] Set test owners for test_type_promotion.py (#66866)
a015964cf8,sparse_frontend,Untopiced,Strided masked reduction: prod. (#66386)
793f366e34,build_frontend,Untopiced,[skip ci] Set test owners for sparse tests (#66863)
e86d8323cb,jit,Untopiced,"[JIT] Add special cases for batch_norm, instance_norm in alias_analysis (#66554)"
5569d5824c,nn_frontend,Untopiced,Fix documentation of arguments for torch.nn.functional.Linear (#66884)
6e67150f57,build_frontend,Untopiced,[skip ci] Set test owner for test_mkldnn.py (#66845)
f56a1a59a3,package,Untopiced,Add simple backwards compatibility check for torch.package (#66739)
450221c534,sparse_frontend,Untopiced,Sparse CSR: Add tensor.resize_ and tensor.copy_ (#63510)
257239972c,visualization,Untopiced,Fix attr_to_scope's key in `torch/utils/tensorboard/_pytorch_graph.py` (#65692)
08cb31a03e,distributed,Untopiced,[PyTorch][1/N] Basic implementation of ShardedEmbedding using ShardedTensor. (#66604)
18bbc4c2b7,cpp_frontend,Untopiced,[Static Runtime] Fix a bug in aten::index (#66940)
e046386be8,skip,Untopiced,Avoid inlining error reporting in checked_convert (#66721)
4ad6c144f6,skip,Untopiced,[JIT][Easy] Shape cleanups (#65148)
63b41e1f4d,skip,Untopiced,[JIT] Add partial evaluation graph stitching logic (#65377)
b059f035be,skip,Untopiced,Fix bug preventing optimization from firing (#65573)
eaba976d49,skip,Untopiced,Add x + 0 optimization (#65574)
0196b984f3,skip,Untopiced,Add Handling of Cat in Shape Analysis (#65575)
2dd23ebfdb,skip,Untopiced,Add support for multi output nodes in partial eval graph stitching (#66097)
17889ad26e,skip,Untopiced,Add support for cat in output stitching (#66098)
ab1e4eac42,jit,Untopiced,[Static Runtime] Add FuseListUnpackV2 (#66509)
db4165892b,mobile,Untopiced,[SmartCompose][OnDevice]fix function name bug in mobile export & Script to convert mobile model (#66915)
a89851a0d9,quantization,Untopiced,[quant][fx][graphmode] Adding a new convert function that produces reference pattern by default (#66925)
f8f04d5424,quantization,Untopiced,[quant][graphmode][fx] Add support for single linear and conv2d (#66950)
8beabffac3,skip,Untopiced,[PyTorchEdge] Make aten function common to aten and torch_common (#66663)
bdb889aca1,jit,Untopiced,[nnc] Use a descriptive name for fused kernels when profiling (#66990)
b6df043f1f,distributed,Untopiced,Add torch.nn.init.uniform_ operator to ShardedTensor. (#63997)
70a5113e03,releng,Untopiced,[ROCm] update Magma for 4.3 release (#65203)
32e790997b,amd,Untopiced,[Rocm]Reduce severity of detected possible memory leak from assertion to warning (#65973)
f5c5ab2868,build_frontend,Untopiced,[skip ci] Set test owner for cpp-extensions tests (#66837)
960e3216a4,build_frontend,Untopiced,[skip ci] Set test owner for named tensor tests (#66849)
8cbdf49dce,quantization,Untopiced,[qnnpack] Remove conv_utils.h (#66605)
13b8599831,build_frontend,Untopiced,[skip ci] Set test owner for test_dispatch.py (#66840)
23321ba7a3,python_frontend,Untopiced,Fix bug [#66780]: wrong input to torch.is_floating_point (#66783)
d73b88b473,fx,Untopiced,Unsqueeze bug fix (#66889)
a7ec4b53d2,fx,Untopiced,Splitter: Transformer_encoder (#66952)
53cf7e844f,jit,Untopiced,[SR] Fix bug in FuseListUnpackV2 (#67021)
a33f341cee,releng,Untopiced,[ci] try setting MAX_JOBS on windows builds to reduce OOMs (#66986)
9d4549295d,onnx,Untopiced,ONNX export: propagate node metadata across passes (#45256)
40e5d31a52,python_frontend,Untopiced,Add OpInfo for torch.bincount (#65796)
ce6f4b3a02,distributed,Untopiced,Setup c10d extension Backend class attr the same way as builtin ones (#66991)
94f4e9a995,autograd_frontend,Untopiced,Enable warning tests for nondeterministic backward functions (#66736)
78f970568c,build_frontend,Untopiced,Add dummy op to use instead of searchsorted (#66964)
00a871c5c9,build_frontend,Untopiced,[skip ci] Set test owner for multiprocessing tests (#66848)
6f1ba16d6d,build_frontend,Untopiced,[skip ci] Set test owners for cpp test (#66836)
b07371f19c,build_frontend,Untopiced,[skip ci] Set test owners for serialization tests (#66862)
062ae8df0e,skip,Untopiced,Automated submodule update: tensorpipe (#65353)
892ac08a02,autograd_frontend,Untopiced,Do not generate not_implemented error for forward AD when input with tangent passed to non-differentiable function (#66926)
462f333c01,python_frontend,Untopiced,[numpy] add torch.argwhere (#64257)
fcfa06586d,skip,Untopiced,Wextra fix for NamedTensor.cpp (#66897)
f29e5220a6,skip,Untopiced,Revert D31474901: [pytorch][PR] [numpy] add torch.argwhere
6a224b3370,quantization,Untopiced,Set test owners for quantization tests (#66832)
33790c4e06,fx,Untopiced,Implement histogramdd on CPU (#65318)
b696d64ef4,build_frontend,Untopiced,Binaries without AVX512 kernels shouldn't report CPU Capability as AVX512 on machines with AVX512 support (#66703)
b40a940192,skip,Untopiced,Strided masked reduction: mean. (#66784)
2578de4851,build_frontend,Untopiced,[skip ci] Set test owner for test_cuda* tests (#66838)
20f08d23a0,skip,Untopiced,Revert D31838513: Strided masked reduction: mean.
35965869cf,skip,Untopiced,Enroll bowangbj@ to PyTorch distributed package (#67062)
28fac23409,cuda,Untopiced,Fixes CUDA vs CPU consistency for index_put_ when accumulating (#66790)
4fe8055b9f,vmap_frontend,Untopiced,made functorch not decompose by default (#66945)
77beccaedb,caffe2,Untopiced,Do not build PyTorch with caffe2 by default (#66658)
01ced45217,mobile,Untopiced,[iOS] Bump up iOS CocoaPods version to 1.10.0 (#67058)
8ea985f240,quantization,Untopiced,[quant][fx][graphmode] Rename files and functions for convert and add do_not_use suffix (#66955)
fe102b9888,mobile,Untopiced,diff tool (#66854)
d3fc3c4ded,linalg_frontend,Untopiced,Implement forward AD for linalg.matrix_exp (#62716)
5f58764d1d,mobile,Untopiced,[PyTorch Edge][type] Add type support for NamedTuple custom class (import) (#63130)
d1a5612a3e,cuda,Untopiced,remove accscalar from i0 and i0e (#67048)
3d7a344c5e,dataloader_frontend,Untopiced,Fix ArchiveReader to keep archive path (#67035)
051ea5ccbf,jit,Untopiced,[Static Runtime] Bundle function & function_kind to carry them together (#66974)
6e6ede2e70,jit,Untopiced,[JIT] Re-enable alias sensitive peepholes (#65860)
d9c4b3feab,nn_frontend,Untopiced,Do rowwisemoments computation in `float` for `half` `LayerNorm` (#66920)
391eb1dbe3,jit,Untopiced,[JIT] UseVariadicOp handles multiple lists (#66288)
40a8a50913,jit,Untopiced,Add static_runtime::fused_equally_split (#2)
f2582a59d0,jit,Untopiced,[SR] Add rvalue overload for operator() (#66648)
e8742f15cf,quantization,Untopiced,[quant][graphmode][fx] Add observation_type.py (#67063)
d1986a1cf5,skip,Untopiced,[BE] delete frontend.cpp (#66581)
53a163a015,onnx,Untopiced,[ONNX] Export nn.Module call as ONNX local function (#63589) (#66140)
136abf5aff,onnx,Untopiced,[ONNX] Update sum symbolic to handle dtypes (#64289) (#66141)
7a78f715a6,onnx,Untopiced,[ONNX] Add warning for inplace updates on tensor.shape in tracing mode (#63170) (#66142)
b18c298f24,onnx,Untopiced,ONNX: Delete or document skipped ORT tests (#64470) (#66143)
a0fc14c20f,onnx,Untopiced,[ONNX] Add diagonal symbolic (#64454) (#66144)
6f3f302d9f,onnx,Untopiced,[ONNX] Deprecate fold_if pass (#65697) (#66145)
2c0fe338da,onnx,Untopiced,[ONNX] Modify softplus symbolic to support beta!=1 (#65001) (#66146)
0bc9928f31,onnx,Untopiced,"[ONNX] Symbolic: dynamic input for OneHot, bool for Einsum (#65940) (#66147)"
1da628bdb7,onnx,Untopiced,[ONNX] Update slice process shape to support rank only inference (#65782) (#66149)
7379d4db20,skip,Untopiced,[BE] minor improvement to dist quantization (#66649)
a7bbf8814c,quantization,Untopiced,[quant][graphmode][fx] Move quant-fx2trt unittests to test_quantize_fx.py (#67064)
7e5aa0d35a,python_frontend,Untopiced,fixed unique arguments documentation (#66132)
018e06edca,distributed,Untopiced,[torchelastic] Skip tests in tsan mode (#67103)
7b0408684b,skip,Untopiced,Fix linter (#67122)
f50bf16c04,distributed,Untopiced,Revert D31663043: [BE] minor improvement to dist quantization
f6c88fa99d,distributed,Untopiced,Revert D31627107: [BE] delete frontend.cpp
df3f82a1ef,distributed,Untopiced,"Add more FSDP unit tests to cover core logic, freezing weights and flatten parameter wrapper (#66904)"
cf3a5160f8,distributed,Untopiced,[BE] move init_multigpu_helper to common_distributed (#67050)
af1a2df825,cuda,Untopiced,enable better depthwise conv perf on cudnn 8.2+ (#58749)
3f5adf4f9c,quantization,Untopiced,[quant][graphmode][fx] Use the new convert function instead of the old one in quant-fx2trt tests (#67065)
83f70db95c,skip,Untopiced,Fix common device computation for comparison ops. (#66245)
d13829e6be,quantization,Untopiced,[quant][[fx] update observer_fqn to not depend on node.name (#66767)
8460fa5707,quantization,Untopiced,[quant][fx] Add an option in convert_fx to accept qconfig_dict to skip quantization (#66878)
2d81d5ab0a,quantization,Untopiced,[quant][graphmode][fx] Remove fbgemm_backend_config_dict for now (#67066)
64c68edaf3,python_frontend,Untopiced,[pt] Add Half precision support for bucketize and searchsorted op (#67077)
7b55dc8340,skip,Untopiced,Use kernel_func_name from aotCompiler (#66337)
313939c9c6,quantization,Untopiced,[quant] Fix lint errors (#67138)
b6fa998892,mobile,Untopiced,Revert D31514095: Use kernel_func_name from aotCompiler
580efb35a5,distributed,Untopiced,[FSDP] Add some comments after reading the code. (#66956)
0000c88e10,distributed,Untopiced,[FSDP] No need for list() in _get_shard (#66957)
b51731527d,distributed,Untopiced,[ez] [Docs] Missing import in example for post_local_sgd (#67047)
f1b5f1898b,skip,Untopiced,Automated submodule update: kineto (#67133)
a7ebf76a15,jit,Untopiced,jit trace (#59949)
364c4959c3,quantization,Untopiced,[quant] Fix docs error in convert_fx (#67152)
09c7771e9c,onnx,Untopiced,Set test owners for jit tests (#66808)
dd81fa9027,jit,Untopiced,[JIT] Freeze allows preservation of submodule attributes (#66102)
adc21f1966,quantization,Untopiced,[quant] Fix docs build (#67169)
6c985b57ff,fx,Untopiced,OpInfo : nn.functional.embedding (#66997)
edd4d246c3,nn_frontend,Untopiced,Accept 0-dim channel inputs in convolution layer (#66256)
364645cd9d,jit,Untopiced,[SR] Factor operator() implementation into separate function (#67125)
a0495b3cdb,jit,Untopiced,[SR] Remove unused operator() overload (#67001)
acb340de75,complex_frontend,Untopiced,[Pytorch][Bootcamp] Add fixes and vanilla testing for Adagrad non-vectorized and vectorized optimizers to handle complex numbers (#66671)
8d164a36fb,mobile,Untopiced,Use `at::native::is_nonzero` in promoted ops to improve portability (#67097)
5d9ff8f30e,jit,Untopiced,[Static Runtime] Add static_runtime::fused_sigrid_transforms (#66659)
d68bb50ef3,build_frontend,Untopiced,Disable SVE when cross-compiling for M1 (#67114)
9de0888891,package,Untopiced,Move the registration of CPython builtin modules to BuiltinRegistry (#67085)
ac948f4f35,releng,Untopiced,.github: Migrate linux-xenial-py3.6-gcc7 to GHA (#67072)
4dce051cb0,mobile,Untopiced,[jit][edge] Add control stack frame to lite interpreter (#65963)
f4dd88489a,linalg_frontend,Untopiced,Better and more consistent error messages in torch.linalg (#62734)
a6d0339492,mobile,Untopiced,[Pytorch Edge] Extend runtime compatibility to custom classes (#66972)
333717eaf0,fx,Untopiced,Improve assert failure message in test_get_torch_func_signature_exhaustive (#67039)
700b39a3df,sparse_frontend,Untopiced,Sparse CSR CUDA: add `torch.addmm` with all inputs sparse (#63511)
b8dfb45ac2,releng,Untopiced,Refactor cub namespace handling (#66219)
12daa4f663,mobile,Untopiced,[jit][edge] Enable CALL instruction in lite interpreter. (#65964)
4ac8d06911,quantization,Untopiced,[quant] Remove unused print in quantization_patterns.py (#67191)
239b38268b,fx,Untopiced,[fx2trt] Better trt layer name (#67200)
059ae96007,jit,Untopiced,[jit] Factor findAllNodes into one place. (#65965)
0acc21b412,vulkan,Untopiced,[vulkan] Add 2D transposed convolutions (#67104)
fa7fb7b4d9,build_frontend,Untopiced,[skip ci] Set test owner for test_profiler.py (#66831)
1f55dd83ac,skip,Untopiced,[WIP] wrap XLATensors into Python XLA wrapper class (#65841)
0d7d446154,jit,Untopiced,Disallow annotations on instance attributes outside __init__ (#67051)
7acf0c6d4b,mobile,Untopiced,[PyTorch Edge][type] Add type support for NamedTuple custom class (export) (#62612)
ad5731cacc,performance_as_product,Untopiced,[PyTorch] Add flop count for bmm and baddbmm (#66636)
ecf7e96969,mobile,Untopiced,"[Light] Remove ambiguity from compile_spec names, use actual output type (#67209)"
bfcde08612,fx,Untopiced,[trt] Algorithm recorder/replayer (#4)
3596e13d45,distributed,Untopiced,Add torch.nn.init.normal_ and torch.nn.init.kaiming_uniform_ ops to ShardedTensor (#67057)
0e371e413d,fx,Untopiced,[fx-acc] add automated graph opt testing using AccOpProperty (#67228)
b96337cf47,package,Untopiced,add frozen_pyyaml as a builtin library to torch::deploy (#67127)
31bcfa3760,distributed,Untopiced,[sharded_tensor] refactor sharded_tensor file structure (#67199)
0c1b7545b6,jit,Untopiced,[Static Runtime] Add more debug info to verify_no_memory_overlap() (#67206)
38cbaeb8a4,onnx,Untopiced,Update deprecated import paths. (#67250)
83355f9537,jit,Untopiced,[SR][easy] Alias for c10::Symbol::fromQualString (#67162)
a6d702a3ee,releng,Untopiced,add support for ubuntu 20.04 to CI docker images (#66942)
49251d05ec,build_frontend,Untopiced,[skip ci] Set test owners for NNC tests (#66833)
71b7182ee2,package,Untopiced,[skip ci] Set test owner for deploy/package tests (#66830)
2abffaf050,distributed,Untopiced,Consolidate c10d and dist imports in test_c10d_common.py (#67203)
5b345e767e,quantization,Untopiced,QNNPACK: Update to use pytorch/cpuinfo.git repo as a third party dependency (#67106)
3c61700cf7,autograd_frontend,Untopiced,`torch.linalg.householder_product`: forward AD support (#67043)
129e99fbce,python_frontend,Untopiced,__getitem__: Ensure Tensor subclasses are not treated as tuples (#67202)
828a9dcc04,cpp_frontend,Untopiced,[nn] MarginRankingLoss : no batch dim (#64975)
204ffd33ee,linalg_frontend,Untopiced,[CUDA][Linalg] Add gesvd as SVD fallback; optimize SVD gesvdj performance (#64533)
d47a9004c8,mobile,Untopiced,[skip ci] Set test owner for mobile tests (#66829)
49bf24fc83,nn_frontend,Untopiced,Updated error message for nn.functional.interpolate (#66417)
d7ac6e977a,distributed,Untopiced,Fix test_create_store_multi flaky test (#66953)
7052c41899,releng,Untopiced,.github: Add workflow to build all docker images (#67215)
34ee5b11ff,releng,Untopiced,.github: Add 4xlarge nvidia gpu to scale-config (#67262)
6c22b96082,mobile,Untopiced,[Pytorch Edge] Extend Tracer to Custom Classes (#67004)
a33d3d84df,skip,Untopiced,Strided masked reduction: mean (2nd try) (#67088)
1ce500f56f,cpp_frontend,Untopiced,[easy][PyTorch] Use `at::native::is_nonzero` (#67195)
99b34b320b,jit,Untopiced,"Make fb::sigrid_hash_compute_multipler_shift return a std::tuple<int64_t, int64_t> (#67123)"
197dec14b3,releng,Untopiced,.github: Change periodic docker jobs to always_rebuild (#67267)
2ca552160b,distributed,Untopiced,[DDP] logging improvements (#67059)
273ab55fc4,sparse_frontend,Untopiced,Revert D31914868: Strided masked reduction: mean (2nd try)
1926156752,distributed,Untopiced,Prevent TCPServer get deleted too early (#67204)
066a980e7b,jit,Untopiced,[PyTorch][Static Runtime][easy] Fix ValueGroup comment (#66965)
6ce14e7b51,jit,Untopiced,[PyTorch][Static Runtime] Cleanup: add valueVecFromFastSet (#66996)
fdc74e2373,linalg_frontend,Untopiced,Port triangular_solve to structured kernel (#61857)
81d188101f,releng,Untopiced,.github: Use 4xlarge instances for linux gpu (#67264)
a72a6365c9,autograd_frontend,Untopiced,disallow requires_grad=True in make_tensor for integral inputs (#67149)
f510193e22,jit,Untopiced,[jit][edge] Export maybe-used interface methods from modules. (#65966)
00b0d4eeed,skip,Untopiced,Add register_frozenpython.cpp to the torch::deploy interpreter library in the OSS build (#67280)
0e8bd0c8d6,jit,Untopiced,[Pytorch Delegated Backend] Add macro to define sentinel value of debug handle. (#66584)
7ac8ed741d,skip,Untopiced,fix binding to the wrong python module (#67246)
1541bb823a,skip,Untopiced,.github: Switch 8xlarge to 4xlarge instance_type (#67299)
fcefed9517,package,Untopiced,Revert D31935958: Add register_frozenpython.cpp to the torch::deploy interpreter library in the OSS build
dfa7225a38,complex_frontend,Untopiced,[Pytorch][Bootcamp] Add fix and testing for non-vectorized Adadelta optimizer to handle complex numbers (#66587)
d691bc1207,python_frontend,Untopiced,Revert D31937065: [pytorch][PR] fix binding to the wrong python module
cdc9b26281,vulkan,Untopiced,[Vulkan] Optimize cat operator for channel dimension (#67207)
28570664d5,vulkan,Untopiced,[Vulkan] Add vulkan_perf_test with google benchmark (#67230)
c88da701e2,fx,Untopiced,[hpc][inference] enable cuda graph in engine holder (#66738)
632719c214,distributed,Untopiced,Enable c10d trampoline tests on MacOS (#67205)
e52d0e773b,jit,Untopiced,[tensorexpr][ir][quant] Adding qscale and qzero to tensorexpr IR Buf (#66675)
a3aa9df59f,distributed,Untopiced,Add barrier to ProcessGroup trampoline (#67236)
f2f7b02b4c,vmap_frontend,Untopiced,Add support for vmap+fwdAD for basic out-of-place op (#66291)
7d1c0992e1,releng,Untopiced,GHA: add back runner type for distributed tests (#67336)
2669e4ed4e,releng,Untopiced,Revert D31945507: .github: Switch 8xlarge to 4xlarge instance_type
4b9464f4b9,fx,Untopiced,[fx]Early return if a node tries prepend self (#67068)
7c48b9ee25,sparse_frontend,Untopiced,Sparse CSR CUDA: add `triangular_solve_out` (#61858)
b55a2500d2,distributed,Untopiced,[jit] Remove graph() call from abstract Function interface. (#65967)
0101b1ea2b,releng,Untopiced,[skip-ci] .github: Set linux gpu instances to be non-ephemeral (#67345)
1ec732bc46,jit,Untopiced,Add fp16/fp32 autocasting to JIT/TorchScript (#63939)
72e25c9f4e,jit,Untopiced,[Static Runtime][DI] Add variadic grouped_accessor_op (#66289)
5347dab851,onnx,Untopiced,Set test owners for onnx tests (#66860)
fa70d72e95,mobile,Untopiced,Set kernel func name from AOT Compiler (#67229)
7484941eaa,fx,Untopiced,Wrap TRTInterpreter result with wrapper (#67307)
3a1aa31a2f,build_frontend,Untopiced,Add dummy bfloat16 VSX implementation (#67331)
9900310133,skip,Untopiced,Fix sign warnings in CUDA kernels (#66753)
d9a5668983,onnx,Untopiced,[ONNX] Add dim argument to all symbolic (#66093) (#67270)
708f7b1209,autograd_frontend,Untopiced,Update extending doc to cover forward mode AD (#66962)
2267a984eb,amd,Untopiced,[ROCm] Add sparse mappings for CUDA->HIP translation (#67323)
f20614af21,jit,Untopiced,[jit] Allow custom class functions to be traced in invokeScriptMethodFromPython(). (#67380)
dea8b27433,mobile,Untopiced,[Pytorch Edge] Make some torchbind classes selective (#67340)
9ebc6357b3,jit,Untopiced,[SR] Vectorize int version of fmod (#67313)
882446c1d2,package,Untopiced,add frozen_numpy to :builtin_registry_cuda target (#67396)
9e175400ac,python_frontend,Untopiced,Moving python binding to _C and its decl to the right pyi file (#67365)
1cfdb6f4c6,quantization,Untopiced,[quant][fx] add pass to duplicate dequant nodes with multi use (#67118)
c8dd90c858,jit,Untopiced,[PyTorch] Fix extra refcount bumps in ClassAttribute (#66723)
fae1c0a434,jit,Untopiced,[PyTorch] Reduce refcount bumps in ClassType (#66724)
7e1a53cd5c,jit,Untopiced,[Core ML] Fix error messages (#67410)
04aba42ed7,jit,Untopiced,[Core ML] Assign Core ML computationUnit to executor (#67411)
e332d80299,jit,Untopiced,[iOS][CoreML] Remove shape information from TensorSpec (#67412)
0117ada47c,quantization,Untopiced,[quant][graphmode][fx] Add input_idx_to_dtype and ouptut_idx_to_dtype to backend_config_dict (#67067)
71a67d0ce9,skip,Untopiced,[sharded_tensor] simplify init_from_local_shards API (#64481)
fc664ac272,distributed,Untopiced,[sharded_tensor] easier initialization for Shard (#66351)
90b722c544,jit,Untopiced,specializeGradSumToSize patch - propagate profile_none through profile_ivalue (#63941)
6827d36c1a,jit,Untopiced,[Static Runtime][DI] Fuse list unpack and variadic_grouped_accessor_op (#66585)
62feadd76f,skip,Untopiced,Replace issue templates with new issue forms (#65917)
3aadff651c,quantization,Untopiced,[quant][embedding qat][bugfix] Fix and test QAT EmbeddingBag from_float error message (#66989)
7da9c4ed2e,jit,Untopiced,[SR] NNC out variant for aten::where (#67255)
eea20bfa15,quantization,Untopiced,fixed type checking errors in fuse.py (#66799)
9deb602726,onnx,Untopiced,"[ONNX] Use Reciprocal operator instead of Div(1, x). (#65382) (#67271)"
02a78bdba7,onnx,Untopiced,[ONNX] Support conv-bn fusion in blocks (#66152) (#67272)
961fd76a9a,onnx,Untopiced,[ONNX] Relax check on Prim::PythonOp nodes for ONNX_FALLTHROUGH (#66172) (#67273)
6293e0ad61,skip,Untopiced,update coverage ignore to not skip whole modules (#67395)
2366948085,skip,Untopiced,[LT] Add ir_util for ComputePostOrder (#67282)
eb8b80b76f,releng,Untopiced,Add test owners for elastic tests (#67293)
6900aacf54,performance_as_product,Untopiced,[fbcode] Fix operator_benchmark with jit mode (#67382)
d3f03af496,skip,Untopiced,Fix indentation in forward_grad.h (#67359)
b27b1ff809,autograd_frontend,Untopiced,Fix deadlock when forward and backward AD are used at the same time (#67360)
6ed68f3f84,jit,Untopiced,Document `torch.jit.is_tracing()` (#67326)
0c93c8e39a,releng,Untopiced,Disable linux-xenial-cuda10.2 config (#67344)
938afa37a3,distributed,Untopiced,Remove process group barrier and all_reduce function calls from tensorpipe agent (#65946)
325b15039c,distributed,Untopiced,Add FSDP tests to verify forward overlap and memory usage (#67117)
a8f85300da,quantization,Untopiced,[quant][graphmode][fx][test] Refactor test code for quant-fx2trt unit tests (#67070)
b100a9ea82,jit,Untopiced,"Back out ""Make fb::sigrid_hash_compute_multipler_shift return a std::tuple<int64_t, int64_t>"" (#67456)"
665c148e42,skip,Untopiced,move some codegen utilities into utils.py (#63094)
03f3a0331b,python_frontend,Untopiced,add slice/select/diagonal_scatter variants as primitive ops (#64430)
b0a8ca2cb5,cpp_frontend,Untopiced,add tags for inplace view ops in native_functions.yaml (#65412)
0032fa7725,skip,Untopiced,Add a Functionalization pass in core (#64432)
55b7387e45,fx,Untopiced,Timing cache for Tensort (#67214)
cee4e8f35d,build_frontend,Untopiced,Add FlexiBLAS build support per #64752 (#64815)
4e873d6799,distributed,Untopiced,Formatting changes (#66257)
4a1f73ccb3,quantization,Untopiced,[qnnpack] Remove asymmetrical padding parameters in qnnpack (#67102)
f3aae62942,python_frontend,Untopiced,Port `tril` and `triu` to structured kernels (#67055)
5b8b2382d1,python_frontend,Untopiced,Mark mv as CompositeExplicitAutograd (#67373)
bd5e6fe5ac,build_frontend,Untopiced,Skip complex128 dtype for test_addmm_sizes_all_sparse_csr Windows test (#67453)
0795735351,jit,Untopiced,[jit] Clean up unneeded virtual methods from Function interface. (#65968)
b8f07689f2,amd,Untopiced,[ROCm] Enable frexp support for ROCm builds (#67226)
8363da3f92,jit,Untopiced,[SR][C2][easy] Benchmarks report # of ops (#67436)
5ef62c88a9,jit,Untopiced,[jit] Replace get_executor() with call() in abstract Function interface. (#65969)
60472594e1,mobile,Untopiced,[jit][edge] Implement torch::jit::Function for mobile funciton. (#65970)
afb8434440,jit,Untopiced,[SR] Native implementation for aten::view (#67341)
82c356505f,releng,Untopiced,Revert D31894777: [pytorch][PR] Replace issue templates with new issue forms
9f01937caf,jit,Untopiced,[PyTorch][easy] Deduplicate memory planner creation code (#67265)
354363b57a,jit,Untopiced,[SR] Native implementation for aten::size (#67346)
6696c59af4,nn_frontend,Untopiced,Adding `optimizer` attribute to SequentialLR (#67406)
d0bc01fac2,releng,Untopiced,ci: Migrate hardcoded docker builds to GHA (#67455)
2661507488,jit,Untopiced,Adding support for Symbolic Shapes in Inplace Ops #65642 (#65729)
fc82ad186a,skip,Untopiced,Add Initial NNC Dynamic Shapes Flow (#66136)
d8bde98f36,fx,Untopiced,Workaround the bug of TRT which creates extra outputs (#67327)
7c2d3e6d32,caffe2,Untopiced,Wextra caffe2/ (#67319)
609da98154,onnx,Untopiced,[ONNX] Update value name copying logic for onnx (#66170) (#67275)
40920185ac,onnx,Untopiced,[ONNX] Remove the argument enable_onnx_checker of export() method entirely. (#66611) (#67276)
43d51254bf,onnx,Untopiced,Deprecate the argument _retain_param_name of export() method entirely. (#66617) (#67277)
26241994b2,onnx,Untopiced,Remove the argument strip_doc_string of export() method entirely. (#66615) (#67278)
18807273cb,skip,Untopiced,Fix Ads build broken due to comparison type mismatch (#67526)
4052393af8,caffe2,Untopiced,Revert D31450501: Wextra caffe2/
96c868217c,package,Untopiced,[deploy] fix TypedStorage serialization (#67499)
5e46a4f6bd,fx,Untopiced,Fixes to make trt timing_cache really work (#67524)
2bb20c0e48,quantization,Untopiced,[quant] Move test file to fx2trt folder (#67129)
acdc754918,quantization,Untopiced,[quant][graphmode][fx] Add support for ObservationType.OUTPUT_SHARE_OBSERVE_WITH_INPUT in backend_config_dict (#67210)
99282126dc,quantization,Untopiced,pytorch quantization: document the custom module APIs (#67449)
d58f209326,quantization,Untopiced,add dequantize support for fp16 + cuda (#67234)
f95ed474ac,python_frontend,Untopiced,Norms Op Info (#67442)
2e156f649e,skip,Untopiced,Sort output of *NativeFunctions.h (#67046)
c6a6c09383,python_frontend,Untopiced,Add OpInfo for `torch.nn.functional.gaussian_nll_loss` (#67356)
1d900ee22f,python_frontend,Untopiced,Add OpInfo for `nn.functional.hinge_embedding_loss` (#67381)
8b8fb4f4e6,python_frontend,Untopiced,Add OpInfo for `nn.functional.gaussian_nll_loss` (#67376)
e2e20e79fb,python_frontend,Untopiced,Add OpInfo for `nn.functional.poisson_nll_loss` (#67371)
bcd301a457,python_frontend,Untopiced,Add OpInfor for `nn.functional.ctc_loss` (#67464)
bf31995194,python_frontend,Untopiced,Add OpInfo for `nn.functional.cosine_embedding_loss` (#67465)
16d937b0df,nn_frontend,Untopiced,Fix strided _conv_double_backward() with 3D input / weight (#67283)
ddc9bd335b,python_frontend,Untopiced,Adds reference vs. noncontiguous OpInfo test (#67434)
285d5a55b9,distributed,Untopiced,Add API usage to torch.RPC (#67515)
69f86ecd3a,sparse_frontend,Untopiced,Sparse CSR CUDA: add `torch.add` with all inputs sparse (#63948)
fcba8018c2,build_frontend,Untopiced,Update codeowners for sphinx conf (#67548)
c19cda5782,build_frontend,Untopiced,[skip ci] Add test owners for a special hi-pri class of tests (#67553)
6259601c8a,onnx,Untopiced,Set test owners for tests with unknown owners (#67552)
d6b15bfcbd,mobile,Untopiced,[jit][edge] Load interface methods to corresponding ClassTypes. (#65971)
12ede84dbb,mobile,Untopiced,[jit][edge] Enable lite interpreter to correctly handle INTERFACE_CALL instruction. (#65972)
60a80c5bbd,jit,Untopiced,[jit] Move ModuleIndex operator to selective build. (#67483)
66202b7f8d,mobile,Untopiced,[Pytorch Edge] Expose runtime operators versioning (#67385)
5c77ccefe0,python_frontend,Untopiced,Resolves #67227 documentation issue (#67379)
ba74b03b0d,distributed,Untopiced,"Back out ""[sharded_tensor] simplify init_from_local_shards API"""
289b0f7b04,cpp_frontend,Untopiced,Resent the reverted PR: Add register_frozenpython.cpp to the torch::deploy interpreter library in the OSS build (#67303)
2cac92f470,jit,Untopiced,[SR] Migrate sigrid_transforms_torch_bind to new FuseListUnpack (#67163)
ad89d994c9,jit,Untopiced,[Static Runtime] Support recordio format input for benchmark (#67530)
d4493b27ee,skip,Untopiced,make `TORCH_(CUDABLAS|CUSOLVER)_CHECK` usable in custom extensions (#67161)
69adbc8778,fx,Untopiced,Fix splitter_base and add unit test for trt splitter (#67569)
c00806beda,skip,Untopiced,Add skipXLA and expectedFailureXLA decorator (#66857)
4a2bbc619d,skip,Untopiced,move functionalize fallback out of aten/core (#67564)
aa16de517d,skip,Untopiced,Revert D31984694: [pytorch][PR] make `TORCH_(CUDABLAS|CUSOLVER)_CHECK` usable in custom extensions
b24c34426f,fx,Untopiced,Add OpInfo for torch.unique and torch.unique_consecutive (#67529)
a95c94f075,fx,Untopiced,[fx2trt] fix acc_tracer when run against module that contains ScriptModule submodules (#67567)
510e3026a9,python_frontend,Untopiced,[numpy] add torch.argwhere (#64257)
e01279cc2e,linalg_frontend,Untopiced,Disable reduced precision reductions for fp16 GEMMs (#67578)
d0662f2f76,python_frontend,Untopiced,Add adaptive_max_pool OpInfo (#67405)
97f29bda59,python_frontend,Untopiced,Relaxes tolerance on ROCm test_noncontiguous_samples_matmul (#67593)
7fbcf79684,quantization,Untopiced,[tensorexpr][nnc] Support quantization (#66676)
0d7cf825fc,jit,Untopiced,[SR] Drop support for aten::__is__ and aten::__isnot__ (#67550)
9cdd1d7e48,skip,Untopiced,Docs module check (#67440)
00da7b9a3b,vmap_frontend,Untopiced,Set test owner for vmap (#67582)
39ad7b670e,jit,Untopiced,[SR] Native implementation for aten::squeeze (#67441)
885a8e53ba,linalg_frontend,Untopiced,replace onlyOnCPUAndCUDA with onlyNativeDeviceTypes (#65201)
da29655797,mobile,Untopiced,Disable miopen test for convolution on mobile (#66564)
a122ba776a,skip,Untopiced,Fix less_than_lowest warnings (#67422)
4d99bc839b,skip,Untopiced,Remove TH/THC Storage functions for unused dtypes (#67480)
251278d385,distributed,Untopiced,[skip ci] set more tests with owners for distributed and elastic (#67583)
e80cb08cc8,jit,Untopiced,[jit][shape_prop] Fix jit registration of unpack_sizes ops for prepacked (#66737)
76f57cd442,skip,Untopiced,[CODEOWNERS] Remove @neginraoof (#67631)
ba369ea053,mobile,Untopiced,check to ensure profiler_edge is only added when use_kineto is on (#67494)
0cbfd466d2,distributed,Untopiced,Remove ProcessGroup from TensorPipeAgent initialization (#66708)
234bd6dc56,quantization,Untopiced,[quantized] Add bilinear quantized grid_sample (#66879)
45d5b3248b,cpp_frontend,Untopiced,Fixed C++ BatchNorm pretty_print() with optional momentum (#67335)
53e6aca8b3,quantization,Untopiced,[Pytorch Edge] Make More Classes Selective (#67397)
c4bf196334,skip,Untopiced,Strided masked reduction: mean (2nd try) (#67088)
152f665dee,cpp_frontend,Untopiced,Inserted check for PyObject_IsInstance in THPVariableCheck (#67588)
ec6b472e0a,vulkan,Untopiced,[vulkan] Add prepacking for conv2d_transpose (#67358)
c65f332da4,package,Untopiced,torch::deploy unity and its demo (#67134)
cd51d2a3ec,python_frontend,Untopiced,"Adding OpInfo for `logical_or`, `logical_and`, `logical_xor` (#67178)"
4061239fdd,skip,Untopiced,[qnnpack] Remove redundant fp16 dependency (#67281)
9e71ea292d,distributed,Untopiced,Fix test_init_pg_and_rpc_with_same_socket by retrying on addr in use error (#67638)
7cd62621fb,mobile,Untopiced,[PyTorch] Adopt faster Tuple::create (#65381)
d9bac7c316,jit,Untopiced,[PyTorch] Add IValue::toTupleRef() (#65504)
eb1b8a2160,releng,Untopiced,pytorch_android_gradle_custom_build_single migrated from Circle to GHA. (#67577)
82f7f8d471,quantization,Untopiced,[PyTorch] Adopt IValue::toTupleRef() where obvious (#65505)
2644725937,jit,Untopiced,[SR] Migrate gather_ranges_to_dense to new FuseListUnpack (#67164)
33d62266f2,mobile,Untopiced,[PyTorch][easy] Avoid allocating OperatorName strings in append_operator (#66134)
3e218dbd27,mobile,Untopiced,[PyTorch] Capture function args from schema by reference (#65951)
6b1d8e5bb2,quantization,Untopiced,Revert D31861962: [qnnpack] Remove redundant fp16 dependency
0ee8473af7,jit,Untopiced,[SR][easy] Fix FuseListUnpack 0-use corner case (#67165)
b00206d473,vulkan,Untopiced,[vulkan] Use 3D textures for everything (#67647)
5ad169b7cc,distributed,Untopiced,Adding in Wrap functions for FSDP from Fairscale (#67292)
9cef2033f3,fx,Untopiced,Modify decorator for acc op converters (#67636)
16c62a6dc9,quantization,Untopiced,[PyTorch Edge] Optimize Dequantize Tensor with Intrinsics (#65844)
92cfda1785,quantization,Untopiced,[PyTorch Edge] Clean up Quantize Tensor code (#66220)
23bd3cf5b2,quantization,Untopiced,[PyTorch Edge] Parallelize Quantize and Dequantize Tensor (#65845)
a831713786,quantization,Untopiced,[PyTorch Edge] Use Integer Subtraction (Instead of Float) in Non-FBGEMM Dequantization (#67115)
9e97ccbd7a,releng,Untopiced,.github: Migrate iOS workflows to GHA (#67645)
7deb1726ea,build_frontend,Untopiced,Remove native_functions.yaml dependency from ScanKernels.cu (#66620)
96e3d1a76c,sparse_frontend,Untopiced,Remove native_functions.yaml dependency from Sorting.cu (#66621)
0b2f68eadf,fx,Untopiced,Remove special FX OpInfo list (#67520)
885da61d7d,distributed,Untopiced,[PG NCCL] Disable NCCL health check (#67668)
16ee6409ee,python_frontend,improvements,Changed value constraint of exponential dist (#67184)
91971dfc2a,releng,Untopiced,[BE] [GHA] Use `aws ecr get-login-password` (#67709)
54241a9cfa,quantization,Untopiced,[quant][fx] Add support for fused modules in _convert_do_not_use (#67245)
18955d3564,distributed,Untopiced,Raise warning when calling collectives on non-member group objects (#67639)
e86a5a3a1a,jit,Untopiced,[Static Runtime] Add PyTorchPredictor::predict_managed_result to return managed output tensors (#65598)
06d1be2447,caffe2,Untopiced,[NOOP][clangformat][codemod] Enable CLANGFORMAT for caffe2/caffe2/* (#67624)
7f3326a6d2,distributed,Untopiced,[FSDP] CPU offload resubmit (#67249)
d352587210,jit,Untopiced,add a few convenience helpers to removeAllXXX to Block and Node (#67423)
a8757cdd70,jit,Untopiced,type inputs (#67424)
3db536e55e,jit,Untopiced,add jit_trace_module python binding (#67425)
510336499b,jit,Untopiced,[PyTorch][Static Runtime] Separate overlap checks for easier debugging (#66637)
e32d7f7525,mobile,Untopiced,ATen | Fix potential crash if `MTLCreateSystemDefaultDevice` return nil (#66859)
92a85ecbab,quantization,Untopiced,add a quantized hardsigmoid inplace variant (#65740)
05d1dcc14c,python_frontend,Untopiced,Split channels_last test cases for tensor conversion OpInfos (#67368)
ea4d983885,performance_as_product,Untopiced,"Modify ""gemm"" code to enable access to ""sbgemm_"" routine in OpenBLAS (#58831)"
d58ef2bbff,jit,Untopiced,[TensorExpr] Fix lowering for aten::softmax for the case when dtype parameter is None. (#66516)
008a58d226,jit,Untopiced,[TensorExpr] Add lowering for aten::conv1d. (#66517)
00afe9ba7b,jit,Untopiced,[TensorExpr] Add lowering for aten::embedding. (#66518)
ff5c61a74e,jit,Untopiced,[TensorExpr] Add lowering for aten::max (reduction). (#66519)
383c1f51b1,jit,Untopiced,[nnc] Fixed handling of 0-sized tensors in cat (#67734)
4a106e41e9,fx,Untopiced,[fx2trt] Add torch.nn.function.pad support for fx2trt (#67498)
1ffd43cf0c,releng,Untopiced,generated-pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-full-jit migrated to GHA (#67695)
201f7d330a,skip,Untopiced,Remove duplicate check in distributions arg validation (#67741)
a23814577b,distributed,Untopiced,Overload TestCase not vanilla TestCase for some elastic tests (#67700)
88c61b8d06,skip,Untopiced,"Added forward derivatives for neg, diag, inverse, linalg_eig (#67339)"
15a3c374e2,skip,Untopiced,[nnc] Add support for dynamic shapes in TensorExprKernel (#67197)
3f33ada8d5,releng,Untopiced,.github: Forward fix generating GHA workflows (#67777)
a5b57c9433,linalg_frontend,Untopiced,"Avoid prematurely casting GEMM parameters `alpha`, `beta` to `scalar_t` (#67633)"
89c4e8c22b,caffe2,Untopiced,[NOOP][clangformat][codemod] Enable CLANGFORMAT for some folders in caffe2/* (#67746)
6df0d7d502,skip,Untopiced,[lint] add basic lintrunner compatibility (#67110)
98be5216e2,autograd_frontend,Untopiced,"Revert D32104006: [pytorch][PR] Added forward derivatives for neg, diag, inverse, linalg_eig"
f455030931,quantization,Untopiced,Adding a docstring for memoryless in observer args (#67690)
6cc6a5fd9d,releng,Untopiced,Fix a bug in TorchBench ondemand CI. (#67743)
0eaa01ead1,jit,Untopiced,[SR] Add EliminateTrivialEquallySplit graph pass (#67166)
9b1caca185,jit,Untopiced,[SR] Macro to clean up c10::Symbol maps in passes (#67484)
fe91906ad7,releng,Untopiced,Remove Declarations.yaml dependency from gen_autograd (#67496)
4d601a1c36,skip,Untopiced,"codegen: Split up source, header and Declarations.yaml generation (#67497)"
89b02fc70b,skip,Untopiced,[StaticRuntime][Easy] Correct typos in test_static_runtime (#67739)
5fd93fb5f8,build_frontend,Untopiced,broaden retries on TestHub (#67779)
05e17e7ff6,distributed,Untopiced,Add API usage logging for several other RPC APIs. (#67722)
fddfb81dd0,jit,Untopiced,Add BF16 type to _autocast_to_full_precision (#67707)
603116a6ae,jit,Untopiced,[Core ML][easy] Assign missing properties to the executor (#67737)
ca445645f9,jit,Untopiced,Revert D31902471: [nnc] Add support for dynamic shapes in TensorExprKernel
83e8612d11,nn_frontend,Untopiced,Clean up test autograd (#67413)
fd77fff0b1,distributed,Untopiced,[FSDP] customizable backend in test (#67135)
2766662ca9,distributed,Untopiced,[PyTorch][2/N] Basic implementation of ShardedEmbeddingBag using ShardedTensor. (#67188)
88d86de7d8,releng,Untopiced,Add lint to ensure all test files have headers with ownership info (#66826)
2486061c72,jit,Untopiced,[JIT] make x (+ or -) 0 and x (* or /) 1 peepholes type promotion aware (#67688)
99c7a9f09d,skip,Untopiced,fix bfloat16 autocast skip (#67822)
1baed45c6b,quantization,Untopiced,[fbcode][static runtime] out-variant for quantized::linear_dynamic_fp16 (#67663)
b8d365ca3a,skip,Untopiced,ci fix (#67826)
04fe4382ec,skip,Untopiced,Automated submodule update: tensorpipe (#67769)
8e1ead8e4d,nn_frontend,Untopiced,Fix the kl_div docs (#67443)
cfd998c197,distributed,Untopiced,Remove ProcessGroup RPC backend placeholder as part of 1.11 (#67363)
61ed9285dd,skip,Untopiced,Automated submodule update: tensorpipe (#67845)
927da4d32f,build_frontend,Untopiced,Remove native_functions.yaml dependency from Sort.cu (#66793)
4262c8913c,build_frontend,Untopiced,Remove native_functions.yaml dependency from TensorTopK.cu (#66794)
bd8feb33d4,distributed,Untopiced,Update distributed contributing guide to show how to run one test in test_distributed_spawn (#67801)
af1bd88fc4,python_frontend,bug_fixes,"Allow scalars for aliased binary ops {`multiply`, `subtract`, `divide`} (#65937)"
8b0c2c18eb,onnx,Untopiced,Fix pretrained=True for test_pt_onnx_trt (#67818)
7c739e1ab9,build_frontend,Untopiced,Resubmit #67161 (#67735)
90d311b268,skip,Untopiced,[RPC] Add exception logging to constValue() (#67802)
618bab593c,releng,Untopiced,.github: Output expected vs. actual (#67703)
6f0a1f2b8d,releng,Untopiced,Only set sccache_epilogue to run on build job exits (#67798)
ea94dde573,skip,Untopiced,Move Concat Linear out of Optimize Numerics (#67196)
db15a7c0b3,skip,Untopiced,Fix Freezing Docs Parameters (#67201)
08d630b9a6,skip,Untopiced,Formatted with Black (#67792)
279af1a668,skip,Untopiced,Revert D32154787: Formatted with Black
86aea79217,jit,Untopiced,Revert D32154786: Fix Freezing Docs Parameters
3d4a6ff15d,jit,Untopiced,Revert D32154788: Move Concat Linear out of Optimize Numerics
da59bd1d13,skip,Untopiced,TST Adds device transfer into module info tests (#65488)
641ba36a4e,dataloader_frontend,Untopiced,fix annotation for Demultiplexer (#65998)
938bab0bfd,skip,Untopiced,[PyTorch] Add int version of vectorized PrefixSum to Benchmark (#67865)
e8ac8c005d,skip,Untopiced,[NOOP][clangformat][codemod] Enable CLANGFORMAT (#67854)
8bed46ef38,skip,Untopiced,[WIP][LTC] Upstream class Shape (#67672)
c541c69e89,skip,Untopiced,Fix minor typo in contributing.md (#67855)
a20a64af4e,distributed,Untopiced,Increased tolerance for test_zero_model_parallel tests (#67765)
d5d342b237,sparse_frontend,Untopiced,Sparse CSR CUDA: Support mixed memory format input for triangular_solve (#66401)
56dda833ff,skip,Untopiced,Small updates to RELEASE.md (#65489)
01809731bc,jit,Untopiced,[Static Runtime] Cache managed tensor Storages (#66638)
b0c05297f9,jit,Untopiced,[Static Runtime] Arena allocate StorageImpls for managed tensors (#66130)
c2ceba8ada,mobile,Untopiced,[PyTorchEdge] Move all serialize/deserialize files to a separate target (#66805)
f5daa9f76b,build_frontend,Untopiced,[iOS] Enable ARC for CMake build (#67884)
31fc9d6539,fx,Untopiced,Introduce version control for tensorrt converter decorator (#67886)
4b084bc832,performance_as_product,Untopiced,Benchmarks for various fusers (#67622)
ec8a71f9ac,jit,Untopiced,Dtype Analysis for Unary and Binary ops with Metatensors (#66898)
57335a9ee3,skip,Untopiced,Converting hardswish to strucutred kernels with metatensor support (#66899)
d04389e6f0,skip,Untopiced,Moving parts of the Shape Registry into a common file (#66948)
853298481b,skip,Untopiced,Adding Custom Rules to Device Propagation (#66973)
b8e165e841,skip,Untopiced,Adding custom testing based on opinfos input for ops with custom rules. (#67500)
f1754319e3,skip,Untopiced,Merging the implementations of ClearProfiling (#67575)
5a48868d39,quantization,Untopiced,[qnnpack] fix benchmarks after an API update (#67768)
daaad47d9c,package,Untopiced,Allow torch::deploy unity embed xar file of any size (#67814)
ba9d9d488e,fx,Untopiced,Implement padding with slice layer (#67888)
2f68878a05,jit,Untopiced,[Static Runtime] Add a comment on clients taking ownership of managed output tensors (#67554)
0c8569bec9,jit,Untopiced,Revert D32175959: Merging the implementations of ClearProfiling
b1ac7f51a1,skip,Untopiced,Revert D32175957: Adding custom testing based on opinfos input for ops with custom rules.
38af37f409,jit,Untopiced,Revert D32175958: Adding Custom Rules to Device Propagation
4d5338228f,jit,Untopiced,Revert D32175960: Moving parts of the Shape Registry into a common file
bb8978f605,nn_frontend,Untopiced,Revert D32175963: Converting hardswish to strucutred kernels with metatensor support
b098264f22,skip,Untopiced,Revert D32063662: [pytorch][PR] TST Adds device transfer into module info tests
53ebccbe78,skip,Untopiced,Fix warnings produced when running test_optim.py (#67756)
07a08fb95f,python_frontend,docs,Fix typo in LinearLR docs (#67840)
33b7790907,autograd_frontend,Untopiced,Fix conv_transpose3d backward with non-contiguous grad_out (#67829)
13a69d23b1,distributed,Untopiced,Add retry logic for test_multitenancy and documentation for find_free_port (#67775)
823ae3a4ff,autograd_frontend,Untopiced,[forward ad] Also check layout of grad matches that of self for inplace over view (#67816)
ace2183195,distributed,Untopiced,[FSDP] Address follow up comments for CPU offload (#67813)
02e35ce17b,onnx,Untopiced,[ONNX] Update onnx function export with comments and clean up (#66817) (#67803)
9269080b47,mobile,Untopiced,[PyTorchEge] backport test (#67824)
ee7412dd29,jit,Untopiced,autodiff fix for autocast_to_xxx (#67648)
9dafb6434b,skip,Untopiced,"remove use of THGenerateAllTypes, clean up (#67867)"
a4a6d056e6,mobile,Untopiced,Add ownership to more edge tests (#67859)
e7a3bbce89,jit,Untopiced,[nnc] Add support for dynamic shapes in TensorExprKernel (#67861)
cdd5d16489,foreach_frontend,Untopiced,[Foreach] Implement L1&L2 norm (#62646)
f6a4c80a5a,nn_frontend,Untopiced,Refactor cuDNN Convolution memory format and Conv-Bias-Relu code (#65594)
240e8d5cc5,python_frontend,improvements,Updated searchsorted functionality (#66818)
f6402c469e,distributed,Untopiced,(torch/elastic) fix scale down bug caused by calling rdzv_handler.shutdown() on premature agent failures (#67749)
b1ecfc6d45,releng,Untopiced,Add timeouts for GHA jobs for pytorch/pytorch (#67912)
f70e8064f4,skip,Untopiced,Don't #define NUM_THREADS (#67258)
10411e3561,quantization,Untopiced,[quan][fusion] Fix a additional_fuser_method method for fuse_fx (#67876)
9e8016d8c4,skip,Untopiced,Revert D31932215: [pytorch][PR] Don't #define NUM_THREADS
efdb17b984,python_frontend,Untopiced,Add meta support to tensor range factories (#67032)
22afe82ce3,distributed,Untopiced,[rpc] Switch RPC agent check to TORCH_CHECK and add more descriptive error (#67882)
80178d6152,distributed,Untopiced,[DDP] Fix some issues with code example in DDP docstring (#67883)
ae501a9727,mobile,Untopiced,[PyTorch Edge] Update bytecode version compatibility check (#67417)
9cacf2b718,package,Untopiced,Add custom zipper script to zip python modules for torch.deploy (#67006)
9fb3ba9d7b,distributed,Untopiced,Revert D31762735 (#67924)
191b48b12f,fx,Untopiced,[torch.fx] Fix replace pattern mechanism (#66442)
5bc89275dd,jit,Untopiced,[SR] Eliminate no-ops (#67437)
0dc99dcf59,cuda,Untopiced,Update __init__.py (#67900)
b546cdf401,jit,Untopiced,[SR] Out variant for prim::NumToTensor (#67856)
b3770766c4,skip,Untopiced,Fixes deprecation warnings in `test_optim.py` (#67954)
5bb5bfccf7,skip,Untopiced,[lint] add lintrunner support for circleci_linter (#67872)
4b021280ad,skip,Untopiced,[lint] add nativefunctions to lintrunner (#67890)
419c58ea9c,skip,Untopiced,[lint] add newlines linter to lintrunner (#67894)
53f118c800,skip,Untopiced,[lint] improve mypy lintrunner config (#67936)
d201102d36,skip,Untopiced,[lint] Add the rest of the grep linters (#67932)
8e2528132b,skip,Untopiced,[lint] small updates to .lintrunner.toml (#67942)
db456d16ee,autograd_frontend,Untopiced,`torch.lobpcg.backward`: do not save non-Variable types with `ctx.save_for_backward`. (#67994)
4a8f27445d,quantization,Untopiced,[Quant] Add dynamic QAT Linear module (#67325)
a1d733ae8c,fx,Untopiced,Avoid convert trt.Dims to tuple in hot path (#67960)
ea60e7d559,skip,Untopiced,"Op info for activation functions 2 (softsign, tanh, tanhshrink, threshold, celu, sigmoid, mish, hardsigmoid) (#67492)"
ead59b5ff3,onnx,Untopiced,[ONNX] Suppress ort warnings in onnx related test (#67054) (#67804)
37688148ae,onnx,Untopiced,[ONNX] Support opset 15 (#67121) (#67805)
958d517643,onnx,Untopiced,[ONNX] Fix new_full and full_like for Python 3.9 (#67124) (#67806)
eb22d06e5e,onnx,Untopiced,[ONNX] Use human readable enum for dtype scalars (#66822) (#67807)
f57c63032e,onnx,Untopiced,[ONNX] Fix reciprocal when input is not floating point (#67471) (#67808)
2e523ed229,jit,Untopiced,[JIT] additional support for CallMethod with autocasting (#67925)
0b09d62cf3,dataloader_frontend,Untopiced,[hackathon][DataPipe] adding .pyi file generation for torch.utils.data.datapipes (#67374)
db9b4f1a37,releng,Untopiced,[ROCm] Bump magma source to pickup memory leak fix (#67225)
9094947b0a,releng,Untopiced,use better secrets for upload labels workflow (#68013)
82398e38ab,releng,Untopiced,Upgrade and fix boto3 version to 1.19.12 (#68025)
5b036d5f2b,onnx,Untopiced,[Doc] [ONNX]Fix a broken url for ONNXRuntime custom op (#67944)
0d8a8a2e41,fx,Untopiced,[fx2trt]organize converter utils (#68015)
fd198a2fea,fx,Untopiced,[fx2trt] fix import in oss tests (#68016)
3f048c637f,distributed,Untopiced,[distributed] Render `torch.distributed.optim` members (#67885)
a2ab06514b,cuda,Untopiced,Fixes CUDA vs CPU consistency for index_put_ when accumulating (part 2) (#67189)
4b1d044498,skip,Untopiced,[WIP][resubmit] Don't #define NUM_THREADS (#68008)
25cd81876d,cuda,Untopiced,Fix typo grid_sampler_3d_cuda (#67752)
36d9a74bc6,distributed,Untopiced,Enforce that test cases extend from correct TestCase (#67819)
417dc7f86c,skip,Untopiced,"Revert D32007691: [pytorch][PR] Op info for activation functions 2 (softsign, tanh, tanhshrink, threshold, celu, sigmoid, mish, hardsigmoid)"
c5e5264be2,cuda,Untopiced,Disable TF32 in `pinv_jvp` and `pinv_backward` (#67948)
f8297d40fc,distributed,Untopiced,Adds a `maximize` flag to SGD. (#67847)
f9422e1c6b,autograd_frontend,Untopiced,Fix deadlock for multi-output forward AD (#67995)
0a9cd6d461,quantization,Untopiced,Removes unnecessary `no_pretrained_model` from test_quantize_fx.py (#67836)
577a4d34a7,python_frontend,bc_breaking,making import_module private and deprecating public method (#67990)
803e88d418,dataloader_frontend,Untopiced,[DataPipe] Fixing pickling issues with fork and demux (#67930)
c581f56c74,skip,Untopiced,Fix Dispatching not considering List[Optional[Tensor]] for dispatch (#66506)
114ef8c5ea,cpp_frontend,Untopiced,Add SiLU backward Aten symbol (#67665)
96b4f2296e,skip,Untopiced,CppSignature: Compare types by their mangled names (#67987)
9ae3f3945b,distributed,Untopiced,Add remote_module logging to the __new__ method. (#68035)
5e19fb61fd,jit,Untopiced,[SR] Release reference to JIT module if possible (#67911)
6e53d6df83,jit,Untopiced,[SR] Introduce StaticMethod (#67981)
acb035f513,skip,Untopiced,Revert D31609714: Fix Dispatching not considering List[Optional[Tensor]] for dispatch
442d7d72de,distributed,Untopiced,fixed type checking errors in options.py (#68056)
a473417076,skip,Untopiced,[LT] Merge permutation_util into master (#67766)
a027551358,skip,Untopiced,[LT] Merge cache.h (#67929)
7d931fb082,skip,Untopiced,replace platform specific CI environment variables with generic ones (#68022)
cbf596bf8e,sparse_frontend,Untopiced,Sparse CSR CPU: add `addmv_out` (#61536)
726e2ed715,skip,Untopiced,[lint] add more lints to lintrunner (#68069)
e86058559a,skip,Untopiced,"Op info for activation functions 2 (softsign, tanh, tanhshrink, threshold, celu, sigmoid, mish, hardsigmoid) (#67492)"
d2438a8901,quantization,Untopiced,[qnnpack] Lock before weightpacking in qlinear (#68012)
a9c2f11d2a,jit,Untopiced,Update Freezing Logic and add new passes (#68024)
45ac6f2b65,quantization,Untopiced,[quant] Fix comparison against reference for test_qat_functional_linear (#68061)
c7eaec86f0,distributed,Untopiced,[NCCL] Patch bfloat16 support (#67843)
273f7ae9b3,fx,Untopiced,fx: Update fx.rst (#68043)
4100a5cc48,releng,Untopiced,Revert D32286934: [pytorch][PR] replace platform specific CI environment variables with generic ones
be4150139a,skip,Untopiced,bugfix for conditional functionalization (#67715)
fe46d6c68f,skip,Untopiced,functionalization: map copy_() -> to().expand_as() (#67878)
7c90bd77ec,autograd_frontend,Untopiced,Test functionalization pass in python (#66101)
eaf0457eef,distributed,Untopiced,[distributed][docs] Delete distributed optimimzer section from RPC and add reference to namespace docs page (#68068)
43ef6816f2,python_frontend,Untopiced,OpInfo for `nn.functional.cross_entropy` (#63547)
55e3b23abe,mobile,Untopiced,[Pytorch Edge] Generic Build Features for Selective Build (#67817)
8d025bbc2d,releng,Untopiced,.github: Migrate macOS workflows to GHA (#67717)
1b2a366932,jit,Untopiced,[SR] Enforce checks for resizing of the internal buffer in MemoryPlanner in unit tests (#67941)
d8f0087e08,releng,Untopiced,.github: Fix sccache for macOS workflows on push (#68094)
b1a42298a4,nn_frontend,Untopiced,Simplify example for nn.Flatten (#67472)
078c655985,jit,Untopiced,[nnc][mobile] temporarily disable quantization external functions (#68029)
790763b0fe,cuda,Untopiced,Add an option to disable reduced precision reductions for FP16 GEMM (#67946)
91af74c934,cpp_frontend,Untopiced,remove Generate* macro files (#67940)
3f1a3f7b18,fx,Untopiced,Fix ads dense arch regression (#68071)
6b44e75f6b,jit,Untopiced,aliasing fixes (#66977)
bf31d4b2b5,quantization,Untopiced,[PyTorch] Replace copy_ with data_ptr<float>() since input Tensor's dtype is guaranteed to be float (#67788)
b0817e19e0,jit,Untopiced,[PyTorch] Avoid reading file from stream for 0 byte Tensor storage (#67787)
cb2a41e508,mobile,Untopiced,[PyTorch Edge] Don't use LeftRight in mobile (#66064)
94b6fa6f8b,nn_frontend,Untopiced,Adds an optimizer instance variable to ChainedScheduler (#68010)
a6c0edff1a,complex_frontend,Untopiced,fix gradcheck to generate valid input for forward AD complex (#68001)
6011c35a79,skip,Untopiced,[LTC] Upstream class BackendDevice (#68027)
147de8243b,skip,Untopiced,Fixed deprection warnings with `.data<T>()` in SpectalOps.cpp (#67993)
9a2db6f091,nn_frontend,Untopiced,Factor backend routing logic out of convolution forward (#67790)
ae5864498d,python_frontend,Untopiced,torch.allclose opinfo (#68023)
8dfbc620d4,nn_frontend,Untopiced,don't hardcode mask type in mha (#68077)
746a31b290,fx,Untopiced,Logger integration format (#67962)
ecd5b1a8d4,jit,Untopiced,[SR] Native implementation for aten::split (#67476)
a3bb95c1b5,releng,Untopiced,don't include label in ci: sev issue (#68093)
c075f0f633,distributed,Untopiced,Update rpc testing to include USE_TENSORPIPE directive (#68080)
d6e6064efc,skip,Untopiced,[LT] Upstream backend interfaces (#67927)
22e73f616c,autograd_frontend,Untopiced,Update unpack_dual to return named tuple (#68062)
a510f4139b,fx,Untopiced,Fix lambda function broke torch.save
f171c78c04,nn_frontend,Untopiced,add unpack_sequence and unpad_sequence functions (#66550)
0e366b8e5f,fx,Untopiced,Make `torch.fx.experimental.fx2trt.passes` a package (#68139)
ca7d0062ad,mobile,Untopiced,[PyTorch Edge] Better error message when training attribute is not found (#68103)
47bc47f2b9,jit,Untopiced,[SR] Add runtime check to correct bad schema alias info (#67825)
1f07efd0f2,jit,Untopiced,[SR] Fix aten::split schema (#68135)
a229c3e51a,mobile,Untopiced,Add complete type name in error message when fail to export model (#67750)
a6a2616558,skip,Untopiced,Automated submodule update: kineto (#67445)
aea4e61ec3,skip,Untopiced,skip test_jit_legacy (#68129)
f02efc749a,distributed,Untopiced,[Dist CI][BE] Run each test in its own process for test_distributed_spawn (#67901)
4466ba8f30,quantization,Untopiced,Working POC of define-by-run quantization (#64676)
d4ae789655,fx,Untopiced,OpInfos for new_blah functions and some _like functions (#67357)
5efe5e243a,fx,Untopiced,Ease constrain for fuse path in trt lower (#68148)
f89572f417,sparse_frontend,Untopiced,Add feature: zeros_like() from a dense tensor to a sparse tensor (#68108)
362c6069b9,mobile,Untopiced,[nnc] Lazy lowerings registration; custom classes network params (#67623)
864c6b3794,mobile,Untopiced,[nnc] aotCompiler outputSpec support quantized outputs (#67711)
35f1617001,python_frontend,bug_fixes,Implement Entropy methods for Binomial and Multinomial distributions (#67609)
9e7b314318,nn_frontend,Untopiced,OpInfo for `nn.functional.conv1d` (#67747)
a1ace029e2,nn_frontend,Untopiced,Add host-side memory requirement for `test_softmax_64bit_indexing` (#67922)
84d3df8027,cuda,Untopiced,Fast cuda layer norm (#67977)
c2642b6465,sparse_frontend,Untopiced,Sparse CSR CPU: add `torch.add` with all inputs sparse (#64391)
97a386805e,mobile,Untopiced,[Pytorch Edge] Add selective macros to metal ops (#68134)
bd5f33f91e,skip,Untopiced,demo backend decoupled from operators (#66100)
61a94495d9,dataloader_frontend,Untopiced,[DataPipe] adding ZipperMapDataPipe (#68032)
fe90313d02,python_frontend,performance,Avoid index_put_ overhead in histogram kernel's inner loop (#67815)
6cade3362b,fx,Untopiced,[fx-acc] add optimize_noop graph opt (#68131)
b473ca999b,skip,Untopiced,[lint] add cmakelint to lintrunner (#68191)
7b376bf844,distributed,Untopiced,Remove ProcessGroup from TensorPipeAgent initialization (#68128)
cd4e31ff21,skip,Untopiced,[LTC] Add some comments to BackendDevice() (#68156)
db014b8529,python_frontend,new_features,Add `set_deterministic_debug_mode` and `get_deterministic_debug_mode` (#67778)
40bedf6206,linalg_frontend,Untopiced,Fix test_triangular_solve testcase enumeration (#67635)
b07a11929d,linalg_frontend,Untopiced,Array API: Add torch.linalg.cross (#63285)
4fe3965b3a,python_frontend,bug_fixes,Fix dtype arg typing for Tensor.type doc string (#67019)
1e8f836c44,python_frontend,Untopiced,Remove OpInfo non-contig inputs (#67677)
f9ea41f257,skip,Untopiced,"Fixes spelling error writeable to writable, improves warning, and documentation (#67664)"
0420545639,python_frontend,improvements,Enable all dtype combinations in `torch.Tensor.view(dtype)` (#66493)
d049772538,cpp_frontend,Untopiced,Bump dlpack.h to latest version (#65047)
6afb414c21,linalg_frontend,Untopiced,Nan in linalg eig (#67544)
9571eb599c,skip,Untopiced,[lint] fix up clangtidy lintrunner integration (#68192)
301369a774,nn_frontend,Untopiced,[PyTorch][Fix] Pass the arguments of embedding as named arguments (#67574)
3dc0754c53,releng,Untopiced,[pytorch][mobile] deprecate the LLVM-based static analyzer (#68180)
9cb65df79f,jit,Untopiced,[Static Runtime] Fallback to disabling manage_output_tensors instead of crashing when wrong API is used (#67939)
89d556f648,cpp_frontend,Untopiced,add VS extension in doc (#63944)
613c1aca6d,python_frontend,Untopiced,Adds support for automated error and warning testing (#67354)
799ebce3aa,fx,Untopiced,Add algo recorder/replayer to lower.py (#68194)
1181628d85,skip,Untopiced,BE: Use TORCH_CHECK instead of explicit c10::Error (#68187)
e795315c63,quantization,Untopiced,Changes and fixes to prepare for dynamic conv (#68175)
0bd0a67c4f,caffe2,Untopiced,[lint][fbcode/caffe2] CLANGFORMAT
dc24503a89,skip,Untopiced,"Fix Hash(c10::Scalar), account for garbage data in union (#68201)"
5c3a9f3fdc,python_frontend,Untopiced,adding opinfo for torch.nn.bilinear and torch.nn.glu (#67478)
98bab78e11,skip,Untopiced,Revert D32039318: [pytorch][PR] Bump dlpack.h to latest version
96d116fec2,jit,Untopiced,[JIT] Add additional debug output when op cannot be found in AliasDb (#68099)
33353fb828,skip,Untopiced,Python tracer for profiler (#67407)
06e8cb9e04,distributed,Untopiced,"Manually Disabling two TestDistBackendWithSpawn tests on ROCm, test_ddp_profiling_torch_profiler and test_ddp_sync_bn_training_vs_eval (#68255)"
66b52d5b49,jit,Untopiced,[TensorExpr] Convert linear_clamp_run to using schema in NNC lowerings. (#66523)
f6e45102d2,quantization,Untopiced,[quant][embedding qat] Support non-partial functions in qconfig comparison (#68067)
6ddaf3bd37,skip,Untopiced,"[LT] Upstream TsNode, TsNodeLowering, TsLoweringContext (#68154)"
a82e51a7ae,cuda,Untopiced,Move some cub templates out of the header file (#67650)
8bf150f21b,skip,Untopiced,Revert D32178667: [pytorch][PR] Python tracer for profiler
da5ffe752a,build_frontend,Untopiced,Add reporting for flaky tests in CI (#68150)
a8b93cb3ec,vmap_frontend,Untopiced,More aggressively market functorch.vmap when torch.vmap gets called (#67347)
48c8de45b0,onnx,Untopiced,[ONNX] Remove the argument example_outpus of export() method entirely. (#67082) (#67809)
282221c5d6,fx,Untopiced,"Fuse unsqueeze, cat, sum for inline_cvr (#68289)"
80339e85c5,build_frontend,Untopiced,Fix disabling bot with subprocessing (#68290)
e511a7a5b4,jit,Untopiced,[TensorExpr] Remove non-determinism in iterating over unordered_set of intermediate buffers. (#68277)
52e93fca2c,jit,Untopiced,[TensorExpr] Fix some TE python bindings. (#68232)
ec94bb787a,jit,Untopiced,[TensorExpr] Add a way to define target triple/cpu/attrs for llvm codegen and turn on the AOT workflow. (#66527)
cb14a258a2,distributed,Untopiced,[c10d] Fix object-based collectives for debug mode (#68223)
f7366ca51b,quantization,Untopiced,implemented quantize_per_tensor_dynamic and added a corresponding test script (#68004)
6fb8ebcd92,jit,Untopiced,[tensorexp] Add strides to Buf (#68018)
be281fc597,jit,Untopiced,Check for None in torch.jit.Graph.create (#68253)
1adeeabdc0,fx,Untopiced,Fix trt tuple(Dims) throwing issue (#68318)
4c87aa77d1,dataloader_frontend,Untopiced,[DataPipe] Traverse DataPipe graph excluding primitive and callable (#67783)
1c0d6ff835,fx,Untopiced,[fx][const fold] Allow to set up a function to modify const_nodes for split_const_subgraphs (#67784)
d1c529bd0b,releng,Untopiced,replace platform specific CI environment variables with generic ones (#68133)
43874d79e7,linalg_frontend,Untopiced,Fix failing test due to a bug in NumPy when using OpenBLAS (#67679)
24b60b2cbf,skip,Untopiced,[lint] lintrunner fixes/improvements (#68292)
56024e91c9,releng,Untopiced,GHA: Enable flaky test reporting by setting PYTORCH_RETRY_TEST_CASES=1 (#68300)
12026124cc,nn_frontend,Untopiced,Avoid the view for mkldnn case in 1D convolution (#68166)
5b05983497,skip,Untopiced,[bugfix] fix two edge cases in functionalization (#68269)
30cda0b28c,skip,Untopiced,[bugfix] functionalization pass for view ops without a 'self' first argumennt (#68339)
c697eeba72,jit,Untopiced,[JIT] Combine concat nodes where possible (#67000)
ccd9675569,skip,Untopiced,[lint] Disable modernize-use-nodiscard (#68354)
549e014963,python_frontend,Untopiced,[docs] fix torch.histc's min/max arg types (#64191)
0cf46fb0de,fx,Untopiced,[fx2trt] fix a bug in conversion from negative dim to positive dim (#68360)
e3bcf64ff8,quantization,Untopiced,[qnnpack] Remove redundant fp16 dependency (#68011)
6adbe044e3,quantization,Untopiced,Added nearest-exact interpolation mode (#64501)
ea0a558487,releng,Untopiced,GHA CI: make the default config use only one GPU (#68382)
7b958fbec4,releng,Untopiced,ci: Build periodic jobs with DEBUG=1 (#67192)
0823d18fcd,skip,Untopiced,make TSComputation ctor explicit (#68286)
9ed49449b3,jit,Untopiced,[SR] Add net level record functions (#68091)
2fd468e5f8,jit,Untopiced,[jit] Set the graph input types before interpreting the graph during tracing (#68242)
529ebae0ac,nn_frontend,Untopiced,Bugfix for TorchScript RNN RELU and TANH (#61274)
09615cd0b0,quantization,Untopiced,Adding Dynamic Conv and ConvT ops/modules (#68176)
9d25554d45,onnx,Untopiced,[ONNX] Allow registration of custom symbolics for aten namespace (#66481) (#67810)
d32efe8bc2,onnx,Untopiced,[ONNX] Remove the argument use_external_data_format of export() method entirely. (#67080) (#67811)
722af775c3,onnx,Untopiced,[ONNX] ConstantMap setters to update existing value instead of emplace (#67630) (#67812)
86c1368611,fx,Untopiced,[fx][const fold] Add test/example for skipping quant/dequant pattern (#68378)
065018d812,mobile,Untopiced,[pytorch][xros] Ensure all pytorch mobile operators build ok in XROS mode (#68266)
7ee84ad321,quantization,Untopiced,Refactoring quantized op tests to combine test classes (#68282)
27cc11226d,caffe2,Untopiced,make broadcast fastpath the default for currently rolled-out ops (#68365)
d541aa8cbe,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
ec742c65d5,skip,Untopiced,Fix a sign comparison issue in BatchLinearAlgebraLib.cpp (#68293)
438ca7603f,skip,Untopiced,Fix sign comparison issue in Histogram.cpp (#68294)
33e9a0b5f6,profiler,Untopiced,[Reland] Python tracer. (#68325)
01a8862582,jit,Untopiced,OpInfo tests for `nn.functional.max_pool{n}d`. (#68075)
add79722dd,linalg_frontend,Untopiced,Correct `householder_product` docs. (#68335)
bf60c6e71b,jit,Untopiced,[JIT] remove prim::SetAttr from list of ops with side effects (#68311)
bc3d380ed1,python_frontend,bug_fixes,Throw error when saving storages that view same data with different type (#66949)
6226a3cf74,vulkan,Untopiced,[Vulkan] Implement permute operator (#68274)
ba16b1eca7,jit,Untopiced,[numpy] Alias `arctan2` to `atan2` (#67010)
45b2f41c3e,package,Untopiced,[package] fix torchscript classes in package (#68028)
8678472ec8,jit,Untopiced,[PyTorch][Static Runtime] Save 2 pointers in ProcessedNode (#67860)
6acde23bec,jit,Untopiced,[PyTorch][Static Runtime] Switch input/output repr to 2-byte offsets (#67934)
639258499f,jit,Untopiced,"[PyTorch][Static Runtime] Add & use ""small array"" for ProcessedNodeInputs (#67935)"
5c3529a86d,skip,Untopiced,[lint] small pass to make lint clean (#68367)
391be39575,linalg_frontend,Untopiced,Use reduced precision switch in `test_addmm_baddbmm_overflow` (#68399)
10e9d80ad1,jit,Untopiced,[PyTorch][Static Runtime] Don't track scalar ivalues (#67702)
030ee34216,fx,Untopiced,Add OpInfo for torch.nonzero (#67459)
df129fa8d6,cpp_frontend,Untopiced,[PyTorch] Support MaybeOwned<IValue> (#68157)
04056df475,releng,Untopiced,[android][fbjni] Update fbjni to 0.2.2 (#68400)
ed00a763a2,jit,Untopiced,[PyTorch] Don't force refcount bump when accessing DictEntryRef key/value (#68158)
86399d8e0c,python_frontend,docs,Add histogramdd to torch.rst (#68273)
bbc24222d2,jit,Untopiced,[PyTorch][Static Runtime] Refcount bump pass in native_ops (#68159)
755be54c77,jit,Untopiced,[PyTorch][Static Runtime] Borrow outputs in static_runtime::dict_unpack (#68160)
8954c92529,jit,Untopiced,[PyTorch][Static Runtime] Borrow outputs in static_runtime::VarTupleUnpack (#68161)
76e9dbb0f4,fx,Untopiced,[torch.fx] add code-gen customizability and support for setting breakpoint in code-gen'd forward() call (#67139)
284758b585,nn_frontend,Untopiced,correct NLLLoss parameters default value (#68426)
27eca2c6fd,releng,Untopiced,Revert D32467139: [pytorch][PR] [android][fbjni] Update fbjni to 0.2.2
a8bcfc90f5,distributed,Untopiced,fix fsdp overlap flaky test (#68415)
515d9fb2a9,python_frontend,Untopiced,Add OpInfo for torch.histc (#67452)
75ccb07b26,jit,Untopiced,[SR] LOG->VLOG (#68477)
5cfca5524c,jit,Untopiced,[JIT] clear GraphFunction.optimized_graphs_ after freezing a module (#68316)
affa3f846c,sparse_frontend,Untopiced,Sparse CSR CPU: add `torch.addmm` (#65606)
173c0f8a98,releng,Untopiced,Actually enable PYTORCH_RETRY_TEST_CASES for linux tests (#68486)
fd85d925b0,skip,Untopiced,Fix some sign issues (#68361)
aa9ee8d02a,jit,Untopiced,[Static Runtime] Avoid copying function objects per StaticRuntime instance (#68368)
4c346bd073,autograd_frontend,Untopiced,"Added forward derivatives for neg, diag, inverse, linalg_eig (#67837)"
6e640a0acf,distributed,Untopiced,Revise the socket implementation of c10d (#68226)
fea2bb64c8,python_frontend,Untopiced,"OpInfos for stft, istft, fftshift, ifftshift (#68198)"
2317e28e9e,autograd_frontend,Untopiced,Enable complex autograd for col2im / im2col (#68199)
1cade067e3,vulkan,Untopiced,[Vulkan] Vulkan backend is now thread-safe (#67733)
9de730ebba,quantization,Untopiced,q_avgpool: Loop over batch dimension inside operators (#66819)
9c15523793,distributed,Untopiced,Attach unused parameter info to static graph error message (#68413)
f6696c5a85,distributed,Untopiced,export CPUOffload in _fsdp package (#68308)
6186b90c53,caffe2,Untopiced,[Contrib][Fakelowp] Change Lut Size for Tanh (#68334)
faa1e8b7cf,skip,Untopiced,Fix flaky test_nccl_timeout (#68403)
01b30922dd,jit,Untopiced,[static runtime] fuse gather+to+lengths_to_offsets (#64075)
0dc3f829d9,jit,Untopiced,Nvfuser code bump 11 5 (#67943)
54ac64f035,releng,Untopiced,Revert D32477989: [pytorch][PR] Actually enable PYTORCH_RETRY_TEST_CASES for linux tests
3e3bf40b0a,distributed,Untopiced,Revert D32452012: [pytorch][PR] Fix flaky test_nccl_timeout
143491e0ad,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
3da2e09c9b,nn_frontend,Untopiced,"Added antialias flag to interpolate (CPU only, bilinear) (#65142)"
cac3cd1433,python_frontend,improvements,add torch.diff support for n greater than 1 (#67260)
61206ba4db,jit,Untopiced,[SR] Add StorageGroup abstraction (#68279)
693fe2fd9b,jit,docs,docs: Added Union to supported types in documentation (#68435)
4e21d77dbb,skip,Untopiced,Use TORCH_CHECK in MapAllocator (#68424)
3b4f072383,cpp_frontend,devs,Remove TH/THC Storage data and copy functions (#68127)
2f37a39a5c,quantization,Untopiced,[quant][graphmode][fx] Refactor node_name_to_target_dtype to make it more clear (#68317)
f3e2fefe09,releng,Untopiced,Actually enable PYTORCH_RETRY_TEST_CASES for linux tests (#68486)
e2aeb4a7af,cuda,Untopiced,Improve native layer norm backward perf (#68238)
4eb772fde6,jit,Untopiced,Refactor saving jit::Module to mobile .pt in 2 steps: (#66494)
b0bdf588ea,onnx,Untopiced,[ONNX] Release values cached in global object (#68210)
53bfb00ee1,skip,Untopiced,[bugfix] TensorList args in functionalization pass (#68395)
d71092f668,releng,Untopiced,[android][fbjni] Update fbjni to 0.2.2 (#68400)
9d9ca88f5c,cuda,Untopiced,[predictor][trt] Expose more CUDA/CuDNN info to at::Context and BC stage 1 (#68146)
e72b9db48e,fx,Untopiced,[fx2trt] add converter for acc_ops.hardtanh (#68550)
9807787135,python_frontend,new_features,`scatter_reduce` (#68115)
146a7f68e2,distributed,Untopiced,Enable desync root cause analysis for NCCL (#68310)
96ba2099d1,distributed,Untopiced,Fix c10d TCP store with mutex (#68499)
952ca25daa,sparse_frontend,Untopiced,Sparse CSR: add `convert_indices_from_csr_to_coo` (#66774)
35712a8eb4,distributed,Untopiced,[reland] simplify init_from_local_shards API (#68021)
4635f5711f,jit,Untopiced,[static runtime][dper] multi_env tests for static runtime: selective enable (#67467)
dbbb02474b,cuda,Untopiced,[GPU host alloc] Fast path for size 0 malloc (#68532)
6c9cf5e6ea,quantization,Untopiced,[quant][embedding qat] eager mode QAT for Embeddings (#66429)
f9ef807f4d,nn_frontend,Untopiced,Replace empty with new_empty in nn.functional.pad (#68565)
f0e2ad5037,vmap_frontend,Untopiced,Stop warning spamming about vmap in gradcheck (#68586)
b1aa45a8a7,python_frontend,Untopiced,Fix `_make_wrapper_subclass`'s storage_offset handling (#68268)
a2d187a672,skip,Untopiced,[BE] MapAllocator: report map error on Linux (#68545)
857fed1f42,autograd_frontend,Untopiced,torch.linalg.qr: forward AD support (#67268)
d5d2096dab,nn_frontend,Untopiced,[testing] make @dtypes mandatory when using @dtypesIf (#68186)
533e72e0a4,cuda,Untopiced,Fix DLPack CUDA stream convention (#67618)
7d38768d84,fx,Untopiced,Rename splitter result (#68303)
f120335643,skip,Untopiced,[static runtime] dequantize out variant (#67873)
0706607abc,skip,Untopiced,Add linalg.solve_triangular (#63568)
748d9d2494,jit,Untopiced,Revert D32187063: [static runtime] dequantize out variant
48771d1c7f,nn_frontend,Untopiced,[BC-breaking] Change dtype of softmax to support TorchScript and MyPy (#68336)
9f4e004abd,linalg_frontend,Untopiced,Revert D32283178: Add linalg.solve_triangular
ab1d879b33,jit,Untopiced,[WIP] forbid aliasing between the outputs of a differentiable graph (#67732)
244691db98,distributed,Untopiced,surface ncclUniqueId store broadcast error (#68597)
77db720c65,skip,Untopiced,Moving parts of the Shape Registry into a common file (#66948)
71a031e954,skip,Untopiced,Adding Custom Rules to Device Propagation (#66973)
3757a16c7a,skip,Untopiced,Adding custom testing based on opinfos input for ops with custom rules. (#67500)
003f6ccec6,distributed,Untopiced,[BE] rename some tests in test_c10d_common (#67828)
a990a7ac31,distributed,Untopiced,[torchelastic] Remove stale `test_get_default_executable` test (#68609)
e7002c62ae,jit,Untopiced,[nnc] External functions quantized via Dispatch (#68572)
913ac27112,autograd_frontend,Untopiced,Fixes forward AD codegen for multiple formulas (#68535)
9ce3c630ba,python_frontend,docs,[Docs] Mention `torch.bfloat16` in `torch.finfo` (#68496)
ff125a3624,python_frontend,docs,Minor changes in documentation (#68557)
a39060c001,package,Untopiced,textray demo for unity
ca92111758,nn_frontend,Untopiced,Add native_dropout (#63937)
39747dc456,jit,Untopiced,"[nnc] Loweings for flatten, xnnpack prepack op (#68470)"
68d8ab0cc6,fx,Untopiced,[const_fold] Fix call_module const folding (#68614)
758d7dea9c,performance_as_product,Untopiced,torch.monitor - Initial C++ Stats (#68074)
833dcaf2d6,sparse_frontend,Untopiced,Sparse CSR: Add `torch.sin` (#68123)
ced57eb490,jit,Untopiced,[PyTorch][Static Runtime] Delete incorrect alias analysis code (#67075)
343723e2ad,jit,Untopiced,[PyTorch][JIT][easy] Delete unnecessary overload of MemoryDAG::mayAlias (#66966)
18312313c4,profiler,Untopiced,[Profiler] Add missing guards (#65812)
f82f14de17,profiler,Untopiced,[libkineto] Refactor 4/n: Simplify activity logger step 2/3 (#68329)
c2c859bdf2,quantization,Untopiced,[quant][embedding qat] Add benchmarks for QAT Embedding+EmbeddingBag (#66560)
4bcff4733d,python_frontend,Untopiced,Add OpInfos for parcel Elementwise Binary II (#68085)
f74779e403,mobile,Untopiced,[android] Lite interpreter naming for android nightly publishing (#68651)
0809553cf0,jit,Untopiced,refactor assert_close to be more modular (#67794)
f99f5ee088,skip,Untopiced,add support for None in assert_close (#67795)
2cab77f810,sparse_frontend,Untopiced,Masked normalization infrastructure and strided masked softmax (#68333)
5880a2f1ef,fx,Untopiced,Allow fuse unsqueeze cat sum with multiple input (#68650)
da4a95c79a,cuda,Untopiced,[ROCm] Use hipCUB/rocPRIM scan algorithms for large index support (#68487)
a6d862c50a,quantization,Untopiced,[quant][graphmode][fx] Add support for weight and bias dtype in backend_config_dict (#68602)
ee4cfaa286,jit,Untopiced,[SR] Add utility class to determine tensor ranges (#68284)
875ba3dddb,quantization,Untopiced,[quant][trt] Add support for torch.addmm in TensorRT (#67537)
183dcdf551,distributed,Untopiced,[reland] Fix flaky test_nccl_timeout (#68544)
bbe2aae84c,releng,Untopiced,Support cuda 11.5: install magma for cuda in conda (#68665)
2455cc2adf,autograd_frontend,Untopiced,Address case when layout of tangent is not same as base (#66292)
21d203b5ca,autograd_frontend,Untopiced,Add internal assert for tangent layout mismatch for view ops (#66293)
e358c49a5b,python_frontend,Untopiced,Add OpInfo test and fix a couple cases (#66294)
7bb401a4c9,nn_frontend,Untopiced,Add forward AD support for miscellanous operators (#67820)
5456d8c8f3,autograd_frontend,Untopiced,Add vectorized Jacobian and Hessian computation with forward AD (#67041)
993b7a2052,skip,Untopiced,Remove doubly nested anonymous namespace (#68555)
a545a409f8,quantization,Untopiced,[quant][graphmode][fx] Support input_quantized_idxs and output_quantized_idxs in the new convert (#68042)
37edb7483a,caffe2,Untopiced,[torchelastic][1/n] Fix `caffe2.test.distributed.launcher.api_test` flaky tests (#68624)
6cca14d02f,fx,Untopiced,[fx2trt][easy] Replace all network.add_activation() call with helper function (#68676)
578507cb7b,cuda,Untopiced,Fix nanmedian result using more CUDA memory than necessary (#68591)
af564e73b8,sparse_frontend,Untopiced,Strided masked log_softmax. (#68461)
c41d8290b3,distributed,Untopiced,Rename shard_lengths to shard_sizes to be more inline with Tensor sizes. (#66464)
8e51381bac,mobile,Untopiced,Make AOT compiler generic (#68637)
e554d8b89c,skip,Untopiced,Fix retry on connect failure decorator (#68600)
95f4cd0ba9,python_frontend,performance,Implement topk with sort for some cases (#68632)
75955e4ef8,cpp_frontend,Untopiced,[clone][sparse] Add `torch._C._sparse` namespace (#68672)
d0eff8d846,sparse_frontend,Untopiced,Strided masked softmin. (#68463)
ca499567d2,quantization,Untopiced,barebones numeric suite for quantization with dynamic tracing (#67776)
ed6ef0eec4,quantization,Untopiced,dbr quantization: inline scale and zp (#68251)
f8b084c563,quantization,Untopiced,dbr quant overhead[1/x]: remove expensive calls to named_modules (#68309)
9cf4779ec9,quantization,Untopiced,dbr quant: refactor `get_func_output_obs_type` to only use `seen_op_info` (#68341)
57472ec414,quantization,Untopiced,dbr quant: refactor `get_quantized_op` to only use `seen_op_info` (#68342)
2755cf457c,quantization,Untopiced,dbr quant: refactor AutoQuantizationState._get_input_args_quant_dequant_info (#68343)
52cc9cb0ee,quantization,Untopiced,dbr quant: refactor AutoQuantizationState._get_packed_param_name (#68344)
629f9a5532,quantization,Untopiced,dbr quant: clean up AutoQuantizationState.get_op_convert_info flag (#68345)
9fba8971a7,quantization,Untopiced,dbr quant: move model level utils into own file (#68346)
c7ecf1498d,quantization,Untopiced,dbr quant overhead[2/x]: precalculate op_convert_info (#68347)
c72ffee497,quantization,Untopiced,dbr quant overhead[3/x]: speed up AutoQuantizationState.mark_cur_op_complete (#68350)
3fc9bc43c6,quantization,Untopiced,dbr quant overhead[4/x]: speed up hook type calculations (#68351)
16a6e0612d,quantization,Untopiced,dbr quant: clean up key types in AutoQuantizationState mappings (#68369)
b3a7d696b3,quantization,Untopiced,dbr quant overhead[5/x]: remove unnecessary asserts (#68370)
b7d58745c8,quantization,Untopiced,dbr quant overhead[6/x]: remove unneeded isinstance checks in `op_convert_before_hook` (#68371)
74ba1067a6,quantization,Untopiced,dbr quant overhead[7/x]: speed up AutoQuantizationState.reset_to_new_call (#68372)
f1021bcf38,quantization,Untopiced,dbr quant overhead[8/x]: small speedup in op_needs_quantization (#68373)
592053f115,quantization,Untopiced,dbr quant: simplify relatedness logic (#68374)
95c00cf029,quantization,Untopiced,speed up quantized relu6 inplace kernel (#68404)
ba230de118,quantization,Untopiced,dbr quant: remove more asserts from hot paths (#68431)
e1c449ff34,quantization,Untopiced,dbr quant overhead[9/x]: precalculate when to skip op_convert_after_hook (#68432)
3b3dc1ade8,sparse_frontend,Untopiced,Sparse CSR CPU: add `triangular_solve_out` (#62180)
2d06c081ca,python_frontend,Untopiced,Fix test issue with householder_product for non-contiguous inputs. (#68231)
d2a90f91bc,skip,Untopiced,[opinfo] use dtypes instead of dtypesIfCPU (#67619)
fb556c91ce,releng,Untopiced,[BE] delete frontend.cpp (#67400)
7c6a8a47db,distributed,Untopiced,[BE] minor improvement to dist quantization (#67401)
148f323856,skip,Untopiced,Revert D32541986: [pytorch][PR] [opinfo] use dtypes instead of dtypesIfCPU
a66ff81837,dataloader_frontend,Untopiced,[DataPipe] Optimize Grouper from N^2 to N (#68647)
39ec0f321b,releng,Untopiced,GHA: add print_tests_stats step to MacOS workflow (#68669)
ddc22ea3b2,distributed,Untopiced,[Dist CI][BE] test_c10d_nccl run in subprocess (#68503)
9554ebe44e,distributed,Untopiced,[Dist CI][BE] c10d gloo tests run in subprocess (#68504)
a2e35e167b,skip,Untopiced,refactor: update f-string for swa.utils.py (#68718)
b46c89d950,skip,Untopiced,Add linalg.solve_triangular (#63568)
d176c82bd5,sparse_frontend,Untopiced,[sparsity] Fix and enable the pruning tests (#66411)
e7d8f096c9,sparse_frontend,Untopiced,[sparsity] Fix GPU training for sparsity (#66412)
74e6d2ce67,jit,Untopiced,fix typos in jit_language_reference.rst (#68706)
5d300e761d,fx,Untopiced,Add OpInfos for parcel Activation Functions I (#68521)
d6a68e0b8d,distributed,Untopiced,[PyTorch][3/N] Enable the rest forward spec options for ShardedEmbedding and ShardedEmbeddingBag. (#67799)
b845b9876b,sparse_frontend,Untopiced,[sparsity] Fix for the failing pruner test (#68794)
a31aea8eaa,quantization,Untopiced,[quant][graphmode][fx] Add support for specifying reference quantized module mapping in backend_config_dict (#68227)
445b31abff,nn_frontend,Untopiced,Initial version of general convolution_backward (#65219)
98e51895ef,distributed,Untopiced,[dist_quant] change op registration to each file instead (#68797)
3282386aa4,quantization,Untopiced,Added additional string to search cpu flags for vnni detection (#67686)
e7e1b76106,releng,Untopiced,Require CMake 3.13 when building with Ninja (#68731)
23288fdacc,python_frontend,Untopiced,Making norms inputs independent (#68526)
73f494d690,releng,Untopiced,.circleci: Remove migrated CUDA 10.2 build (#68782)
c0e6dc9ac7,nn_frontend,Untopiced,"[pytorch] Fix loading from checkpoint after ""maximize"" flag was introduced in SGD (#68733)"
959cb03132,mobile,Untopiced,Populate operator_input_sizes_ (#68542)
84047ff342,distributed,Untopiced,Add API usage logging to ShardedTensor and fix a few tests. (#68771)
8e343ba5db,nn_frontend,Untopiced,Revert D32611368: [pytorch][PR] Initial version of general convolution_backward
2cd48d14ef,linalg_frontend,Untopiced,Fix `test_svd_errors_and_warnings` warning message when cuda >= 11.5 (#68683)
78dce417a1,releng,Untopiced,[BE] Simplify magma installation logic (#68778)
998daf44d6,fx,Untopiced,All get_attr node to be in64 type (#68818)
79b67d9a4a,quantization,Untopiced,[Quant] Refactor handling of FixedQParams operators (#68143)
8fb9ce4927,cuda,Untopiced,Update Documentation to Make CUDA Call Explicit (#67973)
d9f3feb5a2,jit,Untopiced,[SR] Use std::vector::reserve for StaticModule constants (#68834)
b5b62b3408,releng,Untopiced,Cleanup old TD logic (#68842)
b28ddd72d3,sparse_frontend,Untopiced,Sparse CSR CUDA: Add block torch.addmm when mat1 is sparse (#68707)
7d8a79b6f3,jit,Untopiced,[nnc] llvm_codegen quantization types for vectype (#68736)
be7e159e71,distributed,Untopiced,Remove extraneous logging (#68830)
31d36fd35d,build_frontend,Untopiced,fix sccache issue on Windows CPU (#68870)
7802953dd5,jit,Untopiced,[nnc][quantization] quantized ops for BI bytedoc via aten (#68790)
208e109dbf,sparse_frontend,Untopiced,Revert D32633806: Sparse CSR CUDA: Add block torch.addmm when mat1 is sparse
b69155f754,python_frontend,bug_fixes,Avoid dtype mismatch error in `torch.save` if storages are unallocated (#68787)
c7d5e0f53f,fx,Untopiced,"OpInfos for torch.atleast_{1d, 2d, 3d} (#67355)"
cf54416925,python_frontend,docs,Add docs entry for `adjoint`. (#68869)
aceb46e4ce,skip,Untopiced,"OpInfos for torch.{flatten, column_stack} (#67555)"
1940cc028e,quantization,Untopiced,[quant][graphmode][fx] Fork subgraph_rewriter from torch.fx to quantization (#68228)
bc3bdbc8f4,skip,Untopiced,implemented 'torch.distributions.constraints.symmetric' checking if the tensor is symmetric at last 2 dimension. (#68644)
d44e610efa,skip,Untopiced,"[CUDA Pinned Memory] Event recording with non-blocking copies should track the storage context, not the tensor data pointer (#68749)"
96929ea995,python_frontend,Untopiced,Update empty and empty_like examples in docs (#68874)
14dc9759f2,fx,Untopiced,"Revert D32650384: OpInfos for torch.{flatten, column_stack}"
c5f63f859e,releng,Untopiced,Add slow path to `getCustomClassTypeImpl` (#68717)
4afa5ea0ab,skip,Untopiced,native_functions.yaml: remove SparseXPU which is added by accident (#68791)
c2e3b92db4,jit,Untopiced,partial revert of D32522826 (#68889)
f14c16e509,skip,Untopiced,Revert D32599540: [pytorch][PR] implemented 'torch.distributions.constraints.symmetric' checking if the tensor is symmetric at last 2 dimension.
5fdcc20d8d,jit,Untopiced,[JIT][Symbolic Shape Analysis] expose op shape functions (#68748)
cffad597ea,skip,Untopiced,Tune test_reference_numerics_normal (#68019)
01ddd5dde6,python_frontend,Untopiced,[opinfo] use dtypes instead of dtypesIfCPU (#68732)
b10929a14a,linalg_frontend,Untopiced,Add linalg.lu_factor (#66933)
6ae34ea6f8,linalg_frontend,Untopiced,Revert D32521980: Add linalg.lu_factor
d095f498a0,python_frontend,Untopiced,Tensor docs (#63308)
3315c4b31e,skip,Untopiced,add instructions for unhandled exceptions in assert_close (#68722)
871cd7c5b9,autograd_frontend,Untopiced,"Forward-mode AD support for torch.split, torch.split_with_sizes (#68566)"
f398320e0d,skip,Untopiced,packaging: Include lazy headers in package_data (#68817)
17ba936da0,releng,Untopiced,.github: Migrate XLA tests to GHA (#64320)
3d504ae1b4,skip,Untopiced,[RELAND] Fix Dispatching not considering List[Optional[Tensor]] for dispatch (#68073)
787ded5103,skip,Untopiced,Add lazy::Shape::numel() (#68314)
7b701ce2d4,cpp_frontend,Untopiced,Add set_to_none option to C++ API (#68801)
9ee5db490b,sparse_frontend,Untopiced,neg_sparse: Fix output dtype (#68885)
61a4204d80,sparse_frontend,Untopiced,Sparse CSR CUDA: Add block torch.addmm when mat1 is sparse (#68707)
ee59a09772,releng,Untopiced,Implement sharding for MacOS jobs (#68784)
51f4ac40fd,releng,Untopiced,ci: Use default blank if no TEST_CONFIG (#69008)
250d0bd20b,distributed,Untopiced,[RPC][Dist CI][BE] RPC tests run in subprocess (#68821)
3bd7dbf119,distributed,Untopiced,[Dist CI][BE] Remainder of c10d/store tests run in subprocess (#68822)
f5fa91ba2e,sparse_frontend,Untopiced,Sparse: Add additional opinfo tests (#68886)
d9e7d85390,cpp_frontend,devs,Remove TH/THC Storage (#68556)
f776f30780,dataloader_frontend,Untopiced,Keep the sequence or mapping type in `default_collate` (#68779)
fb63bb60ec,sparse_frontend,Untopiced,Strided masked norm. (#68584)
b468566208,cuda,Untopiced,Add ModuleInfo-based CPU / GPU parity tests (#68097)
62847a2b9c,distributed,Untopiced,Fix bug on empty GLOO_SOCKET_IFNAME_ENV (#68933)
af49805a73,python_frontend,Untopiced,Port lerp to structured kernels (#68924)
89a145fd91,sparse_frontend,Untopiced,Sparse CSR CUDA: Add torch.sparse.sampled_addmm (#68007)
1342f19a8c,cuda,Untopiced,Add ModuleInfo-based device transfer tests (#68092)
be70477a7b,quantization,Untopiced,dbr quant overhead[10/x]: disable torch_function overrides for leaf nodes (#68836)
e3af582f92,quantization,Untopiced,dbr quant overhead[11/x]: speed up module convert hook (#68837)
515db56755,quantization,Untopiced,dbr quant: remove unnecessary outputs hook (#68838)
a03fe9ba61,quantization,Untopiced,dbr quant overhead[12/x]: turn off overrides for module convert output hook (#68839)
2ad4727ad9,quantization,Untopiced,dbr quant: fix debugging fqn info for converted model (#68840)
f253370bb9,quantization,Untopiced,dbr quant overhead [13/x]: cache results of get_module_hook_type (#68841)
27228656e6,fx,Untopiced,[FX][docs] Document gotcha about training flag (#68915)
f1a3512b78,releng,Untopiced,Adding Linux cuda 11.5 workflows (#68745)
7194faed7f,profiler,Untopiced,[PyTorch] Optimzize mergeRunCallbacks for RecordFunction (#68387)
1d0416397a,profiler,Untopiced,[PyTorch] Change from unique_ptr to optional for RecordFunction state (#68397)
178010455d,skip,Untopiced,Vectorized: Use inline namespace instead of anonymous (#67655)
b8c3693281,distributed,Untopiced,Remove autograd-enabled collective APIs from distributed docs (#69011)
863f321c6d,nn_frontend,Untopiced,Fix typo in AdaptiveLogSoftmaxWithLoss docs (#68926)
933d5b561f,cpp_frontend,Untopiced,Fixed links to RNN docs in comments (#68828)
61ea2fc35e,skip,Untopiced,Fix device type / dtype handling for parametrized test names (#65217)
e6a8d15a4c,skip,Untopiced,cpu_kernel_vec: Hoist stride checks out of loop (#68962)
ce53baf573,skip,Untopiced,Merging the implementations of ClearProfiling (#67575)
ac1fe91dc9,skip,Untopiced,Clean up some THC includes (#69024)
8cc9ec2f6b,mobile,Untopiced,Add option to get input dtype from user (#68751)
a93f505ee5,jit,Untopiced,[TensorExpr] IRPrinter: print sizes and name when visiting a Buf. (#68755)
75ce040620,jit,Untopiced,[TensorExpr] Allow for 'keepdim' argument in aten::mean in NNC's external call. (#68756)
3186d36972,jit,Untopiced,[TensorExpr] Supress TracerWarnings in test_unsupported in test_jit_fuser_te.py. (#68757)
fbaa19a6fa,skip,Untopiced,Sparse: Implement simple unary ufuncs operators (#68887)
0cdeb586ae,skip,Untopiced,[LTC] Upstream some utilities (#69046)
ec1339a48b,skip,Untopiced,[CUDA Pinned Memory] Alternative implementation of pinned memory allocator focusing on multi-threaded scalability (#68906)
f7d598948a,build_frontend,Untopiced,Remove native_functions.yaml dependency from TensorModeKernel.cu (#66913)
174eea8a05,build_frontend,Untopiced,"Remove native_functions.yaml dependency from IndexKernel.{cpp,cu} (#66914)"
b47ae9810c,skip,Untopiced,Split cuda: list cpp files that go in _cu library explicitly (#67216)
5b37ac54cb,quantization,Untopiced,dbr quant overhead [14/x]: cache whether an op is a module (#68877)
39ab417107,jit,Untopiced,[Static Runtime] Fix fb::expand_dims schema (#68636)
b83e8d560b,skip,Untopiced,[LT] Sync LTC branch changes on torch/csrc/lazy/core (#69012)
6fea7499c2,dispatcher,bug_fixes,CompositeImplicitAutograd compliance testing (#65819)
d3de3546d9,skip,Untopiced,Revert D32099294: Split cuda: list cpp files that go in _cu library explicitly
7342b654a1,jit,Untopiced,[static runtime] dequantize out variant (#68664)
cfc75c2137,skip,Untopiced,[JIT] Separate GPU implementation of frozen_conv_add_relu_fusion.cpp (#68149)
cd3e37cbe4,jit,Untopiced,[Static Runtime] [Code Cleanup] Reduce indentation depth in ops.cpp (#69028)
8fef7c09f5,jit,Untopiced,Remove finput from slow2d signatures (#68896)
251686fc4c,sparse_frontend,Untopiced,Revert D32706197: Sparse: Implement simple unary ufuncs operators
81246ed01c,skip,Untopiced,Markdown was linking to repo rather than pytorch.org website (#68937)
0aa9d177fe,fx,Untopiced,[fx] remove CPatcher (#69032)
8f9f559453,python_frontend,docs,ammend tensors.rst and torch.rst for doc generation (#69030)
5e0302e1d0,quantization,Untopiced,[quant][embedding qat] Set FakeQuant zeropoint dtype matches observer (#68390)
49abda208b,jit,Untopiced,[JIT] internal build bug fix (#69061)
994f110a6f,distributed,Untopiced,Refactor DDP checkpoint tests (#68792)
fcd1375b2b,distributed,Untopiced,[DDP][BE][Docs] Clarify checkpoint support (#68827)
c48e6f014a,vulkan,Untopiced,[vulkan] Update VMA settings to reduce memory usage (#69088)
22690c2cb6,cuda,Untopiced,Use `cub::FutureValue` to simplify 64bit indexing split of cub scan (#66711)
1d84d8c5d8,mobile,Untopiced,[PyTorch] Remove StringView from RecordFunction interface (1/2) (#68410)
ef7ed082ec,profiler,Untopiced,[PyTorch] Remove StringView from RecordFunction implementation [2/2] (#68411)
e60fd10659,quantization,Untopiced,[fbgemm] remove assumption number of rows is in 32 bit (#69066)
aa2163eba5,releng,Untopiced,.github: Add linux.large instance type (#69165)
6953b7e269,releng,Untopiced,[BE] Fix mypy local run on MacOS (#69097)
f6f1b580f8,skip,Untopiced,Fix mypy in cpp_extension.py (#69101)
e534c5efd7,build_frontend,Untopiced,CMake: Include instead of copying cpu kernel files (#67656)
b05237f5e4,mobile,Untopiced,[Pytorch Edge] Add bool to copy kernel (#69106)
219db3b4e1,python_frontend,Untopiced,Add OpInfo for torch.linalg.tensorsolve (#68810)
8586f374bc,mobile,Untopiced,[Pytorch Edge] Get Operator Version from model file (#68677)
845a82b635,python_frontend,bug_fixes,Debug positive definite constraints (#68720)
999e52a795,distributed,Untopiced,[FileStore] log timeout in err msg (#69167)
7fad758e02,distributed,Untopiced,[FSDP] AutoWrap Main API (#68155)
19b87292fc,jit,Untopiced,Add TE fuser ops (#68825)
a23d1036ab,jit,Untopiced,Add ops for BI (mean) (#68826)
afff381824,skip,Untopiced,Automated submodule update: tensorpipe (#69089)
1da1707568,skip,Untopiced,Sparse: Implement simple unary ufuncs operators (#68887)
e5e0c19882,fx,Untopiced,OpInfo : embedding_bag (#67252)
478069d6f2,skip,Untopiced,Remove duplicate .DS_Store in gitignore (#68981)
f9e69af22e,autograd_frontend,Untopiced,Modify LU_backward and lu_solve_backward to use linalg_solve_triangular (#63569)
840fe8e4e6,releng,Untopiced,Fix MacOS artifact upload (#69188)
c08e95dd9c,build_frontend,Untopiced,Introduce `IS_LINUX` and `IS_MACOS` global vars (#69093)
d507fd63f3,nn_frontend,Untopiced,Check that block height and width are positive in `nn.Fold` (#69048)
486ae5c733,dataloader_frontend,Untopiced,Dataset & IterableDataset attribute errors prints attribute (#69021)
deaf745aee,python_frontend,improvements,Add kl divergence between normal and laplace distribution. (#68807)
23633bdb5c,dataloader_frontend,Untopiced,record the datapipe for each pieces of Dataset (#67613)
1842364b30,sparse_frontend,Untopiced,Strided masked normalize. (#68694)
92f168941e,distributed,Untopiced,remove accidentally committed redundant debug print (#68510)
e6c435bf96,skip,Untopiced,[LTC] Upstream helpers for c10::Device <=> BackendDevice (#69064)
cd043c335f,jit,Untopiced,Revert D32329330: [JIT] Separate GPU implementation of frozen_conv_add_relu_fusion.cpp
2eef5e76db,nn_frontend,Untopiced,add `extra_repr` for `nn.ZeroPad2d` (#69206)
3bf4080fd9,nn_frontend,Untopiced,Change misleading MaxUnpool2d example to better demonstrate output_size usage (#68936)
263125a962,nn_frontend,docs,Fix RAdam docstring on LR default value (#69186)
d72d476875,jit,Untopiced,[pyper] add flag to disable clip_ranges_gather fusions (#69198)
3c1e2ff9eb,nn_frontend,Untopiced,fixing layer_norm cuda bug (#69210)
fced51eaf7,distributed,Untopiced,[torch][distributed] Check for file existence before invoking cleanup logic in FileStore destructor (#68603)
124bb6a19d,skip,Untopiced,RegisterDispatchKey.cpp: remove redundant code (#68983)
b9738e923e,mobile,Untopiced,[Operator Versioning][Edge] Add old models and unittest (#67726)
291e56eda4,mobile,Untopiced,[Pytorch Edge] Update Black Box Api with operator versioning (#68678)
370d0afc1b,sparse_frontend,Untopiced,Strided masked var. (#68738)
929f2a750a,skip,Untopiced,"Back out ""[CUDA Pinned Memory] Alternative implementation of pinned memory allocator focusing on multi-threaded scalability"" (#69191)"
cbe0a38d8c,skip,Untopiced,"Back out ""[CUDA Pinned Memory] Event recording with non-blocking copies should track the storage context, not the tensor data pointer"" (#69193)"
ad182479b0,package,Untopiced,[deploy] docs (#69251)
d8a44270d6,dataloader_frontend,Untopiced,[DataPipe] Simplify BatcherIterDataPipe by removing 'unbatch_level' argument and functionality (#68594)
40fb28ea87,jit,Untopiced,[JIT] Compute input sym shapes in partial eval graph (#68281)
41d35dc201,skip,Untopiced,Add ability for a mobile::Module to save as flatbuffer (#67351)
12621c3a39,distributed,Untopiced,support pure fp16 training in FSDP (#68417)
c60232d89a,distributed,Untopiced,[shard] add back init_from_local_shard_and_global_metadata API (#69226)
ed3b73fd4d,jit,Untopiced,[Static Runtime] Skip ProcessedNode:: verify_no_memory_overlap() for out variants (#68639)
00ebbd5ef6,releng,Untopiced,Revert D32010095: [pytorch][PR] Add ability for a mobile::Module to save as flatbuffer
0465f64bb8,dataloader_frontend,Untopiced,[DataPipe] Adding BatcherMapDataPipe (#68197)
28c519961f,jit,Untopiced,Follow the undefined Tensor <-> None rule better in torch dispatch (#67793)
397183f44c,skip,Untopiced,Add Lazy Tensor codegen infra (#69020)
3157371bb4,quantization,Untopiced,[quant][embedding qat] Fix bug enforcing quant_min <= zero_point <= quant_max for float zeropoint (#68852)
abda069ce2,skip,Untopiced,[quant][embedding qat] Support Embedding QAT via FX API (#68296)
668574af4a,skip,Untopiced,Add efficient zero tensors (#64837)
113684cf81,jit,Untopiced,Fix crash in `checkCustomClassType` if arg is null (#69259)
78ab3cde4a,jit,Untopiced,Do not modify type map from getCustomClassTypeImpl() (#69261)
4484c04513,skip,Untopiced,[quant][embdding qat] Add FX support for QAT EmbeddingBag (#68121)
0de7a618a3,skip,Untopiced,functionalization: update is_aliased() logic (#68881)
698c35e743,vmap_frontend,Untopiced,Add functorch TLS to ATen/ThreadLocalState (#69181)
5c816520b3,fx,Untopiced,ns for fx: fix bug in graph matcher (#69238)
ff3fc37267,distributed,Untopiced,[BE] rewrite ProcessGroupNCCLTest to be MultiProcess (#67705)
17e5200441,skip,Untopiced,use irange for loops 8 (#66743)
84aa1ddedd,quantization,Untopiced,[quant] Remove warning for quantized Tensor in `__dir__` (#69265)
b22e4d4aea,skip,Untopiced,[PyTorch][SR] Add more to() tests & extend debug logging in testStaticRuntime (#67219)
21686923e8,jit,Untopiced,[PyTorch][SR] More debug logging (#67220)
14ed4df305,jit,Untopiced,[PyTorch][Static Runtime][easy] give to_copy_functor a name (#67701)
d673b1ec59,releng,Untopiced,.github: Switch ciflow-should-run to self hosted (#69166)
2ea70a6462,jit,Untopiced,Aloow Union of scalars to be NumberType (#66591)
4056251a18,nn_frontend,Untopiced,Add missing spaces to an error message (#69289)
7142b0b033,releng,Untopiced,.github: Add linux.large to actionlint.yaml (#69304)
97750e03a4,mobile,Untopiced,[torch][edge] Add int to the copy kernel. (#69297)
f587267dc7,onnx,Untopiced,Revert D31705359: use irange for loops 8
bb522c9d7a,releng,Untopiced,"Enabling CUDA 11.5 for binary builds, Adding windows workflows for CUDA 11.5 (#69262)"
dde801686b,jit,Untopiced,Expose MobileCode to python (#66592)
cafcf599d0,linalg_frontend,module: deprecation,Deprecate torch.triangular_solve (#63570)
915c26f588,releng,Untopiced,GHA: preserve downloaded JSONs as artifacts (#69258)
db30696be8,quantization,Untopiced,[pytorch][PR] bug fix for D32374003 (#69278)
52219b1017,nn_frontend,Untopiced,Fix `ChainedScheduler.get_last_lr()` (#69112)
8dafe6e147,releng,Untopiced,Forward fix merge conflict (#69319)
ec4c749024,quantization,Untopiced,Revert D32318435: [quant][embdding qat] Add FX support for QAT EmbeddingBag
a0367f8980,quantization,Untopiced,Revert D32404517: [quant][embedding qat] Support Embedding QAT via FX API
b6bcf5a0f1,jit,Untopiced,[TensorExpr] Un-const TEK::kernel_func_name to allow recompilation. (#68854)
48d7d585c8,jit,Untopiced,[TensorExpr] IR Eval: add more logging. (#68855)
1e9dcdd2a0,mobile,Untopiced,[TensorExpr] TensorExprKernel: support custom-class constants. (#68856)
83c4451f60,jit,Untopiced,[TensorExpr] Add a pass to symbolize an input dimension. (#68857)
791d5087ed,jit,Untopiced,"[TensorExpr] Add lowerings for quantized ops: cat, mul, conv1d, relu. (#69055)"
36ba1b6b3a,jit,Untopiced,Remove unused _convolution_nogroup op (#68829)
5b2586fe09,skip,Untopiced,[testing] Ignore expected_regex in assertRaisesRegex for non-native device (#68723)
db5425bcd2,jit,Untopiced,re-enable layer_norm in autodiff (#69007)
f786b03f98,releng,Untopiced,ci: Migrate docs push to GHA (#69172)
cc85b68984,releng,Untopiced,.github: Fix ci workflows generation (#69329)
9f39a2de0a,python_frontend,bug_fixes,[fix] add range check for `k` kthvalue_cpu (#68863)
33c3c539b6,skip,Untopiced,THPStorage: Prefer intrusive_ptr over owning raw pointers (#69248)
bede18b061,build_frontend,Untopiced,Add support for C++ frontend wrapper on Linux (#69094)
e2c7bd08b9,fx,Untopiced,Add operator div (#68528)
dbc8d9c947,skip,Untopiced,[C10D] [Easy] Use pinned memory for HtoD copies in Reducer:: sync_bucket_indices (#69298)
572c3e3118,skip,Untopiced,Fix some usages of CUDA_VERSION (#69092)
c572a603a6,python_frontend,Untopiced,fix for python 3.10 for gradient opinfo (#68113)
834bd3134e,autograd_frontend,Untopiced,"Back out ""Add efficient zero tensors"" (#69327)"
a07ffe8d0e,fx,Untopiced,"Add OpInfos for combinations, cartesian_prod, sum_to_size, ldexp, as_strided (#68853)"
276cb8f501,mobile,Untopiced,[Pytorch Edge] Make Tracer support xirp metal segmentation (#69328)
cc46dc45e1,jit,Untopiced,[SR] Factor logic that determines managed tensors out of MemoryPlanner (#68295)
21919be96b,build_frontend,Untopiced,CMake: Update precompiled header and fix support (#67851)
e0fb228e03,releng,Untopiced,Revert of adding windows CUDA 11.5 workflow (#69365)
b761172406,distributed,Untopiced,Revert D32786909: [C10D] [Easy] Use pinned memory for HtoD copies in Reducer:: sync_bucket_indices
6ed7354435,jit,Untopiced,[SR][Code cleanup] Typedef/default for kwargs (#69164)
73d2ca20e0,releng,Untopiced,.github: Add credentials for macos test jobs (#69371)
7ca2da14e9,autograd_frontend,Untopiced,Remove finput and fgrad_input from slow3d signatures (#68897)
77ca153d3e,autograd_frontend,Untopiced,Remove columns and ones from slow2d transpose signatures (#68898)
8f8524a447,mobile,Untopiced,Expose is_metal_available in header (#68942)
5a480831e6,releng,Untopiced,.github: Propagate WITH_PUSH to docs jobs (#69372)
2c84b010e6,jit,Untopiced,[PyTorch] Use toObjectRef in JIT interpreter (#69324)
3e45739543,jit,Untopiced,[PyTorch][JIT] Use stack.pop_back() instead of pop(stack) for DROP (#69326)
6baaec30cd,dataloader_frontend,Untopiced,[DataPipe] Adding ShufflerMapDataPipe (#68606)
ae11264583,fx,Untopiced,Fixed type checking errors in node.py (#68124)
6f7a5ddffc,jit,Untopiced,[SR] Use std::vector::reserve in GetLivenessMap (#68884)
a20b9f8d5c,skip,Untopiced,add HPU case for backend_to_string function (#69225)
80a67cd33c,releng,Untopiced,Limit uploading JSONs to trunk (#69385)
088a4feb41,dataloader_frontend,Untopiced,Update the documentation for AMP with DataParallel (#69218)
a813ddf5ec,cuda,Untopiced,CUDACachingAllocator: make an error message more accurate. (#69174)
855365e9c4,skip,Untopiced,Clean up dead code (#69296)
a3ca4c83a6,jit,Untopiced,[PyTorch] Add torch::jit::toString(const Type&) (#66689)
e92b14bf1f,releng,Untopiced,Update CUDA version to 11.3 and setup proper environment variables. (#69383)
da023611d7,cuda,Untopiced,[CUDA graphs] Fixes make_graphed_callables example typos (#69379)
6a4fa86026,fx,Untopiced,Add OpInfos for misc nn.functional operators (#68922)
9663e08674,cpp_frontend,Untopiced,[Static Runtime] Fix a bug that aten::embedding_bag keeps cannot handle resized input tensors (#69219)
bfe5ad28e6,linalg_frontend,Untopiced,[Linalg] Add a runtime switch to let pytorch prefer a backend impl in linalg functions on GPU (#67980)
0bbe21b172,skip,Untopiced,[LT] Upstream more util functions (#69098)
e8f4c9cc40,skip,Untopiced,[LT] Upstream LazyView and view ops IR Nodes (#69277)
b97903abb8,skip,Untopiced,[Core ML] Avoid recompiling models when the OS version is not changed (#69234)
999e93e6a8,skip,Untopiced,[Static Runtime] Move implementation details from impl.h into internal.h (#69274)
29a45f0009,jit,Untopiced,Revert D32743881: [Core ML] Avoid recompiling models when the OS version is not changed
c97dc9286d,jit,Untopiced,Revert D32780415: [Static Runtime] Move implementation details from impl.h into internal.h
3202028ed1,jit,Untopiced,[Core ML] Avoid recompiling models when the OS version is not changed (#69438)
b6f41bb848,cuda,Untopiced,The Jiterator (#69439)
78b7a419b2,skip,Untopiced,Enable native_dropout/backward for lazy (#69374)
b737e09f60,autograd_frontend,Untopiced,expose return_types in Python (#66614)
a974699633,skip,Untopiced,Skips failing ROCm test (#69456)
bf01cd5228,skip,Untopiced,Move THC_sleep to ATen (#69038)
38c576cfef,skip,Untopiced,Clean up CODEOWNERS for .github/ (#69395)
a84ed8be6d,cuda,Untopiced,unify compare kernels (#69111)
68b5c86e65,vulkan,Untopiced,[Vulkan] Implement slice operator (#69382)
456139d0ae,fx,Untopiced,FX pass: fuse_sparse_matmul_add (#69340)
f333cde14e,distributed,Untopiced,"[FSDP] Make recursive_wrap, wrap APIs independent of ConfigAutoWrap. (#68776)"
c95277e92a,distributed,Untopiced,[FSDP] Remove auto_wrap() (#69356)
00245fed96,distributed,Untopiced,"[FSDP] Kill config_auto_wrap_policy, remove policy from enable_wrap, (#69357)"
1859e5f000,distributed,Untopiced,[FSDP] Enforce wrapper_cls as a mandatory kwarg in enable_wrap. (#69358)
f87faf3c29,releng,Untopiced,.github: Volume mount local netrc for docs push (#69472)
4d81175a07,skip,Untopiced,add VSX dispatch for fft_fill_with_conjugate_symmetry_stub (#68914)
e032dae329,skip,Untopiced,[Autograd/Checkpoint] Checkpoint implementation without reentrant autograd (#69027)
7c2489bdae,distributed,Untopiced,[PyTorch][Distributed] Enable Reduce Scatter and modify all_to_all for sharded linear with more test cases. (#68786)
65b0f389d2,distributed,Untopiced,[PyTorch][Distributed] Use auto-grad enabled collections for the shared linear op to enable backward grad calculation (#68096)
9e678446a2,mobile,Untopiced,[Pytorch Edge] Add new_empty_strided to tracer (#69492)
bc89528931,jit,Untopiced,Initialize upgrader and operator version files (#68772)
59e98b66ac,skip,Untopiced,Revert D32704467: [Autograd/Checkpoint] Checkpoint implementation without reentrant autograd
cfe3cbb392,fx,Untopiced,[fx2trt] Use weights shape as normalize shape in layer norm (#69401)
a2d1cadfdb,fx,Untopiced,[fx2trt] Add a helper function to generate specs for dynamic batch size (#69405)
e23827e6d6,fx,Untopiced,[fx2trt] [Prep for release] Add type hints to converters and separate main files (#69458)
3211588308,fx,Untopiced,[fx2trt] Separate sign from `trunc_div` and use it for acc_ops.sign (#69486)
bcd0303834,fx,Untopiced,[fx2trt][easy] add sparse flag to TRTInterpreter (#69495)
bd8a4a9372,skip,Untopiced,[wip][quant][graphmode] produce reference pattern for binary ops and then rewrite to quantized op (#68229)
e55b939732,releng,Untopiced,Enable build-split for all CUDA-11.x version (#69494)
8b1e49635a,skip,Untopiced,[JIT] Separate GPU implementation of frozen_conv_add_relu_fusion.cpp (#68149)
ca945d989a,quantization,Untopiced,[quant][graphmode][fx] Add default_replay_qconfig for ops like reshape (#69249)
2dd46d3aa9,fx,Untopiced,FX: ensure node stack trace survives copying (#69368)
4670f0f2c5,distributed,Untopiced,Set non-default backend names to lower case (#69400)
945d2e380c,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
617a3bd944,releng,Untopiced,GHA: Re enable mac json uploads (#69387)
3edf1b6cee,cpp_frontend,performance,[PyTorch] Avoid no-op shared_ptr dtor when constructing tuple (#69337)
976b076715,releng,Untopiced,[iOS] Add LibTorch nightly build (#69341)
9cb52327a8,quantization,Untopiced,[quant][refactor] Move pattern type definition to ao/quantization/utils.py (#68769)
4478b14e4c,dataloader_frontend,Untopiced,[DataPipe] Unifying API - removing nesting_level argument from MapperIterDataPipe (#69390)
357160e68e,dataloader_frontend,Untopiced,[DataPipe] Unifying API - removing nesting_level argument from FilterIterDataPipe (#69391)
fdfdafd1e6,dataloader_frontend,Untopiced,[DataPipe] Removing usage of unbatch_level from .batch interface and DataFrame (#69393)
bd8d4195a6,dataloader_frontend,Untopiced,[DataPipe] Small change to generation script and update to DataPipe .pyi file (#69392)
aa9fbb9ae9,jit,Untopiced,[JIT] check stack size after calling operator (#68788)
10229e156b,fx,Untopiced,trt engine inspector demo (#66683)
1a202b0c39,autograd_frontend,docs,Docs: Fix broken code syntax in autograd.rst (#69362)
63470f9449,sparse_frontend,Untopiced,Sparse CSR: Implement unary ufuncs (with 0->0 correspondence) (#69292)
60ca6776e2,jit,Untopiced,[JIT] run frozen optimizations on methods other than forward (#68668)
c21169ea41,jit,Untopiced,[JIT] optimize_for_inference on methods other than forward (#69367)
358e908162,jit,Untopiced,Add Union type to TorchScript Language Ref (#69514)
baac51ff4a,releng,Untopiced,Add conda-forge dependency for cuda-11.5 (#69541)
c309637923,releng,Untopiced,Making cuda 11.5 workflows periodic (#69323)
008469c5e2,jit,Untopiced,[SR] Simplify memory re-use algorithm (#68302)
6df7b75186,skip,Untopiced,skip ORT tensor in TensorIterator because it doesn't have storage (#68705)
a8232ee1bc,sparse_frontend,Untopiced,Sparse CSR CUDA: Add block torch.addmv when mat is sparse (#68708)
829b49b867,skip,Untopiced,Output UnionType str rep with () instead of [] (#69502)
cd9da3267c,distributed,Untopiced,Rationalize API exports in torch_python (#68095)
9a7732e852,build_frontend,Untopiced,CMake: Support dynamic codegen outputs (#68246)
1433160a36,onnx,Untopiced,use irange for loops 6 (#66742)
e5a1ee0e5a,quantization,Untopiced,[quant][graphmode] Refactor fusion to use the new Pattern format (#68770)
bede33e3f5,vulkan,Untopiced,[vulkan] Add image format qualifier to glsl files (#69330)
fa39754e11,vulkan,Untopiced,[vulkan] Disable shader optimization to avoid Validation Errors (#69331)
3456c2cbc8,vulkan,Untopiced,Allow build_android.sh to forward Vulkan args (#69332)
049debd97d,autograd_frontend,Untopiced,[Reland][Autograd/Checkpoint] Checkpoint implementation without reentrant autograd (#69508)
8a975c0106,skip,Untopiced,[LT] Sync with the lazy_tensor_staging branch (#69527)
2d38d37f5f,cpp_frontend,Untopiced,use irange for loops (#69533)
05946051f8,quantization,Untopiced,[quant][graphmode] initial support for fusion pattern in backend_config_dict (#69335)
6b950eea27,autograd_frontend,Untopiced,Remove finput and fgrad_input from slow3d transpose signatures (#68899)
e06af79136,fx,Untopiced,Fix sign op converter (#69580)
c236247826,fx,Untopiced,OpInfo tests for `(svd|pca)_lowrank` (#69107)
0ce49000db,distributed,Untopiced,Release GIL during RPC shutdown. (#69586)
2a38e1a76a,distributed,Untopiced,Fix TSAN issue in TCPStore (#69590)
fc8404b5bc,skip,Untopiced,histc: Avoid dispatch in parallel region (#68520)
93aa3603ee,quantization,Untopiced,[quant][embedding qat] Re-Land Support Embedding QAT via FX API (#69333)
f3983f9c47,quantization,Untopiced,[quant][embdding qat] Re-land Add FX support for QAT EmbeddingBag (#69334)
39fb855d91,dataloader_frontend,Untopiced,[DataLoader] Implementing communication processes for Map-style DataPipes (#68549)
fc2614537b,quantization,Untopiced,Updating quantization documentation (#68907)
afaa184b44,jit,Untopiced,[Static Runtime] Avoid evaluating expressions of `Node*` for interpreter fallback op (#69489)
8b20dde932,skip,Untopiced,add python dispatch test back to CI and fix typo in test (#69565)
7e49f4638c,python_frontend,Untopiced,add `OpInfo` for `torch.nn.functional.kl_div` (#65469)
f54745a6ff,python_frontend,Untopiced,add `OpInfo` for `torch.diagflat` (#65680)
2cb385dd6e,fx,Untopiced,"OpInfo for `nn.functional.dropout2d`, revise sample inputs for `dropout` (#67891)"
ee60b5ddf3,skip,Untopiced,Improve efficiency of shape hash by not using tostring (#69496)
30bb4e0071,cuda,Untopiced,Add nvidia-smi memory and utilization as native Python API (#69104)
3e6164449f,skip,Untopiced,Add efficient zero tensors (#64837)
2808563e69,skip,Untopiced,Forward fix for failing master (#69625)
a0efa48c7b,mobile,Untopiced,[Operator Versioning][Edge] Have operator version number available at the loading stage (#67729)
adb619a193,jit,Untopiced,"Adding hardswish, opinfo tests to custom rules (#69399)"
d44d59aa70,distributed,Untopiced,[BE] Enable C++ stacktraces for MultiProcessTestCase (#69175)
7407e3d6fd,nn_frontend,Untopiced,[fix] cross_entropy : fix weight with ignore_index and label_smoothing (#69511)
e279963eef,distributed,Untopiced,Remove remaining THC code (#69039)
51b6981c36,distributed,Untopiced,"[PyTorch Tests] Split out skip logic, make changes for plugins (#67256)"
497ec9d9b8,quantization,Untopiced,Getting NS to work with Ferraris (#68908)
b10381f42d,python_frontend,Untopiced,Port smooth_l1_loss to structured kernels (#67404)
3b27304d20,cpp_frontend,docs,Fix typos in ATen README (#69170)
1c43b1602c,jit,Untopiced,[SR] Scope exit guard for memory planner deallocation (#68795)
7956a405ef,autograd_frontend,Untopiced,Make make_dual also return namedtuple when level less than zero (#68628)
b61c532f96,autograd_frontend,Untopiced,Make make_dual redispatch (#68630)
eb2a803406,distributed,Untopiced,Run test_embedding_bag_with_no_grad_tensors only for TensorPipe (#69626)
3e560239e2,vulkan,Untopiced,[Vulkan] Implement clone operator (#69551)
ecf9c82f24,cuda,Untopiced,Reduce binary size of TensorCompare.cu (#68835)
24d885f5f8,vulkan,Untopiced,[Vulkan] Thread-safe Vulkan backend for OSS (#69576)
dc87cf5fe1,cuda,Untopiced,Fixes mem_get_info when querying on a device other than the current device (#69640)
015e481a41,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
4f5806dee7,sparse_frontend,Untopiced,[AO] Clear the contents of the torch/ao/__init__.py (#69415)
13faaff54c,mobile,Untopiced,[Operator Versioning][Edge] Implement register function for upgrader (#67730)
e948856ce7,sparse_frontend,Untopiced,[sparsity] Add ability to keep sparsity parameters in modules (#66777)
b957b82db7,releng,Untopiced,Replace issue templates with new issue forms - v2 (#69361)
07932e2735,sparse_frontend,Untopiced,[sparsity] Convert function for sparse kernels without a context manager (#66778)
d2917f705a,distributed,Untopiced,Fix errors in `common_utils.py` (#69578)
6de9f0fc94,skip,Untopiced,OpInfo: Allow sample_inputs_func to be any iterable (#69256)
d21646c432,skip,Untopiced,OpInfo: Convert more sample_input_funcs to generators (#69257)
2d5b3101c1,jit,Untopiced,Added ScriptFunction pkl exception for issue #61210 #61381 (#67076)
afb742382a,cpp_frontend,Untopiced,use irange for loops 10 (#69394)
0ccb1dcdbb,python_frontend,bug_fixes,Fix inference_mode decorator (#68617)
17641fed2a,python_frontend,Untopiced,Revert D32942007: OpInfo: Convert more sample_input_funcs to generators
9ad05f2c3a,build_frontend,Untopiced,Upgrade oneDNN to v2.3.3 and package oneDNN Graph API together (#63748)
e963b43691,python_frontend,Untopiced,Extend explanation of `torch.cholesky_inverse` to consider batched inputs. (#69069)
3e20a74b55,jit,Untopiced,[SR] Update memory planner docs (#69559)
193e3c484e,releng,Untopiced,.github: Add fbsync to push triggers (#69718)
d3649309e6,releng,Untopiced,[pytorch][PR] Add ability for a mobile::Module to save as flatbuffer (#69306)
41e1ab0785,skip,Untopiced,Introduce isTensorSubclassLike; add special cases to backwards formulas (#69534)
9aa1b3e396,jit,Untopiced,[Static Runtime] [Code Cleanup] Encapsulate function objects within ProcessedFunction (#69595)
f87f1d08e8,jit,Untopiced,[SR] assignStorageToManagedTensors returns a vector (#69568)
b2e79ed5ec,cpp_frontend,Untopiced,Remove WindowsTorchApiMacro.h in favor of Export.h (#69585)
7dba88dfdb,jit,Untopiced,[nnc][quant] Fix quantized concat (#69596)
3d32a0c139,quantization,Untopiced,"Back out ""[wip][quant][graphmode] produce reference pattern for binary ops and then rewrite to quantized op"" (#69713)"
3f02ad09ec,onnx,Untopiced,[ONNX] shapeValueMap: Represent symbolic shape as value (#68203) (#69545)
be757addfa,skip,Untopiced,Do not use `std::labs` (#69704)
3bb20ae49f,distributed,Untopiced,Make c10d tests -Werror clean (#69703)
a5996a6857,jit,Untopiced,[SR] Wrap check_for_memory_leak with DCHECK (#69588)
aab67c6dff,nn_frontend,Untopiced,Add native masked_softmax (#69268)
531b045446,jit,Untopiced,[tensorexpr] Fix the buf size of discontiguous tensors (#69657)
9962bfb3c9,cpp_frontend,devs,Remove THTensor (#69040)
91d16cb633,jit,Untopiced,[Jit] Fix schema of aten::split int[] version (#69745)
29d759948e,quantization,Untopiced,use irange for loops 2 (#66746)
195b0d0645,skip,Untopiced,fix CompositeImplicitAutograd ops improperly labeled (#69169)
7ea5926130,skip,Untopiced,Make blend operations clang-Wall clean (#69705)
a5ad2cdab5,distributed,Untopiced,Cleanup ProcessGroup.cpp (#69706)
3219f6a487,skip,Untopiced,Make vec512 bfloat16 map function clang-Wall clean (#69707)
e1c583a691,jit,Untopiced,[JIT] simplify logic for merging types during profiling (#69096)
5374d5d8c9,distributed,Untopiced,[shard] fix with_comms wrapper (#69493)
1d269e8c15,jit,performance,[PyTorch] Simple refcount bump fixes in standardizeVectorForUnion & callees (#66695)
d026057bb3,cpp_frontend,performance,[PyTorch] Update SmallVector from LLVM (#69110)
81a60b9813,dataloader_frontend,Untopiced,[DataPipe] Adding output types to DataPipe interface file (#69647)
a5a7e30943,dataloader_frontend,Untopiced,[DataPipe] Adding interface for MapDataPipes (#69648)
77213fa4d3,releng,Untopiced,Fix docker builds for Python-3.6 (#69785)
41c344d460,autograd_frontend,Untopiced,Revert D32739976: fix CompositeImplicitAutograd ops improperly labeled
e305e4d4d8,skip,Untopiced,Suppress common warnings when building by clang (#69710)
59deee8308,skip,Untopiced,Make c10 tests compilable with -Werror (#69711)
731c8255b7,releng,Untopiced,Fix the TorchBench CI when running with a benchmark branch. (#69795)
19fecc63e4,profiler,Untopiced,[PyTorch][kineto] Remove heap-allocated vectors in saveExtraArgs (#69737)
3906f8247a,caffe2,Untopiced,clear predict_net field from PredictorExporterMeta stored in the exporter to save memory (#68485)
17f3179d60,releng,Untopiced,"Back out ""[pytorch][PR] Add ability for a mobile::Module to save as flatbuffer"" (#69796)"
20f7c893c1,jit,Untopiced,Populate runtime with upgrader graph (#68773)
e61fc1c03b,skip,Untopiced,torch/monitor: add C++ events and handlers (#68783)
f575179953,quantization,Untopiced,[quant][fx][graphmode] Move more patterns to use ModuleReLU fuse handler (#69644)
f565167fbd,skip,Untopiced,Revert D32606547: torch/monitor: add C++ events and handlers
db32daf4b2,autograd_frontend,Untopiced,Do not test batched forward grad for inplace ops (#69558)
baf92f9d5a,autograd_frontend,Untopiced,Fix copy_ forward AD to handle broadcasting (#69592)
0dcbd73eee,autograd_frontend,Untopiced,Add some forward AD formulas (#69384)
af7ee9fc01,autograd_frontend,Untopiced,Forward AD for inplace comparison operators (#69597)
a5b5152d7a,jit,Untopiced,Fix typo in aten::full in version_map (#69807)
f0e98dcbd3,nn_frontend,Untopiced,General convolution_backward function (#69044)
0420de3539,jit,Untopiced,[SR] Log SR options (#69809)
fc37e5b3ed,nn_frontend,Untopiced,Hook up general convolution to convolution_backward (#69584)
3d358a7678,distributed,Untopiced,Adds a `maximize` flag to Adam (#68164)
c6c3b43498,jit,Untopiced,[SR][easy] Accessors for value array offsets (#69755)
8dfdc3df82,releng,Untopiced,[ROCm] Refactor how to specify AMD gpu targets using PYTORCH_ROCM_ARCH (#61706)
b08d64202a,cpp_frontend,devs,Remove THGeneral (#69041)
603a1de871,distributed,Untopiced,Fix inefficient recursive update in ShardedTensor.state_dict hook (#68806)
2b81ea4f9a,dataloader_frontend,Untopiced,[DataPipe] Export ShardingFilter (#69844)
b1ef56d646,quantization,docs,[quant][docs] quantized model save/load instructions (#69789)
77a4b89411,releng,Untopiced,Adding windows cuda 11.5 workflows (#69377)
82075c0a19,fx,Untopiced,Create trt plugin base (#69487)
fed9b90ed4,jit,Untopiced,fixing removeProfilingNodes duplicated functions (#1282) (#68804)
6078e12ad6,autograd_frontend,Untopiced,Add forward AD support for as_strided (#68629)
51033ec840,autograd_frontend,Untopiced,Add forward AD layout check for storage numel (#68631)
badf7b0210,autograd_frontend,Untopiced,fix typo changing the generated code (#69899)
4829dcea09,build_frontend,Untopiced,Codegen: Generate seperate headers per operator (#68247)
620a1fcb55,fx,Untopiced,"OpInfos for: normal, bernoulli, multinomial (#66358)"
d90012689f,dataloader_frontend,Untopiced,[DataPipe] Control shuffle settings from DataLoader2 (#65756)
fa615b332d,python_frontend,docs,added set_printoptions examples (#68324)
4f81b2adbb,releng,Untopiced,Remove if conditioning from some MacOS workflow steps (#69788)
f7210f8d90,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
fdcb78df38,nn_frontend,Untopiced,`print` fix in `lr_scheduler` (#68338)
07767569c9,skip,Untopiced,Properly import LooseVersion (#69904)
f7294cd865,jit,Untopiced,[Static Runtime] Skip ReplaceWithCopy when inputs have writters (#69819)
7503ec58b2,jit,Untopiced,[nnc][fix] xnnpack ifdef (#69870)
2c9dd886af,python_frontend,improvements,Modify torch.movedim to handle scalar as no-op (#69537)
8acd0a8b2f,caffe2,Untopiced,Allow row sizes to support int64/size_t. (#69303)
b83b6f7424,skip,Untopiced,allow external backend codegen to be used without autograd kernels (#68529)
aa0cf68c17,skip,Untopiced,allow external backend codegen to toggle whether to generate out= and inplace kernels (#68530)
2e7a91c45f,skip,Untopiced,make codegen'd device guards not cuda-specific. Allow them to be used in external codegen (#68531)
1a299d8f1b,nn_frontend,Untopiced,Add support for transformer layout of masked_softmax (#69272)
1188d89a1d,skip,Untopiced,TestMathBits: Call functions with original sample input values (#68947)
b67eaec853,dataloader_frontend,Untopiced,[DateLoader] more clearly expose 'default_collate' and 'default_convert' to users (#69862)
3e43c478a8,quantization,Untopiced,[Quant][fx] Lower reference conv[1-3]d module (#69228)
fef9981998,skip,Untopiced,Update run_test.py (#69920)
269e92669a,skip,Untopiced,[c2] Remove unused private fields (#69709)
9594a94d80,autograd_frontend,Untopiced,fix CompositeImplicitAutograd ops improperly labeled (#69863)
457ba1dd3e,onnx,Untopiced,"Porting index_add to structured kernels, add an out variant (#65993)"
800a457b6f,distributed,Untopiced,[shard] add ShardedOptimizer (#68607)
c76c6e9bd3,onnx,Untopiced,[ONNX] Add BFloat16 type support when export to ONNX (#66788)
930067d129,releng,Untopiced,Build clang builds with -Werror (#69712)
37ec99c0e4,fx,Untopiced,Open source trt lowering workflow (#69381)
e8d5c7cf7f,nn_frontend,Untopiced,[nn] mha : no-batch-dim support (python) (#67176)
d4f8313497,profiler,Untopiced,Add low level torch.profiler.kineto_profile base class (#63302)
24ee1d13f6,skip,Untopiced,Another attempt to fix version comparison check (#69939)
0ef523633f,skip,Untopiced,Revert D32498570: make codegen'd device guards not cuda-specific. Allow them to be used in external codegen
f6cad53443,skip,Untopiced,Revert D32498569: allow external backend codegen to toggle whether to generate out= and inplace kernels
33363cea64,skip,Untopiced,Revert D32498572: allow external backend codegen to be used without autograd kernels
ebc35a7ead,jit,Untopiced,[JIT] Enable freezing for sparse COO tensors (#69614)
102684b252,quantization,Untopiced,[SR] Fix stack/concat bug (#68777)
bf089840ac,skip,Untopiced,[quant][graphmode][fx] Enable fuse handler for sequence of 3 ops (#69658)
9e4d60a552,mobile,Untopiced,[Operator Versioning][Edge] Use check in cpp source file for upgrader (#67728)
408283319a,mobile,Untopiced,[Operator Versioning][Edge] Change OP to CALL when there is a valid upgrader (#67731)
59000cff91,skip,Untopiced,[quant][fx][graphmode] Add support for conv add pattern in backend_config_dict (#69778)
7a12b5063e,skip,Untopiced,[AutoAccept][Codemod][FBSourceBuckFormatLinter] Daily `arc lint --take BUCKFORMAT`
3b7fc0243c,jit,Untopiced,[PyTorch] Make TypePrinter take const Type& (#69412)
b399a4d7b9,autograd_frontend,Untopiced,Add some reduction forward AD formulas (#69661)
d0fe7db1f6,autograd_frontend,Untopiced,Add formulas for distributions (#69690)
47f11730ec,autograd_frontend,Untopiced,Add testing for forward over reverse gradgrad (#69740)
5c7817fd43,jit,Untopiced,Add test operator in upgrader entry (#69427)
43b8e833e9,jit,Untopiced,Fix bug in aten::full signature in version_map.h to accurately reflect the current schema (#69860)
87bc1f4ed8,quantization,Untopiced,Revert D33024528: [quant][fx][graphmode] Add support for conv add pattern in backend_config_dict
6f9844693f,quantization,Untopiced,Revert D32974907: [quant][graphmode][fx] Enable fuse handler for sequence of 3 ops
d71b8e1a8d,distributed,Untopiced,More distutils.version.LooseVersion changes (#69947)
29914f55bf,skip,Untopiced,Skip print_test_stats checks for tests that use repeat_test_for_types (#69872)
aeedd89d4e,profiler,Untopiced,[PyTorch] RecordFunction: use SmallVector for ObserverContextList (#68412)
873585da2b,jit,Untopiced,[SR] Improve set_inputs (#69087)
c6bcfb152d,profiler,Untopiced,"[PyTorch][easy] Move GlobalRecordFunctionCallbacks{,Entry} to cpp file (#68483)"
986d19c0a7,skip,Untopiced,Avoid adding torch::deploy interpreter library to the data section (#69245)
587f8d9924,skip,Untopiced,OperatorEntry: Avoid unnecessarily templated code (#67986)
24bc3be146,cpp_frontend,Untopiced,[Profiler] Clean up profiler includes. (#69421)
8a08e70bf4,package,Untopiced,Revert D32596676: Avoid adding torch::deploy interpreter library to the data section
1c4c81622c,skip,Untopiced,Support torch.equal for ShardedTensor. (#69734)
a406a427ae,distributed,Untopiced,Revert D33004315: Support torch.equal for ShardedTensor.
bab61be43b,skip,Untopiced,"Codegen: Add root_name property to NativeFunction{,sGroup} (#68687)"
303d60b8da,cpp_frontend,Untopiced,Add TORCH_ASSERT_ONLY_METHOD_OPERATORS macro (#68688)
6ba18ba87e,build_frontend,Untopiced,Codegen: Generate static dispatch headers per operator (#68714)
7bb4b683b5,sparse_frontend,Untopiced,Codegen: Registration now only includes the functions used (#68689)
9c7c1b769a,skip,Untopiced,Functionalization: Only include headers for required ops (#68690)
38cfacd817,skip,Untopiced,Tensor: Define operators override functions in TensorBody.h (#68697)
b28a4100ff,build_frontend,Untopiced,scripts: Fix manylinux2014 promotion to pypi (#70003)
66406ee0f7,jit,Untopiced,[PyTorch][Static Runtime] Fix to() w/dtype bool (#69935)
65ab63310b,profiler,Untopiced,[PyTorch] use div instead of mul when calculating sampling probability (#70001)
32ffad17a9,profiler,Untopiced,[PyTorch][Easy] make GlobalRecordFunctionCallbacks smallvector (#70002)
46ace4ac33,nn_frontend,Untopiced,Add support for masked_softmax when softmax_elements > 1024 & corresponding unit tests (#69924)
98c0fb8b42,sparse_frontend,Untopiced,[sparsity] More descriptive error message for missing parameters (#69895)
bf15dc22bc,cuda,Untopiced,Fix build on latest main branch of thrust (#69985)
3dc3651e0e,autograd_frontend,Untopiced,Remove backward ops for cuDNN convolution (#69901)
4d5dd00e61,autograd_frontend,Untopiced,Remove backward ops for cuDNN transposed convolution (#69902)
dc18048dd8,distributed,Untopiced,[PT-D][Fix] Broken sharded embedding and embedding bag test fix (#69725)
5cc4037369,distributed,Untopiced,[PyTorch][Distributed] Integrate with ShardedOptimizer in the unit test of ShardedLinear (#69569)
eb374de3f5,skip,Untopiced,"Back out ""Revert D32606547: torch/monitor: add C++ events and handlers"" (#69923)"
73a6c36f1b,python_frontend,Untopiced,Add more details to the known limitations section of torchhub docs (#69970)
e6a4988b2d,skip,Untopiced,[LTC] Upstream utils in computation_client (#69621)
28243769f9,skip,Untopiced,[LTC] Upstream several internal ops (#69716)
fe7b6446d5,skip,Untopiced,[LTC] Upstream LazyTensor and LazyGraphExecutor (#69815)
531da0c43b,releng,Untopiced,change asan test shard to 3 (#69843)
a6a1c709ff,cpp_frontend,bug_fixes,Fixed libtorch at::Tensor::print() linking error (#69615)
76d282d447,jit,Untopiced,Nvfuser code bump 12 5 (#69964)
8c7f4a0d0b,jit,Untopiced,[tensorexpr] check for index out of bounds in ir_eval (#68858)
c80b5b8c8f,skip,Untopiced,"Revert D33102715: Back out ""Revert D32606547: torch/monitor: add C++ events and handlers"""
c4281cc92d,distributed,Untopiced,Prototype checkpoint_wrapper (#69955)
b23890177f,mobile,Untopiced,[Operator Versioning][Edge] Codegen upgrader_mobile.cpp (#69194)
ebc66bfeea,profiler,Untopiced,[Profiler] Pull helper methods into dedicated file. (And start `torch/csrc/profiler` folder. (#69255)
7f7966a888,nn_frontend,docs,[Docs] Fix the syntax of documentation (#69958)
c9e898fef8,cpp_frontend,devs,delete TH (#69929)
ff53ed24d2,distributed,Untopiced,fix NameError of docstring in broadcast_object_list (#69810)
9ff8c49ed9,cuda,new_features,Enable cpu scalar arguments for jiterator (#69861)
96fe82ac3c,skip,Untopiced,HANDLE_TH_ERRORS: Move exception translation out of line (#69974)
b4c4a015d6,skip,Untopiced,"Revert D33163841: Revert D33102715: Back out ""Revert D32606547: torch/monitor: add C++ events and handlers"""
8b9b819d22,autograd_frontend,Untopiced,Remove backward ops for miopen convolution (#69987)
1f86e0ee2a,cuda,Untopiced,don't compile pow kernels for non-existent case (#70017)
b199e3c842,distributed,Untopiced,Provide functionality to write custom ShardedTensor ops. (#69874)
15b9e5f8a4,autograd_frontend,Untopiced,Revert D33136054: Remove backward ops for miopen convolution
5f3f327a9d,nn_frontend,Untopiced,update `SequentialLR` signature (#69817)
243e135eb4,sparse_frontend,Untopiced,Sparse CSR CUDA: Add block sparse support for torch.triangular_solve (#68709)
39f65fee47,jit,Untopiced,[jit] Split ClassType into a separate header. (#68036)
d459e79500,mobile,Untopiced,[jit][edge] Remove usage of shared_ptr<mobile::Code>. (#68037)
02c63c3006,releng,Untopiced,extract out c10 targets to the c10 package (#69992)
fa582045fc,skip,Untopiced,Fix lint/mypy violations (#70059)
a73c6a45b6,quantization,Untopiced,[reland][quant][graphmode][fx] Enable fuse handler for sequence of 3 ops (#70006)
62809dc062,releng,Untopiced,.github: Volume mount netrc to home directory (#70057)
9813629500,quantization,Untopiced,[reland][quant][fx][graphmode] Add support for conv add pattern in backend_config_dict (#70007)
92320dfe6e,distributed,Untopiced,[shard] remove set device for nccl (#69946)
ef6f776e82,quantization,Untopiced,[quant][be] Cleanup test cases for eager mode workflow (#69880)
4a6a5d1630,fx,Untopiced,"OpInfos for torch.{flatten, column_stack} (#69237)"
b89c283c80,dataloader_frontend,Untopiced,[DataPipe] Unifying API - removing options to have fn_args and fn_kwargs from IterDataPipes (#69560)
3d51c88032,dataloader_frontend,Untopiced,[DataPipe] Unifying API - removing options to have fn_args and fn_kwargs from MapDataPipes (#69561)
043098ef7f,quantization,Untopiced,[quant][graphmode] Rename backend_config_dict folder to backend (#69882)
c7e0951524,jit,Untopiced,[tensorexpr] Add a stmt recorder to obtain stmt PCs (#66450)
6075ec15b1,jit,Untopiced,[tensorexpr] Add BufMap instruction to reuse the memory of dest buf for src buf (#66451)
bbfd7b75ca,jit,Untopiced,[tensorexpr] Move the allocation of intermediate buffers from TEK to CodeGen (#67143)
ac92f7cc75,jit,Untopiced,[tensorexpr] Remove the optional argument in LoopNest::prepareForCodeGen (#67144)
7abb7667a6,jit,Untopiced,[tensorexpr] Add memory planning to reuse intermediate buffers (#66452)
0f1ceb34ec,quantization,Untopiced,fx quant: refactor qconfig_dict utils to separate file (#69636)
4f450f44bf,quantization,Untopiced,dbr quant: initial support of qconfig_dict for modules (#69719)
b999f87503,quantization,Untopiced,fx quant: move _parent_name to common utils (#69720)
c186773d92,quantization,Untopiced,dbr quant: make fqn during prepare op hook required (#69726)
a4173fc887,quantization,Untopiced,"dbr quant: extend qconfig_dict support to functions, part 1 (#69758)"
f045618dab,quantization,Untopiced,"dbr quant: extend qconfig_dict support to functionals, part 2 (#69766)"
b1d5948b34,autograd_frontend,Untopiced,Remove backward ops for miopen convolution (#69987)
54406314cc,releng,Untopiced,Update PULL_REQUEST_TEMPLATE.md (#70105)
92463573d8,jit,Untopiced,Sanitize string before passing it as shell argument (#70070)
1065739781,sparse_frontend,Untopiced,Fix build on latest main branch of thrust - SoftMax.cu (#70039)
de2d9e2966,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
de296d526f,python_frontend,new_features,move torch.testing from prototype to beta (#69668)
ad79d0dd4b,releng,Untopiced,Add `ciflow/trunk` label (#69575)
950957f857,jit,Untopiced,Fix jit tests assuming sample_inputs is a list (#69975)
ec577300d7,python_frontend,Untopiced,OpInfo: Convert more sample_input_funcs to generators (#69976)
ab2a739851,autograd_frontend,Untopiced,Remove backward ops for miopen transposed convolution (#70063)
35519428a2,autograd_frontend,Untopiced,Remove backward ops for miopen depthwise convolution (#70064)
0d06616c47,nn_frontend,Untopiced,Add `dict` methods to `ParameterDict` (#69403)
e66a8ab4f5,skip,Untopiced,Codegen: TraceType only includes operators being registered (#68691)
9ad940d982,skip,Untopiced,Codegen: ADInplaceOrViewType only include operators registered (#68692)
de992c6b21,python_frontend,improvements,Specify ij indexing when cartesian_prod calls meshgrid (#68753)
bc95e5a196,python_frontend,Untopiced,[ROCm] Skip test_fn_fwgrad_bwgrad_gradient_cuda_complex128 (#70061)
0cfff65395,python_frontend,bug_fixes,Apply contiguous on inputs of cdist backward (#70016)
60eb1e53b2,sparse_frontend,Untopiced,Sparse CSR CPU: Add block sparse support for MKL path (#68710)
2f622e87bd,autograd_frontend,Untopiced,Revert D32596274: Codegen: ADInplaceOrViewType only include operators registered
26e32988bd,jit,Untopiced,Revert D32596264: Codegen: TraceType only includes operators being registered
254360e182,python_frontend,Untopiced,[ROCm] Skip test_fn_fwgrad_bwgrad_* unexpected success tests (#70124)
a6b7521428,build_frontend,Untopiced,always use max cmake when cmake3 and cmake are all existed (#69355)
38e026c14d,cpp_frontend,devs,Add tanh_backward to AT symbols (#70071)
e0f4e28c69,python_frontend,Untopiced,Skip forward-over-reverse gradgrad check for pinv singular on CUDA fo… (#70123)
e35bf56461,releng,Untopiced,[Bazel] Add CUDA build to CI (#66241)
70ed4f3ffc,Uncategorized,Untopiced,Try dropping Torch from typeshed_internal (#69926)
9b14d93d78,releng,Untopiced,Fix bazel workflows (#70137)
9fb199bc12,Uncategorized,Untopiced,Add convolution_backward to aten_interned_strings.h (#70112)
bd9983366b,fx,Untopiced,[fx2trt] Add support for torch.mean (#70052)
9ee3006d58,fx,Untopiced,"[fx-acc][graph-opts] bug fixes for transpose_to_reshape, optimize_quantization, finalize_kwargs_to_concrete"
c463d50098,fx,Untopiced,[fx2trt] Convert to tuple is output_size of adaptive avg pool is an integer (#70144)
5db711f9d3,quantization,Untopiced,[quant][be] Replace QConfigDynamic with QConfig in code (#69864)
9d3a6fa623,Uncategorized,Untopiced,[quant][bc-breaking] Remove QConfigDynamic from quantization api (#69875)
6c68045f60,quantization,Untopiced,[quant][graphmode][fx][be] Fix a typo in quantization/fx/graph_module (#69877)
84b7832010,Uncategorized,Untopiced,Updates CUDA memory leak check to verify against driver API and print more diagnostic information (#69556)
fb34af1b21,jit,Untopiced,[nnc][quantization] Optimize constructTensors in ext functions (#69856)
94abf120c8,quantization,Untopiced,[quant][fx][graphmode][be] Use is_qat instead of model.training as a flag for qat (#69878)
b331752314,quantization,Untopiced,[Quant] Implemented 4 bit embedding op support; added corresponding test case (#69768)
9f512e129b,skip,Untopiced,[Quant] Added 4 bit support for embedding quantized module (#69769)
75718e5059,skip,Untopiced,[Quant][Eager] Added 4 bit support for eager mode quantization flow (#69806)
de4e7dece9,skip,Untopiced,[Quant][fx] Added test for quint4x2 for fx graph mode quantization (#69846)
78f06e0690,jit,Untopiced,fixing conv2d decomposition and tests (#70127)
06d0536dad,Uncategorized,Untopiced,Low precision support for jiterator (#70157)
ef70174f2e,jit,Untopiced,Separate c10::Symbol header from list of interned strings (#69406)
60191196d4,skip,Untopiced,[AutoAccept][Codemod][FBSourceBuckFormatLinter] Daily `arc lint --take BUCKFORMAT`
181120f7d7,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
7ea86dfdb1,Uncategorized,Untopiced,[Profiler] Factor common logic into `torch/csrc/profiler/api.h` (#69459)
8a912014b1,mobile,Untopiced,[Operator Versioning][Edge] Initialize upgrader thread safe (#70161)
ad0cd8a76e,dataloader_frontend,Untopiced,[DataPipe] Improve inline doc and testing for CollatorIterDataPipe (#70139)
978089c381,Uncategorized,Untopiced,Prevent divide-by-zero errors in Timer (#70050)
dd02af6283,nn_frontend,Untopiced,Bilinear no_batch_dim (#69539)
bcc7dbdf37,fx,Untopiced,Change open source unit test deps (#70167)
bcb6076099,Uncategorized,Untopiced,Sparse CSR tensors: storage access should throw (#70072)
41959ce77f,jit,Untopiced,"[JIT] scripting, freezing, serialization for sparse csr (#69555)"
423ce416d8,Uncategorized,Untopiced,Prune osx-arm64 binaries from nightly channel (#70132)
a799ffebd2,fx,Untopiced,Create lower code example (#70142)
65f54bc000,jit,Untopiced,[SR] Optimize VarStack (#68750)
5651e1e3ad,nn_frontend,Untopiced,Add auto_linear formulas and some others (#69727)
fcaecd718a,Uncategorized,Untopiced,Write flaky tests to rockset (#70136)
79a40b22aa,distributed,Untopiced,[Checkpoint] Make checkpoint_wrapper an nn.Module (#70164)
e6d9bb8d57,Uncategorized,Untopiced,reduce the number of instantiations for bernoulli tensor tensor kernel (#70169)
c555b7bacb,caffe2,Untopiced,GHA: Remove caffe2 check in Windows shard 1 smoke tests (#70010)
3e8ef9a272,distributed,Untopiced,Add return type annotation for ShardedTensor (#69945)
f17e76b0f2,Uncategorized,Untopiced,Expand description of bias_sizes arg for convolution_backward (#70195)
6623c4838e,Uncategorized,Untopiced,Handle the corner case when min == max in L2 search (#70207)
24f16de987,jit,Untopiced,[Static Runtime] Support native op split_with_sizes (#69999)
123be0e5b7,quantization,Untopiced,[fusion] Add ConvTranspose+BN fusion support (#70022)
dab3d3132b,skip,Untopiced,Install TensorRT lib on oss docker and enable fx2trt unit test (#70203)
b376d82caf,autograd_frontend,Untopiced,Remove backward op for slow dilated 2d convolution (#70067)
19f898402d,releng,Untopiced,Revert D33241684: [pytorch][PR] Install TensorRT lib on oss docker and enable fx2trt unit test
78bea1bb66,nn_frontend,Untopiced,update example in classification losses (#69816)
fb2a6747b8,quantization,Untopiced,dbr quant: add test for qconfig_dict and methods (#70109)
f291708058,quantization,Untopiced,dbr quant: clean up logging format (#70114)
cce9c9aa45,quantization,Untopiced,dbr quant: stop overridding tensor getters (#70115)
adaf383837,quantization,Untopiced,dbr quant: better fix for bug with recursion on dequantize (#70128)
8e763cd735,Uncategorized,Untopiced,Add explicit OperatorHandle destructor (#70033)
5e222d08a1,Uncategorized,Untopiced,"Revert ""Revert D32498572: allow external backend codegen to be used without autograd kernels"" (#69949)"
e428a90553,releng,Untopiced,Android build migrated to GHA. (#68843)
a197f3fe52,distributed,Untopiced,[FSDP/Checkpoint] Activation offload support in checkpoint_wrapper (#70165)
304efd8e9a,Uncategorized,Untopiced,Change TH_BLAS_MKL into AT_MKL_ENABLED() (#70219)
5504e4ae5c,jit,Untopiced,[nnc] Move DispatchParallel to external_functions (#70221)
c4a6c7a436,Uncategorized,Untopiced,fix cpu binary size increase for clamp (#70168)
91da2d5fa1,jit,Untopiced,[StaticRuntime] Refactor StaticModule to pass in sample inputs (#69473)
bb51519937,cpp_frontend,Untopiced,bug fix FractionalMaxPool2d (random_samples dimensions) (#70031)
c5333cdfba,jit,Untopiced,[nnc] tensorexpr for quantized::add (#70188)
c6d1162325,jit,Untopiced,[jit] Add support for dynamic shape fusion in JIT. (#69474)
a6f953156e,jit,Untopiced,[StaticRuntime] Add TensorExpr fusion with dynamic shapes in SR (#69475)
c321d4c1ca,Uncategorized,Untopiced,[Operator Versioning] Split the upgrader test to a separate file and cover mobile part (#70090)
0544f975e1,distributed,Untopiced,[reland] Support torch.equal for ShardedTensor. (#70145)
a217a62e73,skip,Untopiced,Implementation of Wishart distribution (#68588)
23902fb895,Uncategorized,Untopiced,Fixed typo in torch check for cdist (#70178)
74c834e0dc,dataloader_frontend,Untopiced,[DataPipe] adding a finally statement to ensure hook is reset (#70214)
82c5f298ed,distributed,Untopiced,[shard] fix named_params_with_sharded_tensor (#70228)
21c6de9fdc,Uncategorized,Untopiced,Extend autograd functional benchmarking to run vectorized tasks (#67045)
591ca4d6bc,mobile,Untopiced,[Operator Versioning][Edge] Reorganize upgrader initialization logic for thread safety (#70225)
2d509ff31b,releng,Untopiced,[GHA] Fix doc push jobs (#70269)
6217fee96b,Uncategorized,Untopiced,Revert D33246843: [pytorch][PR] Implementation of Wishart distribution
a86f9806bc,quantization,Untopiced,"Back out ""[Quant][fx] Added test for quint4x2 for fx graph mode quantization"" (#70274)"
47ba28f3b5,quantization,Untopiced,"Back out ""[Quant][Eager] Added 4 bit support for eager mode quantization flow"" (#70272)"
b613fbdbf2,quantization,Untopiced,"Back out ""[Quant] Added 4 bit support for embedding quantized module"" (#70273)"
0bdf4702f6,jit,Untopiced,[jit] Add a new op that composes all of the dynamic shape logic (#69476)
4dec15e6d8,jit,Untopiced,[nnc] Add a run method to TensorExprKernel that takes in output tensors (#69477)
7d4db93a7d,jit,Untopiced,[jit] Handle output tensor being passed in as inputs to TensorExprDynamicGroup (#69478)
633f770c3c,jit,Untopiced,[StaticRuntime] Add out-variant support for TensorExprDynamicGroup op (#69479)
e02d836cb2,Uncategorized,Untopiced,[LTC] Upstream LTCTensorImpl (#70062)
75dbe88b05,dataloader_frontend,Untopiced,[DataPipe] removing unbatch_level from .groupby (#70249)
29f1ccc8f0,Uncategorized,Untopiced,Fix some Composite Compliance problems with binary_cross_entropy backward (#70198)
b5f71375f5,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
31c7e5d629,skip,Untopiced,Install TensorRT lib on oss docker and enable fx2trt unit test (#70203)
69b37a16f3,sparse_frontend,Untopiced,Remove unused CUDASolver.h from SparseCUDABlas (#70281)
4db3a8fc0a,Uncategorized,Untopiced,[nn] TransformerEncoderLayer: no-batch-dim (#69291)
56969bf88a,Uncategorized,Untopiced,make inverse call linalg_inv (#70276)
2806d821b0,fx,Untopiced,Add conversion of torch.permute to acc_ops.permute (#70294)
b7259b8660,quantization,Untopiced,[quant][be] Add a check in prepare_qat to make sure the model is in training mode (#69879)
da63f3f92b,nn_frontend,Untopiced,Corrected typo in Cross entropy formula (#70220)
ce9a2f8ba9,cpp_frontend,Untopiced,[C++ API] Added missing nearest-exact mode and anti-alias flag (#69318)
276253b164,nn_frontend,Untopiced,Fixed wrong return type in ModuleList getitem (#69083)
f36b44bb9e,releng,Untopiced,Remove ciflow_should_run job (#70204)
b37de0a4bb,jit,Untopiced,Update flags in nnc lowering (#70306)
c34aa715fa,Uncategorized,Untopiced,AT_MKL_SEQUENTIAL and build changes (#70259)
1bd147b61a,cuda,Untopiced,Fix masked_softmax's perf for element_size is not 8 (#70271)
0ccccf4ed5,skip,Untopiced,test //c10/... in CI (#69993)
8f4c724bb6,skip,Untopiced,extract //c10/macros into its own package (#69994)
8c41f258f4,skip,Untopiced,allow Bazel to build without glog and gflags (#69995)
7c690ef1c2,cpp_frontend,Untopiced,FractionalMaxPool3d with no_batch_dim support (#69732)
7cdfd86a72,Uncategorized,Untopiced,TestMathBits: test with neg and conj bit set (#68948)
2e94a0d282,Uncategorized,Untopiced,Remove backward ops for NNPACK spatial convolution (#70305)
385c12852e,Uncategorized,Untopiced,[LTC] Upstream LazyTensor <=> at::Tensor utils (#70066)
682fab19d4,jit,Untopiced,[SR] verify_and_correct_memory_overlap handles tensor lists (#69774)
f126501d37,releng,Untopiced,Revert D33141010: allow Bazel to build without glog and gflags
23ab6ce723,Uncategorized,Untopiced,Revert D33141011: extract //c10/macros into its own package
c06b3208d4,releng,Untopiced,Revert D33141012: test //c10/... in CI
a421ee0e52,cpp_frontend,Untopiced,[nn] InstanceNorm : no batch dim for modules (#65323)
bd8e8e3aaf,releng,Untopiced,[GHA] Clean after checkout (#70337)
121d067999,skip,Untopiced,[LTC] Upstream utils to extract BackendDevice from at::Tensor (#70069)
681e78bace,distributed,Untopiced,[Profiler] Address issues from profiler bifurcation. (#70327)
12afe2bb84,python_frontend,Untopiced,update poisson_nll_loss opinfo samples (#70300)
795af1578c,Uncategorized,Untopiced,Revert D33172665: [LTC] Upstream utils to extract BackendDevice from at::Tensor
656d2a7bf6,quantization,Untopiced,[quant][fx][graphmode] Add backend_config_dict for standalone module (#70150)
1d094587ea,skip,Untopiced,[NNC Testing] Randomized loop nest infrastructure (#70174)
b15212c62b,distributed,Untopiced,enable backward pass computation and communication overlap by prefetching all gather (#70235)
a9c7d626e1,distributed,Untopiced,Add the `maximize` flag to AdamW (#70146)
4d49af863f,nn_frontend,Untopiced,GaussianNLLLoss no_batch_dim docs and testing (#69783)
cc8b916395,nn_frontend,Untopiced,Transformer{DecoderLayer} : no batch dim (#70322)
6a84449290,jit,Untopiced,[SR] Fast path for VarStack on scalars (#70210)
d100d98db8,linalg_frontend,Untopiced,`torch.linalg` routines return `torch.linalg.LinAlgError` when a numerical error in the computation is found. (#68571)
3c231e9bd7,distributed,Untopiced,[FSDP] Remove module.wrapper_config support (#70340)
16e6e1a59e,distributed,Untopiced,[Easy] Lint wrap.py file (#70341)
ab57f6d12c,skip,Untopiced,[LTC] Upstream utils to extract BackendDevice from at::Tensor (#70069)
6431ac6c7a,releng,Untopiced,GHA Windows: Propagate exit code from .bat to calling bash script (#70011)
133c7f2cf9,releng,Untopiced,Revert D33301254: [pytorch][PR] GHA Windows: Propagate exit code from .bat to calling bash script
6925576e88,fx,Untopiced,[acc_ops] No longer mark acc_ops.cat as unary (#70365)
3116d87024,nn_frontend,Untopiced,"Add forward AD formulas for `{adaptive_,fractional_,}max_pool{2,3}d_{backward,}` (#69884)"
5ccf28d066,autograd_frontend,Untopiced,Do not use ZeroTensor for inplace ops (#69998)
e429a68478,jit,Untopiced,Allow single node fusion for nvfuser (#70000)
0ee663d2fa,jit,Untopiced,Revert D33234529: [NNC Testing] Randomized loop nest infrastructure
ab4f9862a3,jit,Untopiced,[Compiled Mobilenetv3 Demo] Integrate Compiled Mobilenetv3 into FB4A Playground app (#70370)
911d527b87,skip,Untopiced,Make TorchScript Preserve Fully Qualified Class Name for Python Exceptions (#70339)
df3cbcff28,jit,Untopiced,Add utility methods to find an upgrader (#68355)
63e58d262a,jit,Untopiced,"Extend Graph, CompilationUnit, and schema matching to accept optional operator version number (#69914)"
4ae71c8d34,jit,Untopiced,Add graph op replacement pass (#69915)
bf610f08b0,jit,Untopiced,"Back out ""Make TorchScript Preserve Fully Qualified Class Name for Python Exceptions"""
5a9ea9e386,Uncategorized,Untopiced,Automated submodule update: tensorpipe (#70438)
807f9a828c,skip,Untopiced,Prevent sum overflow in broadcast_object_list (#70336)
b7b32b56f1,distributed,Untopiced,Revert D33281300: Prevent sum overflow in broadcast_object_list
00df885d4e,quantization,Untopiced,quant tests: clean up logs about incorrect tensor copy (#70106)
574dbb584d,quantization,Untopiced,quant tests: fix log spew for HistogramObserver (#70107)
15f14ce0dc,Uncategorized,Untopiced,fix typo in adam docs (#70387)
6f83841582,releng,Untopiced,.github: Temporarily disable xla test config (#70453)
73b5b6792f,nn_frontend,Untopiced,Adds reduction args to signature of F.multilabel_soft_margin_loss docs (#70420)
4ed02748be,distributed,Untopiced,fix typo in the docs of multiprocessing (#70448)
244730eeea,releng,Untopiced,.github: Add needs build for generate-test-matrix (#70456)
c732a26e59,Uncategorized,Untopiced,Add macro to register CPU kernel for all arch types (#70332)
e96dda15e5,autograd_frontend,Untopiced,Remove backward op for slow 2d transposed convolution (#70333)
0fb73035f7,distributed,Untopiced,[Bootcamp Task] Replace string concatenation by fmt::format (#70366)
5e113eb24d,releng,Untopiced,.github: Add linux.4xlarge executor (#70474)
a6eadf9b50,autograd_frontend,Untopiced,Remove backward op for slow 3d convolution (#69978)
a0c99a8d3b,mobile,Untopiced,[Operator Verioning][Edge] Update upgrader codegen with latest change (#70293)
066c9ff08f,releng,Untopiced,Deprecating python 3.6 (#70325)
103fc5f9a5,mobile,Untopiced,Remove unused variable (#70261)
9266b2af73,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
2c67621a19,nn_frontend,Untopiced,"[rnn,gru,lstm]cell : no batch dim (#70236)"
fb736c77a4,autograd_frontend,Untopiced,Remove backward op for slow dilated 3d convolution (#70068)
0460324b9b,nn_frontend,Untopiced,Fix docs rendering for nn.Module.named_modules() (#70491)
8af39b7668,cpp_frontend,Untopiced,AdaptiveLogSoftmaxWithLoss no_batch_dim support (#69054)
2b00dbbbbc,package,Untopiced,fix typos in torch/csrc/deploy/README.md (#70494)
d2abf3f981,nn_frontend,Untopiced,"Added antialias flag to interpolate (CPU only, bicubic) (#68819)"
14f4b91f6e,Uncategorized,Untopiced,Add Nondeterministic Tol to gradient test in test_modules (#69402)
fb78a31916,Uncategorized,Untopiced,Add testing across mem_formats to ModuleInfos (#69317)
c627211651,quantization,Untopiced,[quant][fx][graphmode][be] Change the type for output of convert to be torch.nn.Module (#69959)
18dd5cdba5,Uncategorized,Untopiced,[Operator Versioning][Test] Use hypothesis for better test input data and broader coverage (#70263)
36db501736,autograd_frontend,Untopiced,softplus_backward: remove output arg (#70296)
35251a5528,Uncategorized,Untopiced,[PyTorch] Add Enum to IValue Deepcopy (#69937)
cea3eba617,quantization,Untopiced,[PyTorch Edge][QNNPack] Operator-Level Conv3d Tests (#69309)
9863cd5741,quantization,Untopiced,[PyTorch Edge][QNNPack] Refactor Computing Step Dimensions (#69310)
db37fd3865,quantization,Untopiced,[PyTorch Edge][QNNPack] Depthwise Conv3d Indirection Buffer Setup (#69311)
15d443326c,quantization,Untopiced,[PyTorch Edge][QNNPack] Depthwise Conv3d Weight Packing (#69312)
821c085c9b,quantization,Untopiced,[PyTorch Edge][QNNPack] Depthwise Conv3d mp8x27 (per channel) Neon Kernel (#69313)
3d4590d16f,quantization,Untopiced,[PyTorch Edge][QNNPack] Depthwise Conv3d mp8x27 (per-channel) Sse2 Kernel (#69314)
9c742bea59,quantization,Untopiced,[PyTorch Edge][QNNPack] Enable Depthwise Specific Conv3d Kernel for Kernel Size 3x3x3 (#69315)
14d3d29b16,Uncategorized,Untopiced,make ProcessException pickleable (#70118)
bc40fb5639,Uncategorized,Untopiced,[Reinstate] Wishart distribution (#70377)
401a6b682b,nn_frontend,Untopiced,add BFloat16 support for AdaptiveAvgPool2d on CPU (#56902)
cfc71f56e4,quantization,Untopiced,[quant][fx][graphmode] Support standalone module in _convert_do_not_use (#70151)
e6c3aa3880,autograd_frontend,Untopiced,Remove backward ops for mkldnn convolution (#70467)
7bfaa230be,nn_frontend,Untopiced,[nn] adaptive_avg_pool{1/2/3}d : Error on negative `output_size` (#70488)
6bc06ec3c2,quantization,Untopiced,[PyTorch Edge][QNNPack] Tighten Step Height for Indirection Buffers (#70530)
65faf1a7eb,fx,Untopiced,[fx2trt] Add version check for ProfilingVerbosity bulider config (#70286)
ce86881afa,quantization,Untopiced,[quant][graphmode][fx] Add qat module mapping support in backend_config_dict (#70287)
779f41a78a,Uncategorized,Untopiced,[quant] Add a e2e test for standalone module + custom backend_config_dict (#70152)
fa09099ba3,skip,Untopiced,Codegen: TraceType only includes operators being registered (#68691)
ab49d41bb5,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
1e67570f3a,Uncategorized,Untopiced,Drop omp simd from batch_permutation_op.cc (#70579)
bc3246453b,Uncategorized,Untopiced,Added explicit build command for Windows and clarification on obtaining (#70190)
1b66915f39,mobile,Untopiced,Have type_parser return const reference (#70477)
d9106116aa,mobile,Untopiced,nnapi: Add int32 type torchscript expressions (#70197)
d35fc409ad,skip,Untopiced,Bump version number to 7 and compile old operators with old schema (#68358)
a825351c13,releng,Untopiced,GHA Windows: Propagate exit code from .bat to calling bash script (#70011)
1150046d29,mobile,Untopiced,NNAPI: Add runtime flexible shapes & return shapes (#70334)
7c7eb351c3,nn_frontend,Untopiced,"Populate __name__ for torch.nn.modules.utils.{_single,_pair,...} (#70459)"
8dcfdf39e7,dataloader_frontend,Untopiced,[DataPipe] Renaming FileLoader to FileOpener with deprecation warning for FileLoader (#70367)
f64906f470,Uncategorized,Untopiced,ibm z14/15 SIMD support (#66407)
9cf0de509f,quantization,Untopiced,DispatchStub: Improve type mismatch errors (#67880)
61b562206b,nn_frontend,Untopiced,Fix docstring for nn.ELU (#70574)
0ece9a49d7,jit,Untopiced,Revert D33198155: Bump version number to 7 and compile old operators with old schema
4d08db0cb2,Uncategorized,Untopiced,Flaky tests reporting: use GITHUB_RUN_ID instead of concatenated value (#70604)
d95be99561,skip,Untopiced,Deprecating Python 3.6 (#70493)
bb5b4cceb6,Uncategorized,Untopiced,"Revert ""Revert D32498569: allow external backend codegen to toggle whether to generate out= and inplace kernels"" (#69950)"
7b8c43cd7c,Uncategorized,Untopiced,"Revert ""Revert D32498570: make codegen'd device guards not cuda-specific. Allow them to be used in external codegen"" (#69951)"
6089a0f14a,releng,Untopiced,Extend checkout for pytorch/builder (#70644)
7e58b1dd7b,Uncategorized,Untopiced,Sets device guard in _cudnn_impl functions (#70406)
a60adc7f8a,Uncategorized,Untopiced,fractional_max_pool2d_backward: port to structured kernel (#68245)
95a1952633,Uncategorized,Untopiced,add SparseXPU to dispatch key set autogradother_backends (#70443)
f8f96d4858,Uncategorized,Untopiced,Copy: Re-use existing neg and conj kernel implementations (#68949)
b16b444828,Uncategorized,Untopiced,don't unsqueeze every stack arg if possible (#70288)
8369a46417,caffe2,Untopiced,[maskrcnn] use stable sort in mask rcnn caffe2 ops (#70510)
1adb70c6f0,releng,Untopiced,Revert D33409880: [pytorch][PR] Deprecating Python 3.6
14457bb8cb,autograd_frontend,Untopiced,Remove backward op for slow 3d transposed convolution (#69933)
29ff596dca,nn_frontend,Untopiced,[CUDA graphs] Changes batchnorm to increment num_batches_tracked in place for improved graph safety (#70444)
2292520bdc,Uncategorized,Untopiced,Fix genSparseCSRTensor: generate non-trivial values for uint8 dtype. (#70580)
f8eaebc978,package,Untopiced,Avoid adding torch::deploy interpreter library to the data section (#70208)
d1dbcb1780,jit,Untopiced,Change to use current LLLVM APIs (#70625)
ab7d0df449,sparse_frontend,Untopiced,Support cloning CSR tensors (#70581)
99a10c371f,jit,Untopiced,[PyTorch][Static Runtime] Fix dtype changing between iterations for to() (#67394)
1507ce90b2,jit,Untopiced,[PyTorch][Static Runtime] Avoid managed output tensor DCHECK (#67221)
4d8fc8693c,jit,Untopiced,[PyTorch][Static Runtime] Support memory planning for torch.to() w/o requiring copying (#67223)
10b40acbdb,jit,Untopiced,[PyTorch][Static Runtime] Fast aliasing in select_tensor by manual borrowing (#68122)
44283c2766,mobile,Untopiced,NNAPI: Add qint16 support via int16 (#70621)
12653be434,Uncategorized,Untopiced,[PyTorch] Optimize no input NVTX collection (#70133)
c468e35d83,caffe2,Untopiced,[caffe2] don't use __FUNCSIG__ when building for Windows with clang (#70561)
45f5a3ceb8,vulkan,Untopiced,Fix generating files for Vulkan on Windows (#69696)
80e685e2c0,quantization,Untopiced,dbr quant: start reusing static quant module mappings (#70196)
5b20052857,quantization,Untopiced,dbr quant: start recording ops which are not quantizeable (#70200)
4e90fa6a8c,quantization,Untopiced,dbr quant: break up test class into multiple classes (#70246)
aea3d3ced7,quantization,Untopiced,dbr quant: stop calling eager quant convert (#70247)
1681323ddc,Uncategorized,Untopiced,DOC: Merge extraheader block from theme instead of override (#70187)
e1aa5db108,Uncategorized,Untopiced,Bazel: Only run ATen codegen once (#70147)
adceb13da1,Uncategorized,Untopiced,Copy: Avoid extra dispatch in type-mismatch case (#68950)
657a7e74ed,nn_frontend,Untopiced,Fix docstring for nn.Tanh (#70577)
5543b7ce16,nn_frontend,Untopiced,Fix docstring for nn.Softplus (#70576)
216ae7bc91,nn_frontend,Untopiced,[docs] Transformer: no batch dim support doc update (#70597)
e228b71dae,python_frontend,Untopiced,remove unnecessary skips in rsub OpInfo (#69973)
1aa98c7540,nn_frontend,Untopiced,[docs] multi_head_attention_forward no-batch dim support (#70590)
20489ebdc9,Uncategorized,Untopiced,Increase tensor size for mem check tests (#70603)
8ba27c576c,releng,Untopiced,Upgrade CI to ROCm4.5.2 (#69886)
e1e43c4e71,distributed,Untopiced,Prevent sum overflow in broadcast_object_list (#70605)
4cbe140ec5,releng,Untopiced,Add CI config to test USE_PER_OPERATOR_HEADERS=0 (#69907)
6f473c80a5,releng,Untopiced,Enable fx2trt CI test (#70658)
70d3b2700f,Uncategorized,Untopiced,[LTC] Fix stride accessors in LTCTensorImpl (#70623)
93c7504438,Uncategorized,Untopiced,[PyTorch] Improve StorageImpl::set_data_ptr (#65432)
b60b1b100f,nn_frontend,Untopiced,Set cuDNN deterministic flag for test_conv_double_backward_cuda (#69941)
616afcf981,jit,Untopiced,[jit] [shape analysis] Move constant tensors out of fused subgraphs during generalization (#70320)
34c49d3d3b,Uncategorized,Untopiced,Document torch.quantile interpolation kwarg (#70637)
025cd69a86,Uncategorized,Untopiced,[AMD] Fix some legacy hipify script (#70594)
6c4437118b,skip,Untopiced,Deprecating Python 3.6 (#70493)
be298212a6,cuda,Untopiced,reduce igamma instantiations (#70666)
395f853770,releng,Untopiced,Parallelize docker dependency builds (#70866)
b283b1de39,caffe2,Untopiced,Cleaning code in fbcode/caffe2/c10/core/TensorImpl.h (#70588)
a5bc44422a,Uncategorized,Untopiced,[PyTorch] Remove the List/Dict move operations (#69370)
2431218ee4,Uncategorized,Untopiced,Jiterates more ops (#70663)
ce409d8f50,nn_frontend,Untopiced,docs: clarify smooth l1 == l1 when beta == 0 (#70673)
f9e1a1c97f,Uncategorized,Untopiced,Increase tolerance for test_adadelta (#69919)
8e6d1738a4,skip,Untopiced,Per-overload torch.ops API (#67254)
3a21f38a2e,Uncategorized,Untopiced,Integrate multi_tensor zero_grad into Optimizer base class (#69936)
cfc5519661,sparse_frontend,Untopiced,Support Sparse CSR transpose. Fix clang-tidy warnings. (#70582)
917d56a7e4,Uncategorized,Untopiced,Copy: Fix conj bit being ignored on type mismatch (#68963)
3f53365086,fx,Untopiced,define `get_dot_graph` (#70541)
a35b4b49d2,linalg_frontend,Untopiced,Add linalg.lu_factor (#66933)
012c38e04d,Uncategorized,Untopiced,Add contiguous_strides as a correct replacement of defaultStride (#67789)
4d4e81d869,Uncategorized,Untopiced,Make linalg.lu_factor structured (#66934)
08ef4ae0bc,linalg_frontend,Untopiced,Remove unnecessary sync in linalg.det (#67014)
baeca11a21,linalg_frontend,Untopiced,Remove random_fullrank_matrix_distinc_singular_value (#68183)
1a061c7fe1,python_frontend,Untopiced,"Merge index_{add,fill,copy,select} sampling (#68184)"
2367face24,autograd_frontend,Untopiced,Prefer maybe_multiply when multiplying by a constant (#68185)
884aa2baad,releng,Untopiced,ci: Make linux.*xlarge non-ephemeral (#70869)
402f2934bf,jit,Untopiced,Revert D33262228: Per-overload torch.ops API
8bdbe94344,releng,Untopiced,Add forward compatability tests in CI (#64139)
b0fdca8855,skip,Untopiced,Bump version number to 7 and compile old operators with old schema (#68358)
748790588c,cpp_frontend,Untopiced,Upgrading the loop to use irange (#70326)
6773589a06,cuda,Untopiced,Drop some unused variables (#70879)
4916a21f10,quantization,Untopiced,quantization: fix scale+zp serialization of quantized BatchNorm{2|3}d (#70432)
3051aabd0e,nn_frontend,Untopiced,Add forward AD formulas for convolution and some others (#69956)
ff408fca7f,autograd_frontend,Untopiced,Forward AD formulas for activation backwards (#70460)
bc514cb425,Uncategorized,Untopiced,Skip distributed tests if built with USE_DISTRIBUTED=0 (#70677)
c00d33033c,nn_frontend,Untopiced,Remove repeat test for types in test nn (#70872)
578fe11673,cuda,Untopiced,[pytorch][aten][cuda] fix LpNormFunctor (#70601)
10b55648f5,releng,Untopiced,CI: remove unused yaml and make upload_binary_size_to_scuba script work with GHA (#70643)
19f04da21e,releng,Untopiced,GHA: Make WORKFLOW_ID not a concatenation of run_id and run_num (#70938)
81b52c290f,fx,Untopiced,Adding leaky_relu support for fx2trt (#70799)
524bbb1442,Uncategorized,Untopiced,[LTC] Sync gen_lazy_tensor.py from the staging branch (#70385)
dfb807d65e,quantization,Untopiced,dbr quant: do not attach auto_quant_state to observers (#70256)
c21a540866,quantization,Untopiced,dbr quant: support dynamic linear (#70257)
f742853838,quantization,Untopiced,dbr quant: support functional linear without bias (#70328)
a8929c3278,quantization,Untopiced,dbr quant: unbreak case when child module not returning any outputs (#70329)
b12852eb41,quantization,Untopiced,"dbr quant: support for custom leaf modules, part 1/x (#70330)"
423d8aabbd,quantization,Untopiced,"dbr quant: support for custom leaf modules, part 2/x (#70335)"
c3f0c77b64,quantization,Untopiced,"dbr quant support for custom leaf modules, part 3/x (#70349)"
9c455d7086,quantization,Untopiced,dbr quant: add limited support for `torch.nn.ModuleList` (#70372)
4fa70a2483,Uncategorized,Untopiced,[pytorch] fix hipify_python (#70619)
e7602a1e30,sparse_frontend,Untopiced,Fix multiplication of 0-D sparse tensors (#70749)
b7742b437a,nn_frontend,Untopiced,Allow RNN hidden_size to be 0 (#70556)
6896b2d734,jit,Untopiced,[NNC Testing] Randomized loop nest infrastructure (#70410)
7b8f73dd32,nn_frontend,Untopiced,No-batch-dim support for ConvNd (#70506)
cc7382dd92,Uncategorized,Untopiced,Enable upgraders in TS server (#70539)
e6befbe85c,cpp_frontend,Untopiced,Add flag to optionally average output attention weights across heads (#70055)
ddea6980fe,jit,Untopiced,[PyTorch][JIT] Don't refcount Type singletons (#69579)
880a5b9ea6,jit,Untopiced,[PyTorch] Move prim string ops to JIT op registry (#70501)
6eba936082,skip,Untopiced,[rnn/gru] no batch dim (#70442)
32bf5e0ef9,quantization,Untopiced,Add native impl of gelu for QuantizedCPU (#69968)
70b18b9511,jit,Untopiced,Fix comment indentation issue (#70227)
c18e6b790e,fx,Untopiced,"Adding elu,selu,softsign support for fx2trt (#70811)"
22f5280433,releng,Untopiced,add very dumb retry to ecr gc
23f902f7e4,Uncategorized,Untopiced,Fix incorrect variable in autograd docs (#70884)
df6eb9bbab,fx,Untopiced,Fixed to_folder not saving dtype (#69983)
36d9e03ab7,Uncategorized,Untopiced,Reserve vector in gather_ranges_to_dense_op.h (#70478)
1622546050,onnx,Untopiced,use irange for loops (#70248)
a311cfa800,nn_frontend,Untopiced,Revert D33460427: [pytorch][PR] [rnn/gru] : no batch dim
cdbf83b0c3,jit,Untopiced,[TensorExpr] Add helper passes for AOT pipeline. (#66514)
5d7cc8f22a,jit,Untopiced,[TensorExpr] Add some graph-rewrite passes to prepare models for AOT compilation. (#66515)
8223ef1cd8,jit,Untopiced,[TensorExpr] Clean-up logic for copying input tensors and remove some dead code. (#70535)
1cdc643714,jit,Untopiced,[TensorExpr] Add a pass for trimming JIT graphs. (#66847)
87139d8532,Uncategorized,Untopiced,[LTC] Sync LazyGraphExecutor and LazyTensor with the staging branch (#70867)
d697bb4220,jit,Untopiced,Adapt llvm_codegen.cpp to LLVM TOT (#70810)
6e16c9bb1d,distributed,Untopiced,Add support for deleteKey for FileStore (#69953)
9ad21091dd,jit,Untopiced,[SR] Give VarStackNodeWrapper an iterator (#69922)
f135438d3b,nn_frontend,Untopiced,Dispatch to at::convolution intead of at::_convolution in _convolution_double_backward (#70661)
997fa8671d,nn_frontend,Untopiced,Fix docstring for nn.Hardsigmoid (#70987)
2378421340,distributed,Untopiced,Implement torch.allclose for sharded tensor. (#70331)
40c512f52c,releng,Untopiced,split cuda for all 11.X (#70899)
4e7e8f2826,Uncategorized,Untopiced,[PyTorch] Outline destructor of CppFunction (#63688)
8dfff8b2e2,Uncategorized,Untopiced,Fix scatter for empty indexes (#70662)
08074c8f2d,Uncategorized,Untopiced,Update gradcheck.py (#70950)
c6e727d05b,Uncategorized,Untopiced,Fix adamw formula doc (#68587)
195181d4df,releng,Untopiced,"Revert ""add very dumb retry to ecr gc"""
dd1121435b,Uncategorized,Untopiced,SequentialLR update _last_lr on step (#70558)
0408449244,jit,Untopiced,[jit] Reclaim some binary size. (#68038)
649dda9fee,jit,Untopiced,[jit] Implement DynamicType for TorchScript runtime. (#68136)
0517e719ac,Uncategorized,Untopiced,[jit] Add conformance test for DynamicType with server JIT types. (#69482)
1011ac188f,mobile,Untopiced,[jit][edge] Create DynamicType for OptionalType in mobile. (#68137)
bc026c0577,Uncategorized,Untopiced,[jit] Split Union type and Optional type to separate impl file. (#69483)
0a002f879e,releng,Untopiced,"Actually clean on clean workspace, including hidden files (#71018)"
338eb1b2b3,Uncategorized,Untopiced,[LTC] Export torch::lazy::GetBackendDevice() (#70963)
c8b897333c,nn_frontend,Untopiced,[rnn/gru] no batch dim (#70977)
ed50a35cf8,distributed,Untopiced,[Model Averaging] Update the documentation of PeriodicModelAverager (#70974)
a70297e7cb,mobile,Untopiced,NNAPI: quant logistic fix (#70847)
704fbc29ae,autograd_frontend,Untopiced,Remove backward op for 2d depthwise convolution (#70461)
3febe0d986,autograd_frontend,Untopiced,Remove backward op for 3d depthwise convolution (#70462)
0721fc6474,distributed,Untopiced,Decouple MapDataPipe from Dataset (#70991)
9032d73f3b,releng,Untopiced,Disable cpp tests in multigpu job (#71015)
d7db5fb462,nn_frontend,Untopiced,ctc loss no batch dim support (#70092)
d583eca8c3,releng,Untopiced,Add workflow to sync `fbsync`->`master` (#71013)
ad88354e25,Uncategorized,Untopiced,torch.futures doc formatting (#70630)
00e5610914,quantization,Untopiced,FX quant: allow duplicate named_modules during fbgemm lowering (#70927)
c59c86706e,quantization,Untopiced,[quant] Add back README.md for backend_config (#70964)
d26e5ced72,onnx,Untopiced,Add missing docstrings for ONNX converter API. Fixes #67393 (#67640) (#68489)
0cd474b2ce,quantization,Untopiced,fix op not scriptable
0eb2fc608c,fx,Untopiced,[fx_acc] ensure all acc ops args to be keyword arguments (#70952)
62909facb3,jit,Untopiced,[jit] Decouple ivalue.h from jit_type.h (#70119)
53b9c0f12d,mobile,Untopiced,[jit] Polymorphic IValue::type() for DynamicType. (#70120)
3f3eae6737,Uncategorized,Untopiced,[jit] Split Tensor type implementations to separate file. (#70121)
0a921ba0d0,skip,Untopiced,[jit][edge] Do not reuse mobile type parser for all unpicklers. (#70338)
f626bef598,nn_frontend,Untopiced,Fix docstring for nn.Hardshrink (#71012)
9762aa0fdc,mobile,Untopiced,Revert D33284352: [jit][edge] Do not reuse mobile type parser for all unpicklers.
704af23ee4,Uncategorized,Untopiced,Use a reference in GetSingleArgument (#71007)
11aa1961c1,Uncategorized,Untopiced,Use (void)error_unused to avoid unused warning (#71000)
d1e049c306,cuda,Untopiced,Fix some unused variable warnings and make some stuff const in ReplicationPadding.cu (#70998)
49a07c8922,cuda,Untopiced,Suppress some unused variable warnings in Sorting.cu and TensorTopK.cu (#70999)
785b6905de,Uncategorized,Untopiced,reduce plan generation log spam (#70880)
9267fd8d73,Uncategorized,Untopiced,[WIP] [ATen] Add native_multi_attention_self_attention CPU + GPU implementation (#70649)
2bed616e0f,distributed,Untopiced,[Dist tests] Make event_listener work for all dist tests (#70628)
fca8a0acaa,package,Untopiced,Prevent import race condition that leaves torch.package.PackagePickler with unwanted dispatch table entries. (#71025)
b27dfa70c4,caffe2,Untopiced,caffe2: disable TensorImpl static_assert (temporary)
e1b84e1b6b,Uncategorized,Untopiced,fix loading of older models that don't have maximize (#71023)
c4400fc431,distributed,Untopiced,Retire repeat_test_for_types (#71033)
118bd82dde,package,Untopiced,detect mocked module on saving pass (#70641)
3ef10da97d,package,Untopiced,add support for pickle v4 (#70642)
e9a8bb59b4,jit,Untopiced,Move the apply_tensor_props into its own function for more public use (#67786)
87484d67e3,releng,Untopiced,.github: Enable linux binary builds (#68388)
62441157e3,jit,Untopiced,Have getFilesToLevels return a reference (#71047)
4b47047dae,onnx,Untopiced,[ONNX] Add support for shrink ops (#66969) (#68492)
840459a269,onnx,Untopiced,[ONNX] Relax constant_fold gather with indices rank > 1 (#68140) (#68493)
0adc7cc546,jit,Untopiced,Inline Fallback Functions For Debugging (#70463)
c8332256ee,jit,Untopiced,[JIT] Refactor SR invocation of fusion (#70508)
fb66f561b1,jit,Untopiced,Add copy out to the fallback path in SR invocation of composed op (#70871)
30699cbfd5,mobile,Untopiced,Reland D33284352: [jit][edge] Do not reuse mobile type parser for all unpicklers. (#71048)
cfc1117591,Uncategorized,Untopiced,Update sparse.rst to warn about _values() (#71088)
3fbff80bea,releng,Untopiced,ci: Move MAX_JOBS to not set on Darwin (#71122)
49ed097ebe,fx,Untopiced,Add documentation for lowering (#71116)
569aeec1bc,distributed,Untopiced,fix typo in debugging_hooks.py (#70956)
80659b71a5,quantization,Untopiced,Hoisting common expressions out of If blocks [retry] (#65645)
7a08030903,releng,Untopiced,Fix fx2trt CI test trigger condition (#71014)
78994d13c0,autograd_frontend,Untopiced,"Add forward AD formulas for {batch,layer,group}_norm (#70355)"
7397683b57,autograd_frontend,Untopiced,"Add forward AD formulas for mv, scatter_add, _s_where (#70468)"
a606ea73d6,skip,Untopiced,[ONNX] Link to the wiki (#68505) (#69544)
b12ca69179,jit,Untopiced,[jit][edge] Migrate DictType to DynamicType on mobile. (#70202)
043e84b3d2,skip,Untopiced,Per-overload torch.ops API (#67254)
59e166feb2,quantization,Untopiced,[Quant][DBR] Add test for serialization (#70078)
33a5905cc6,quantization,Untopiced,[quant] fix reduce_range warning (#71027)
5cae40c169,jit,Untopiced,[pytorch][aten][cuda] move CUDAGeneratorImpl.h to ATen/cuda (#70650)
40b80aa490,mobile,Untopiced,[jit][edge] Migrate TupleType to DynamicType on mobile. (#70205)
fdda7b5e8a,skip,Untopiced,[Codemod][FBSourceBlackLinter] Daily `arc lint --take BLACK`
fb8a9732d9,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
d6b7d69d8b,releng,Untopiced,Python3.10 migration adding to binary linux tests (#71130)
ac0d131291,dataloader_frontend,Untopiced,Decprecating routed decoder (#70990)
1b496cf158,Uncategorized,Untopiced,"Fixes doc errors in `Tensor.triu()`, `Tensor.tril()`, `Tensor.ravel()`. (#71057)"
3c2ae2b47c,onnx,Untopiced,Revert D32994274: [ONNX] Link to the wiki (#68505)
cf61738097,Uncategorized,Untopiced,Drop unused variables; make things const; use some auto (#71107)
85c6489cdc,releng,Untopiced,ci: unquote env variables (#71139)
18e1e1d4d3,skip,Untopiced,.github: Re-enable xla test config (#71008)
4f35b9144c,mobile,Untopiced,[jit][edge] Migrate ListType to DynamicType on mobile. (#70212)
940b89b03f,releng,Untopiced,Disable Python-3.6 binary builds (#71163)
14922a136f,releng,Untopiced,Revert D33480077: .github: Re-enable xla test config
a8612cd72a,nn_frontend,Untopiced,Skip failing tests in test_nn if compiled without LAPACK (#70913)
1c8b167327,sparse_frontend,Untopiced,Move implementation of empty_like for sparse COO (#71103)
6c03f8d9e5,cuda,Untopiced,Drop unused variables and add some const (#71106)
45b0bafb38,cuda,Untopiced,Drop more unused variables (#71123)
7d6535cab3,distributed,Untopiced,Make Kineto + distributed a warning rather than an error (#71120)
edf15ebbc2,releng,Untopiced,Adding python 3.10 binary workflows (#71132)
2c8cb8a964,quantization,Untopiced,Speed up quantized upsampling for channels last (#70903)
ecb6defa36,Uncategorized,Untopiced,Fixed docs for forward_ad.make_dual (#71159)
e7634f83ce,jit,Untopiced,[jit][edge] Migrate base types to DynamicType on mobile. (#70233)
433cf44b79,releng,Untopiced,delete ecr_gc_docker job (#71178)
a3b7dd7b78,Uncategorized,Untopiced,Enable nested default hooks (#70932)
786f946098,Uncategorized,Untopiced,[Profiler] Add glue layer to reduce the use of `#ifdef USE_KINETO` in the profiler code. (#69798)
410e91adee,skip,Untopiced,Performance and memory improvements to batched torch.linalg.solve (#69752)
831c129e85,Uncategorized,Untopiced,fx quant: fix test_fx_acc_tracer::test_quantized_batch_norm2d (#71175)
40121456af,sparse_frontend,Untopiced,Sparse CSR: Add `torch.randn_like` (#68083)
9465c24245,jit,Untopiced,[jit][edge] Use dynamic type instead of union types for schema parsers. (#70509)
97e8dcba5e,jit,Untopiced,Fix mis-specified device arg name (#69645)
772b3e92bf,jit,Untopiced,Parse symbolic shapes (#69775)
47ad6628f1,jit,Untopiced,add optional refining (#69776)
9bccb31306,jit,Untopiced,Remove precise tuple construct flag (#71121)
4a8d4cde65,jit,Untopiced,Fix for tensor in list return added to wildcard set (#71170)
93b2399c6c,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
523d448968,Uncategorized,Untopiced,Remove deprecated cuDNN convolution ops (#71128)
b652887ad7,Uncategorized,Untopiced,improve documentation of comparison internals (#68977)
8d05174def,Uncategorized,Untopiced,make meta tensor data access error message for expressive in assert_close (#68802)
802dd2b725,Uncategorized,Untopiced,change sparse COO comparison strategy in assert_close (#68728)
b0a10a709f,Uncategorized,Untopiced,add explanation of quantized comparison strategy in assert_close (#68911)
49a5b33a74,Uncategorized,Untopiced,add a equality comparison helper for assert_close internals (#69750)
928ca95ff0,Uncategorized,Untopiced,fix TensorLikePair origination (#70304)
e1aea9b968,Uncategorized,Untopiced,Add retry to disabled tests file download (#71030)
061be8d600,linalg_frontend,Untopiced,Correct forward AD for linalg.eig and add checks (#70527)
97585ae1e7,linalg_frontend,Untopiced,Simplify forward / backward AD for linalg.eigh and add checks (#70528)
975e7d246e,jit,Untopiced,Remove ignore shapes arg (#71144)
39be20f259,jit,Untopiced,[JIT][NNC] Add handling of strides to dynamic shape support. (#70464)
5480deb183,jit,Untopiced,Add support for permutting dynamic fusion group outputs to channels last format (#70656)
706777bf56,jit,Untopiced,Disable the output invocation in jit (#71138)
6a40bb0fdf,dataloader_frontend,Untopiced,[DataPipe] Update deprecation warning (#71171)
54fe2741a1,fx,Untopiced,[fx2trt] break down div (#71172)
149f5ffa36,jit,Untopiced,Fix inconsistency between new and old upgrader design (#71185)
8f4cec2231,caffe2,Untopiced,[warnings][Caffe2] Suppress warnings in caffe2 headers (#71196)
70951884d4,jit,Untopiced,Add option to load historic operators in IR when the operator is deprecated (#71148)
fd0d4bef03,releng,Untopiced,Edit cron to make the docker jobs run hopefully (#71232)
a71b4dc164,releng,Untopiced,Update nightly wheels to ROCm4.5.2 (#71064)
7884143dff,package,Untopiced,Legacy support for embedded interpreter (#71197)
4d28cef03a,Uncategorized,Untopiced,Added AutocastCPU string (#70013)
479ce1c3a0,Uncategorized,Untopiced,[PyTorch] Add isUndefined to ExclusivelyOwnedTraits<TensorBase> debug msg (#70638)
90ef54f8ea,Uncategorized,Untopiced,[PyTorch] Remove buggy ExclusivelyOwnedTraits<intrusive_ptr<T>> (#70647)
bfe1abd3b5,Uncategorized,Untopiced,torch/monitor: add pybind (#69567)
2290976880,releng,Untopiced,ci: Comment out pull_request trigger for binary builds (#71244)
5749be4678,dataloader_frontend,Untopiced,Fix the shape inconsistency of `out` and `elem` tensor (#71065)
de902b5d02,fx,Untopiced,[FX] Add a default_value arg to Graph.placeholder and fix split_module (#71016)
385773cb77,nn_frontend,Untopiced,add BFloat16 support for MaxPool2d on CPU (#56903)
6c1be299c1,caffe2,Untopiced,caffe2/c10/core/TensorImpl.h: adapt to clang 12 (#70973)
e1f01d2c01,releng,Untopiced,".ci: Add nightly trigger, remove CircleCI linux binary builds (#70957)"
3c0c5bde0e,Uncategorized,Untopiced,[cmake] Uncomment binaries (#71157)
de62bcac66,skip,Untopiced,Implement the patterns module for the multi subgraph rewriter. (#71181)
cc55da8a9b,caffe2,Untopiced,[caffe2/server quant] use new depthwise conv fbgemm interface (#71166)
ad803936d1,skip,Untopiced,Codegen: ADInplaceOrViewType only include operators registered (#68692)
67941c8a94,Uncategorized,Untopiced,"Document `torch.cuda.ExternalStream`, `torch.cuda.caching_allocator_alloc` and `torch.cuda.caching_allocator_delete` (#70126)"
9ca367d48b,jit,Untopiced,[nnc] Use given kernel function name while emitting code (#67781)
7a93d8bb2d,fx,Untopiced,Revert D32374542: Implement the patterns module for the multi subgraph rewriter.
1bc3571078,releng,Untopiced,[pytorch][PR] Add ability for a mobile::Module to save as flatbuffer (#70201)
cd253938a9,jit,Untopiced,[PyTorch][SR][easy] s/input_or_constant_aliases/external_aliases/ (#69852)
1bbea3c3a2,jit,Untopiced,"[PyTorch][JIT] Support mayContainAlias(Value*, ArrayRef<Value*>) (#69853)"
84d4087874,fx,Untopiced,Fix trt const_fold as output use case (#71194)
f6b804ba9f,Uncategorized,Untopiced,Fallback to server JIT type for type checking.
ffdc0e23af,jit,Untopiced,[SR] Add various missing native ops (#71113)
3cc34a4502,jit,Untopiced,[PyTorch][Static Runtime] s/toObject/toObjectRef/ in native ops (#71238)
ff78c73286,onnx,Untopiced,[ONNX] Remove f arg from export_to_pretty_string (#69045) (#69546)
60632a00fe,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
1e3893ecbb,dataloader_frontend,Untopiced,[DataPipe] Removing deprecated DataPipes (#71161)
188b744390,releng,Untopiced,Make docker build cron once a week and not every hour on Wed (#71255)
e1b9d5854a,quantization,Untopiced,[Quant] Add quantized input tensor data type checks (#71218)
4a26624670,quantization,Untopiced,[Quant] Add a guard against shapes for qnnpack qadd (#71219)
003c94c790,quantization,Untopiced,[Quant] Templatize activationLimits function (#71220)
40eb004da5,releng,Untopiced,Use nightly-binary instead of nightly to deduplicate refs for nightlies (#71270)
37eaf7640f,releng,Untopiced,"Revert ""Revert D33480077: .github: Re-enable xla test config"" (#71202)"
83b45fe166,quantization,Untopiced,[ao] disabling dynamic conv/convT ops (#71110)
1de830a985,jit,Untopiced,Use `ptrdiff_t` rather than `ssize_t` (#71271)
71b274d34d,Uncategorized,Untopiced,[pytorch] move ATen/CUDAGeneratorImpl.h to ATen/cuda (#71224)
b64946cbc1,fx,Untopiced,[acc_normalizer] Delete is_wrapped after normalization (#71046)
ade83ed90c,jit,Untopiced,Building Default Inference for Device Type (#69049)
dabcbb2726,Uncategorized,Untopiced,Testing for Default Inference for Device Type (#69052)
4a8aa971cc,jit,Untopiced,Building a TensorProperty AbstractBaseClass (#71184)
03c4d2b9e3,jit,Untopiced,Adding support for Ifs in Device Type Analysis (#69050)
18d91a97e4,jit,Untopiced,Adding custom device type change rules (#69051)
ed9804088a,jit,Untopiced,Adding support for loops (#70209)
e4d522a3cf,Uncategorized,Untopiced,More informative messages for None types comparisons (#69802)
2981534f54,nn_frontend,Untopiced,[nn] cross_entropy: no batch dim support (#71055)
eac3decf93,nn_frontend,Untopiced,ModuleList concatenation (#70887)
e7c87e8b44,quantization,Untopiced,[quant] fix dropout in FX graph mode quantization (#71043)
e47771cca0,quantization,Untopiced,[ao] Removing unused allow list arguments from propagate_qconfig and helper (#71104)
d555d3f0d0,mobile,Untopiced,Update generated header to use flatbuffer v1.12; (#71279)
81f693d509,onnx,Untopiced,[ONNX] minor clarifications of docstrings (#69260) (#69549)
5f2b4be3b9,Uncategorized,Untopiced,[jit] Split DynamicType conformance test into smaller pieces. (#71275)
558622642b,python_frontend,Untopiced,Fix `torch.dsplit` docs dim specification (#70557)
675acfc1f4,Uncategorized,Untopiced,Remove unwanted comma (#71193)
356af8f857,Uncategorized,Untopiced,Do not use `ssize_t` in `python_arg_parser.[cpp|h]` (#71250)
9e45c89891,python_frontend,Untopiced,remove skips from determinant tests (#70034)
910c01020e,nn_frontend,Untopiced,add BFloat16 support for AdaptiveMaxPool2d on CPU (#66929)
d068849cc0,jit,Untopiced,- Fixed memory leak in ir_simplifier.cpp (#71285)
3a0c680a14,distributed,Untopiced,"Jiterates exp2, erfc, erfinv and entr and refactors code_template.h to ATen (#71295)"
88012c7daf,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
84b1c9798c,nn_frontend,Untopiced,add BFloat16 support for AvgPool2d on CPU (#66927)
17bb68618f,Uncategorized,Untopiced,Copy: Fix CPU transpose path ignoring neg and conj bits (#69026)
e531646955,nn_frontend,Untopiced,Fix docstring for nn.MultiHeadAttention (#71100)
054b90f0d6,Uncategorized,Untopiced,add channels last support for ChannelShuffle (#50247)
a4196a9abf,Uncategorized,Untopiced,Remove unused `optimizers` variable in test (#70668)
c7d1501e4d,Uncategorized,Untopiced,fractional_maxpool3d: port to structured kernel (#70414)
680d61daab,Uncategorized,Untopiced,[LT] Remove torch::lazy::convertShapes (#71291)
ac7f188c64,releng,Untopiced,Pin numba ot 0.54.1
d793cc1993,releng,Untopiced,"Revert ""Pin numba ot 0.54.1"""
2faccc2f5d,quantization,Untopiced,[quant] Remove some redundant entries in backend_config_dict for TensorRT (#70971)
d74bb42f7a,dataloader_frontend,Untopiced,Add a missing precondition to `DistributedSampler` docstring (#70104)
94ed61eb5c,releng,Untopiced,Pin numba to 0.54.1 (#71327)
bf82d2012e,jit,Untopiced,[PyTorch] Add IValue::toDimVector & mostly replace toIntVector with it (#71247)
fcbc34a5eb,jit,Untopiced,[PyTorch][Static Runtime] Avoid recomputing input size in dict_unpack (#71252)
b7222e15b6,Uncategorized,Untopiced,[fix] max_pool1d: composite compliance (#70900)
a138aad6e6,mobile,Untopiced,[jit][edge] Return a no-op nullptr for UnionType on mobile for backward compatibility. (#71341)
c43e0286a9,Uncategorized,Untopiced,[PyTorch][Lazy] Make hashing null optionals cheap (#71290)
3ed27a96ed,Uncategorized,Untopiced,[BE] Refactor repetitions into TorchVersion._cmp_wrapper` (#71344)
4bf1be898d,jit,Untopiced,"caffe: fix warning: overloaded virtual function ""torch::jit::Function::call"" is only partially overridden in class ""torch::jit::GraphFunction"""
bb49352354,caffe2,Untopiced,caffe2/torch/csrc/jit/frontend/tree_views: workaround nvcc compiler error
08d8f81704,quantization,Untopiced,[quant][fix][fx][graphmode] Fix qconfig setting for fused modules (#71254)
1ecfa1d61a,package,Untopiced,Load zip file in deploy interpreter (#71072)
a383d01774,caffe2,Untopiced,[fbcode][warnings] Suppress warnings in caffe2/c10 (#71356)
ddf97a59ca,releng,Untopiced,Remove the dependency of pytorch nightly. (#71323)
cf47338191,Uncategorized,Untopiced,[Caffe2][warnings] Suppress -Wimplicit-int-float-conversion in TypeSafeSignMath.h for clang (#71369)
17540c5c80,jit,Untopiced,[warnings][Caffe2] Suppress warnings in non-c10 headers (#71370)
f93ffc9ea8,sparse_frontend,Untopiced,Sparse CSR: Handle zero matrix consistently for triangular_solve (#71304)
7b9fff90d2,Uncategorized,Untopiced,empty_generic: Remove redundant device argument (#70612)
d17f340a2e,Uncategorized,Untopiced,The Cacherator (#71350)
f0db15122f,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
fd9e08df5d,dataloader_frontend,Untopiced,Make Demux serializable with lambda function (#71311)
8d0e354191,Uncategorized,Untopiced,fix CAFFE2_BUILD_MAIN_LIB to the correct C10_BUILD_MAIN_LIB (#70848)
ffdc6b4994,Uncategorized,Untopiced,extract //c10/macros to its own package (#70849)
d665097cad,Uncategorized,Untopiced,allow Bazel to build without glog and gflags (#70850)
2bb6a4f437,Uncategorized,Untopiced,Generate aten_interned_strings.h automatically (#69407)
1eb6146d96,releng,Untopiced,Add manual simple retry to ECR login (#71287)
5243986df6,releng,Untopiced,Update `syncbranches` workflow (#71420)
02ac73a973,releng,Untopiced,ci: Add PR trigger for binary builds workflows (#71431)
ea0524dbc3,jit,Untopiced,[FIX LOG] Complete a '\n' in GRAPH_DEBUG (#70421)
efd274bbcb,Uncategorized,Untopiced,"Fix for windows builds with python 3.10 , getting rid of ssize_t (ssize_t is not a C++ defined type) (#71390)"
a986154950,Uncategorized,Untopiced,Lazy import `packaging` in `torch_version` (#71345)
b8679ee1fc,fx,Untopiced,fix conv+bn folding issue when bn hasn't running states (#71259)
ff8fb717db,releng,Untopiced,Fix `get_git_repo_dir` (#71448)
322f13d914,Uncategorized,Untopiced,[Profiler] Fix memory profile type from recent refactor (#71417)
4fd1992a60,Uncategorized,Untopiced,[Docs][BE] DDP doc fix (#71363)
6964aa2ced,onnx,Untopiced,backout D33469839 (#71443)
6f4c491c6b,quantization,Untopiced,empty_cpu: Add functions that don't depend on Tensor (#70613)
30739f5329,releng,Untopiced,ci: Change binary trigger to be nightly push (#71447)
d5e9a276ea,jit,Untopiced,Adapt to llvm marking SmallVector::set_size private (#71434)
87215ed526,Uncategorized,Untopiced,empty_strided: Factor out generic implementation (#70614)
b4a75af758,fx,Untopiced,[fx2trt] Export some options out (#71315)
125bdb6d51,Uncategorized,Untopiced,empty_meta: Add functions that don't depend on Tensor (#70615)
9b9b878c89,Uncategorized,Untopiced,Fixes jiterator cache macro include + updates CUDA note with cache variables (#71452)
677fab6d1d,Uncategorized,Untopiced,Support broadcast_to on sparse COO tensors (#71073)
9515213070,Uncategorized,Untopiced,[Operator Versioning] Remove version compare as they are decoupled now (#71461)
fbc3b8c1bb,distributed,Untopiced,[RPC] Fix a few flaky RPC tsan tests (#71460)
b56ba296b1,distributed,Untopiced,Support multiple input dims for sharded linear. (#70266)
7ed2a43d26,releng,Untopiced,Adding wheels with py3.10 (#71419)
671a0b5376,releng,Untopiced,Move sccache compilation log to its own group (#71444)
8a9243996c,dataloader_frontend,Untopiced,Lazy load `pandas` when importing pytorch (#71316)
a0ada2d22b,Uncategorized,Untopiced,"Back out ""[pytorch][PR] Performance and memory improvements to batched torch.linalg.solve"" (#71421)"
908fd3d78b,Uncategorized,Untopiced,[fix] composite compliance: quantile and nanquantile (#70894)
75aaa9f92b,Uncategorized,Untopiced,Remove simd qualifier for pragma omp loop in upsample_nearest_op.h (#71462)
3b589c3497,distributed,Untopiced,[DDP Checkpointing] non-reentrant checkpoint tests (#69060)
214f4bf2ff,sparse_frontend,Untopiced,Support sparse.sum on empty sparse tensor (#71091)
ac26f8237c,jit,Untopiced,Allow disabling nvfuser without CUDA (#71358)
15e7d18124,Uncategorized,Untopiced,[jit][edge] Create convinience wrapper for dynamic type construcytors. (#71457)
7ce6db48e5,releng,Untopiced,add rocm GHA workflow (#68552)
bdeec0c7b6,fx,Untopiced,[fx] add documentation to AccOpProperties (#71450)
661d10aab4,Uncategorized,Untopiced,use c10/macros/cmake_macros.h in fbcode build (#70851)
78e1f9db34,Uncategorized,Untopiced,port //c10/macros to common build structure (#70852)
805b7575db,releng,Untopiced,test //c10/... without Google libraries in OSS (#70853)
f45e217c01,Uncategorized,Untopiced,Consolidate the overloads of TensorImpl::shallow_copy_and_detach (#68953)
61713acb07,releng,Untopiced,Add trymerge workflow (#71488)
2dbbb1a921,fx,Untopiced,[fx2trt] Issue warnings instead of error if there's possible const folding opportunities (#71031)
70c9146c40,jit,Untopiced,[nnc] Update block and thread extents in cuda_codegen to use int64_t (#71428)
ef4bc3fa2f,distributed,Untopiced,[distributed] Make rref_proxy._invoke_rpc trully async when needed. (#70206)
06bc6748a1,fx,Untopiced,[acc_ops] Remove usage of kwarg expansion via **locals() for jit scripting support (#71425)
dc5cda0cca,dataloader_frontend,Untopiced,Update min python version to 3.7 in setup.py and mypy configs (#71494)
811af25963,Uncategorized,Untopiced,Fix trivial typo at the doc of `torch.lobpcg` (#71464)
958f9cf5ff,jit,Untopiced,[PyTorch][Static Runtime] Fix extra refcount bumps in layer_norm (#71237)
565f78f571,Uncategorized,Untopiced,[Pytorch] Speed up LayerNorm 4-5% (#71423)
b98e955b24,mobile,Untopiced,[flatbuffer] Fix forward flatbuffer type handling with dynamic type. (#71500)
c59942ac73,Uncategorized,Untopiced,[PyTorch] Fix a bunch of structured kernel refcounting (#71140)
a5d5b11252,releng,Untopiced,Add GitHub merge rules (#71514)
f5b19ba683,distributed,Untopiced,Additional unit test for sharded linear. (#70476)
80b19c4c8c,jit,Untopiced,Enable Python bindings for UntypedStorage (#68945)
99df96d800,fx,Untopiced,Add silu and hardsigmoid converter (#71453)
21b697b646,Uncategorized,Untopiced,add flatbuffer_loader and flatbuffer_serializer as BUCK target (#71463)
06838ce8b1,onnx,Untopiced,fix: do_constant_folding arg when exporting ONNX (#71348)
9f0c808593,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
011fd1d933,dataloader_frontend,Untopiced,[DataPipe] improving DataPipe unit tests (#70215)
a9f44b22c0,linalg_frontend,Untopiced,"Fix composite compliance problems for linalg.{matrix_power, inv, cholesky} (#69437)"
42f7afc4cd,releng,Untopiced,[BE] Improve gitutils Inherit `PeekableIterator` from `collections.abc.Iterator`
17d2a5167e,Uncategorized,Untopiced,Refactor convolution_backward's CudaDepthwise2d case (#71489)
06f14c2d63,Uncategorized,Untopiced,Refactor convolution_backward's CudaDepthwise3d case (#71490)
640bfa7e6f,Uncategorized,Untopiced,Refactor convolution_backward's cudnn cases (#71491)
89c844db9b,Uncategorized,Untopiced,[torch.distributions] Implement positive-semidefinite constraint (#71375)
4868907cf3,Uncategorized,Untopiced,[binaries] fix dump_operator_name binary (#71246)
c7c767726b,releng,Untopiced,Move syncbranches and trymerge to 3.9
11d8fe59fd,releng,Untopiced,"Revert ""Move syncbranches and trymerge to 3.9"""
1c61d8c43f,distributed,Untopiced,[PT1.11] make static graph to be stable (#71459)
e926360cb8,mobile,Untopiced,[Pytorch Edge] Refactor Compatibility Stuff into own directory (#71432)
4b3cf1eaf7,distributed,Untopiced,[BE]Clarify how to check memory saving if using gradient_as_bucket_view (#71483)
53b3904115,distributed,Untopiced,Fix memory leak in ShardedTensor. (#71445)
add774ddbd,releng,Untopiced,.github: Set rocm workflows to only run on PRs (#71567)
10da8726ef,cuda,Untopiced,Fix unused variable warning in adagrad_fused_op_gpu.cu (#71556)
deb1c2f837,fx,Untopiced,Include act_rewriter_allow_list and leaf_module in lower (#71289)
9f1ad2d3e4,cuda,Untopiced,Fix unused variable warning in lp_pool_op.cu (#71557)
8b5775a30f,cuda,Untopiced,Fix unused variable warning in Sorting.cu (#71555)
67385918ab,Uncategorized,Untopiced,move header inclusion (#71307)
e43769e8ab,releng,Untopiced,remove ciflow_should_run (#70321)
08b389fc36,releng,Untopiced,.github: Set DRY_RUN for refs/heads/nightly (#71570)
bc608b6e16,releng,Untopiced,Add gitutils tests (#71580)
2eb4b05b94,Uncategorized,Untopiced,torch/monitor: make tests more robust on windows (#71581)
db2fc82a54,Uncategorized,Untopiced,Generalize IValue's aliased hash handling for opaque tensors (#70371)
dc0a8a6587,Uncategorized,Untopiced,Improve storage assertion of Tensor's enforce_invariants (#70380)
0df607ce00,releng,Untopiced,Separate title and body of commit by 2 lines (#71598)
c92ff47afd,Uncategorized,Untopiced,Use == operator to test type equivalance in pytorch_jni_common.cpp (#71508)
64d221ffbf,releng,Untopiced,Add onnx.rst to the list of mergeable files
9adee84a3f,releng,Untopiced,.github: Improve syncbranch debugability (#71596)
114c13d020,onnx,Untopiced,[ONNX] Minor doc update (#69501)
64a3827d4e,quantization,Untopiced,<Qunat> remove inplace hardtanh in test (#71519)
e2dc2aca93,onnx,Untopiced,Export ONNX models with readable input/output names (#68976)
76fd3cfd38,Uncategorized,Untopiced,fix python version error (#71021)
9f0227a0eb,onnx,Untopiced,"Revert ""[ONNX] Minor doc update (#69501)"" (#71615)"
7ee0712642,Uncategorized,Untopiced,"Fix torch.{unique, unique_consecutive} out of bound (#71540)"
abe361754e,Uncategorized,Untopiced,[fix] isin : non-contiguous input on cpu (#70659)
26c123efbd,Uncategorized,Untopiced,empty_cuda: Add functions that don't depend on Tensor (#70616)
3a963d5621,fx,Untopiced,[fx2trt][torchbench] enable shufflenet lowering (#71562)
c5fe70021c,releng,Untopiced,Fix version strings in CI (#71564)
dea61e7e6c,python_frontend,Untopiced,[Docs] Fixed missing format common args (#70439)
401e755354,Uncategorized,Untopiced,Fix hsplit vsplit dsplit crash when section is 0 (#69342)
7680a0ae9d,quantization,Untopiced,Deprecates _aminmax (#71576)
40d1f77384,cpp_frontend,Untopiced,Codegen: python_torch_functions only include relevant operators (#68693)
36b4c95e74,dataloader_frontend,Untopiced,[DataPipe] adding serialization test for all core IterDataPipes (#71456)
13ea2cb330,dataloader_frontend,Untopiced,[DataPipe] Make GroupBy serializable with lambda function (#71497)
1a917e637c,Uncategorized,Untopiced,Bump dlpack.h to latest version (#65047)
9ada2b0768,releng,Untopiced,add dumb retry to installing miniconda (#71558)
8d880b06a1,fx,Untopiced,stochastic_depth support (#71536)
3a77fb244b,jit,Untopiced,[PyTorch][Static Runtime] Delete cleanup_activations option (#71501)
b40dbdc49f,releng,Untopiced,Fix test ownership lint (#71554)
8b3f58d311,python_frontend,Untopiced,Labels more elementwise binary operators correctly as BinaryUfuncInfos (#71622)
e2191e7084,skip,Untopiced,Fix persistent worker exits before pin_memory thread (#71579)
4e031419aa,linalg_frontend,Untopiced,Skip broken svd tests (#71646)
29a7cb41d8,distributed,Untopiced,[BE] Fix FSDP flaky test (#71525)
d8abe813bc,distributed,Untopiced,"[LocalSGD] Move feature to Beta, clean up some docs (#71621)"
7dd6ead0ac,releng,Untopiced,Update actions/stale to latest version
b09d6224e2,Uncategorized,Untopiced,"Register{Schema,BackendSelect}.cpp: cleanup header includes (#70021)"
71a41323bb,Uncategorized,Untopiced,BackendSelect: Use at::_ops API and per-operator headers (#69840)
0a2cdd18f3,nn_frontend,Untopiced,nice error msg from load_state_dict for non-tensor value (#70596)
84fe4279db,Uncategorized,Untopiced,Structured Kernels: Use at::detail::empty functions (#70617)
26d54b4076,Uncategorized,Untopiced,monitor: add docstrings to pybind interface (#71481)
ae285d837e,caffe2,Untopiced,[1/n][caffe2] Add session based margin loss function in caffe2 operator
91b43b7820,releng,Untopiced,Add clean workspace step to clang-tidy workflow (#71655)
ce3215db70,quantization,Untopiced,Fix nnq.dropout in vision mobilenetv3 pretrain model (#71438)
86aefdc082,dataloader_frontend,Untopiced,Revert D33694867: Fix persistent worker exits before pin_memory thread
47cf0dbf8b,Uncategorized,Untopiced,Prefer at::detail::empty_cuda to the native function (#70618)
c9bd1c60ed,mobile,Untopiced,Move upgraders from python to cpp (#70593)
dcc1e1cd87,releng,Untopiced,"[BE] Use `!{{ common.checkout_pytorch(""recursive"") }}` in binary builds workflows"
9950f3b7e6,releng,Untopiced,[BE][GHA] Further refactor `checkout_pytorch`
4f498f11cd,cuda,Untopiced,Fix unused variable warning in DistanceKernel.cu (#71586)
1794fdd154,cuda,Untopiced,Fix unused variable warning in MultiLabelMarginCriterion.cu (#71594)
70532e32d9,cuda,Untopiced,Fix unused variable warning in EmbeddingBag.cu (#71589)
8d5d875ac7,cuda,Untopiced,Fix unused variable warning in ConvolutionMM2d.cu (#71593)
c7c864bbd1,cuda,Untopiced,Fix unused variable warning in AveragePool2d (#71585)
f269f990f2,Uncategorized,Untopiced,[jiterator] polygamma (#71162)
b9a7dd79f9,cuda,Untopiced,Fix unused variable warning in DepthwiseConv2d.cu (#71584)
6edb06daa6,cuda,Untopiced,Fix unused variable warning in DilatedMaxPool3d.cu (#71590)
6ed46b08ab,cuda,Untopiced,Fix unused variable warning in LossCTC.cu (#71588)
27308642a0,cuda,Untopiced,Fix unused variable warning in layer_norm_kernel.cu (#71587)
3e55fa6385,cuda,Untopiced,Fix unused variable warning in FractionalMaxPool3d (#71591)
ba08440e88,distributed,Untopiced,[Opt Overlap] Remove redundant tests (#71600)
9b3a56eecf,distributed,Untopiced,[Optimizer Overlap] Move hooks to own file (#71601)
3a03af2f50,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
e0d829a266,Uncategorized,Untopiced,Kill the test_torch.py mixin and creates test_scatter_gather_ops (#71691)
c3570fd945,quantization,Untopiced,fx quant: preserve node stack trace throughout prepare and convert (#70757)
8fe82b855e,quantization,Untopiced,dbr quant: do not crash on unsupported qconfig_dict keys if they are empty (#71233)
e12cc227a2,quantization,Untopiced,dbr quant: make QTensorInfo a dataclass and add orig_dtype (#71245)
41afeea791,quantization,Untopiced,dbr quant: split observer insertion to a separate pass (#71253)
99d9883a22,quantization,Untopiced,dbr quant: make SeenOpInfo a dataclass (#71267)
b372be4211,nn_frontend,Untopiced,[nn] lstm : no batch dim support (#71056)
edcd4a20ea,releng,Untopiced,Exit once there's an environment error (#71693)
f75e92a936,Uncategorized,Untopiced,Fix for retracing documentation which would break for n-ary operators (#71599)
8ba1ee6aa7,Uncategorized,Untopiced,[tensorexpr][easy] add missing comma to test_jit_fuser_te.py (#71642)
cda6f40151,skip,Untopiced,Disable XLA config
7bc220e060,distributed,Untopiced,Update distributed.rst for ProcessGroup Extensions (#71482)
506d41d659,Uncategorized,Untopiced,Improve disable name match (#71499)
09f7b42f5c,releng,Untopiced,Add @suo to the list of CI GHF approvers (#71737)
9d47652bee,skip,Untopiced,Fix lint
35e7ac3fa1,Uncategorized,Untopiced,Fix bug in singleCheckErrors (#71706)
b82c4a890d,Uncategorized,Untopiced,Fix aten's native's folder docs. (#71395)
70f3078dd6,jit,Untopiced,[Pytorch Edge] Wrap lowered module in to_backend (#71597)
1295d2699f,Uncategorized,Untopiced,don't include Loops.cuh from Reduce.cuh (#71730)
1cc3291716,Uncategorized,Untopiced,Fix custom function when non tensor argument precedes tensor argument (#71530)
09aeadf4ab,Uncategorized,Untopiced,Fix custom function forward AD internal assert (#71531)
7a0c97195f,Uncategorized,Untopiced,Add save_for_forward to custom function (#71569)
e33cd8f382,Uncategorized,Untopiced,Drop unused variables (#71685)
1df4eca6d7,Uncategorized,Untopiced,[Operator Versioning][Test] Automate model generating process (#70629)
133c213415,nn_frontend,Untopiced,updated the docs for BatchNorm1d and InstanceNorm1d (#71371)
16a9ffba4b,dataloader_frontend,Untopiced,Allow specifying num_samples to RandomSampler even when replacement=False (#71568)
bb157dd4eb,dataloader_frontend,Untopiced,Make methods of internal file_obj visible from StreamWrapper (#71653)
0fdb90da5e,quantization,Untopiced,[warning] Fix TORCH_INTERNAL_ASSERT calls missing condition to check 1/x (#71711)
d32b7d9585,Uncategorized,Untopiced,Logic to auto-categorize commits (#64929)
211deb0364,Uncategorized,Untopiced,Fix CI quick-checks (#71773)
6848e0dae5,nn_frontend,Untopiced,Fix RNN modules with inputs shapes containing-0 in CUDA (#71696)
281663955f,distributed,Untopiced,[Opt Overlap] Create Optimizer Hook State directly from functional optim (#71602)
541817628b,distributed,Untopiced,[Easy] Add comment explaining DistributedOptimizer gating (#71603)
f5a71ec2d6,distributed,Untopiced,[Opt Overlap] Implement as_functional_optim and create_functional_optim (#71604)
9a2b43085d,Uncategorized,Untopiced,Improve docs for `from_dlpack` and `to_dlpack` (#70437)
f866e8b5aa,fx,Untopiced,[fx2trt] Add trt splitter setting (#71717)
456a4dc6bb,jit,Untopiced,[warning] Fix TORCH_INTERNAL_ASSERT calls missing condition to check 2/x (#71767)
22a77d7b92,jit,Untopiced,[warning] Disable broken assert (#71778)
07ca1fc88b,Uncategorized,Untopiced,remove hasPrimaryContext workaround on ROCm (#71146)
f3e81f3eed,jit,Untopiced,Remove copies in jit_log.cpp (#67841)
33403f4848,Uncategorized,Untopiced,edge_order check in torch.gradient only applies to dim argument (#67926)
12e01f7825,cpp_frontend,Untopiced,`linalg.matrix_rank`: fix cpp interface + add more overloads (#70575)
332d67b065,Uncategorized,Untopiced,Add hascuSOLVER flag to Context (#69825)
03f1f0cfe4,Uncategorized,Untopiced,Check the availability of MAGMA / cuSOLVER when setting the Linalg backend. (#69826)
de8d0203e9,Uncategorized,Untopiced,Allow torch.Tensor.real on real-valued tensors (#71718)
dba42056d8,Uncategorized,Untopiced,Release GIL in Tensor indexing functions (#71728)
f3ebf06e98,Uncategorized,Untopiced,Release GIL when assigning to real or imag components (#71747)
ce6e6812b1,cuda,Untopiced,use legacy unrolled kernel for non-trivial offset calc cases (#71710)
5dd6cd55ba,skip,Untopiced,Set correct device id on efficientzerotensors (#71611)
e4500306c8,quantization,Untopiced,[Quant] Enable default reference path for CopyNodeQuantizeHandler (#71168)
24f577dcb2,skip,Untopiced,Disable some forward mode AD tests (#71791)
358b5078ec,mobile,Untopiced,udpate missing ops message (#71294)
965b9f483e,Uncategorized,Untopiced,[cuDNN] Add a new optimized cuDNN RNN algorithm for small RNN hidden_size (#62143)
166d4e4201,nn_frontend,Untopiced,Change `test_conv_large` parameter initialization (#71521)
dfcbe059ec,nn_frontend,Untopiced,Obliviate ALL_TENSORTYPES and ALL_TENSORTYPES2. (#71153)
b36b11cbc1,dataloader_frontend,Untopiced,Separating CaptureDataFrame out of DFIterDataPipe (#71776)
66939e3b94,fx,Untopiced,[acc_tracer] Add test coverage for retracing (#71752)
88c298c28f,jit,Untopiced,Fix symbolic shape function for `flatten` (silvasean's) (#71762)
530e7f6195,Uncategorized,Untopiced,Define check_sizes_nonnegative as inline (#71640)
1dbcde2ade,jit,Untopiced,[TensorExpr] Support scalar intermediate and output values. (#71186)
bd6ec4efb4,jit,Untopiced,"[TensorExpr] Add lowerings for scalar binary ops (+,-,*,/,&,|,^,<<,>>,cmp). (#71298)"
8273912a8c,distributed,Untopiced,[Opt Overlap] Implement _OverlappedOptimizer (#71605)
10ca760c0a,distributed,Untopiced,[Opt Overlap] Implement register_fused_optim in DDP (#71606)
d3354602fc,distributed,Untopiced,[Easy] DDP typo fix (#71607)
e5794974cb,fx,Untopiced,[acc_tracer] Do not rewrite the leaf modules (#71790)
fe277b8717,jit,Untopiced,[jit][edge] Migrate to TypeFactory for jit types on mobile (#71516)
108b37db84,linalg_frontend,Untopiced,[Array API] Add linalg.diagonal (#70599)
40e88b75c4,Uncategorized,Untopiced,extract out //c10/util:base library (#70854)
942a084c46,Uncategorized,Untopiced,Remove state_dict from AveragedModel and use buffers instead (#71763)
f37d2046f8,distributed,Untopiced,Implements allreduce_coalesced for ProcessGroupNCCL (#62140)
bfc481cf67,Uncategorized,Untopiced,extract //c10/core:ScalarType to its own library (#70855)
130ca58601,Uncategorized,Untopiced,extract final two libraries out of //c10/util (#70856)
c6d885e489,Uncategorized,Untopiced,extract out //c10/core:base library (#70857)
25e84fa4e5,autograd_frontend,Untopiced,Add forward AD formulas for some losses (#71026)
adcf34f65a,python_frontend,Untopiced,Revert D33778917: Disable some forward mode AD tests
0891c908bb,Uncategorized,Untopiced,Revert D33768645: Set correct device id on efficientzerotensors
e04ade92ae,Uncategorized,Untopiced,Skip compiledWithCuDNN() call for mobile to avoid segfault (#71775)
7e6312a5df,jit,Untopiced,[SR] Reverse iteration order in resetMemory (#71705)
d3bbb281f3,sparse_frontend,Untopiced,[numpy] add decimals argument to round (#66195)
46817895bd,Uncategorized,Untopiced,[Profiler] Split observer implementations based on ProfilerState (#71135)
c44d0ac181,releng,Untopiced,Implement labelling for release notes and topics check (#71726)
1c8fcc44cb,distributed,Untopiced,[Opt Overlap] Support optimizing partial set of parameters (#71608)
bdcdf94bdd,distributed,Untopiced,[Opt Overlap] Clean up code in _OptimizerHookState (#71620)
d73dc9b7d1,releng,Untopiced,[GHF] Small cleanups
d7e5870b9e,releng,Untopiced,Fixes pr-labels workflow trigger (#71871)
666ff0ae22,distributed,Untopiced,Update _create_c10d_store to check port value (#71863)
9f4bdf7811,mobile,Untopiced,Refactor flatbuffer loader to allow overriding how IValues are parsed. (#71661)
5ee629e50d,releng,Untopiced,.github: Enable windows binary builds (#71484)
5bd33247ec,releng,Untopiced,[GHF] Add revert workflow
7beb030e11,releng,Untopiced,".github: Exclude rocm from ciflow/all,ciflow/trunk"
bbe6144b45,releng,Untopiced,"Revert ""Fix lint"""
c224f82ed3,releng,Untopiced,"Revert ""Disable XLA config"""
b066931106,Uncategorized,Untopiced,fixing of usage of rel_tol for test adadelta (#71880)
ef501e8fed,skip,Untopiced,[bc-breaking][quant][be] Refactor fuser_method to include `is_qat` argument (#70009)
804f13289e,onnx,Untopiced,[ONNX] Update opset_version restriction for local function
cf3ef23713,Uncategorized,Untopiced,Propagate full autocast state to CheckpointFunction's forward-inside-backward (#71169)
ebeeee7b2b,caffe2,Untopiced,[warnings][caffe2] Fix -Wstring-conversion warnings (#71874)
0099796978,Uncategorized,Untopiced,[CUDA Pinned Memory] [Retry] Alternative implementation of pinned memory allocator focusing on multi-threaded scalability (#69299)
76a2c22341,distributed,Untopiced,"[c10d] Improve the ""not yet listening"" warning message of `socket` (#71864)"
027c0d7f8e,Uncategorized,Untopiced,fixed compilations on xla tensor print (#71147)
bf69a61293,mobile,Untopiced,(1/2) Make TorchScript Preserve Fully Qualified Class Name for Python Exceptions: backend change
56511f859a,quantization,Untopiced,Revert D33178977: [bc-breaking][quant][be] Refactor fuser_method to include `is_qat` argument
4523a73288,jit,Untopiced,Fix usages of TORCH_CHECK/_INTERNAL_ASSERT without condition (#71879)
fc6a488e9a,Uncategorized,Untopiced,extract out //c10/core:alignment (#70858)
844a4b47df,Uncategorized,Untopiced,extract out //c10/core:alloc_cpu (#70859)
41690d7804,Uncategorized,Untopiced,define //c10/mobile targets (#70861)
de58a27769,Uncategorized,Untopiced,define //c10/core:CPUAllocator target (#70862)
d4d0ab71b3,quantization,Untopiced,use `torch.testing.assert_equal` in `TestCase.assertEqual` (#67796)
7aa4a1f63e,Uncategorized,Untopiced,torch/monitor: TensorboardEventHandler (#71658)
a6d9dd9370,distributed,Untopiced,"[c10d] Use the term ""errno"" instead of ""generic error"" in logs and error messages (#71865)"
b66f1bc80f,quantization,Untopiced,fx quant: make forked subgraph rewriter preserve stack trace (#71858)
21d307cd22,releng,Untopiced,CUDNN changes for cuda 11.5 (#71869)
fdec94504f,Uncategorized,Untopiced,Rename _scatter_reduce to scatter_reduce and make it unstructured (#71787)
a432b9a7c6,releng,Untopiced,Clean repo after checkout
09c417ae65,autograd_frontend,Untopiced,Add new reduce options and autograd support for scatter_reduce (#71788)
a49f2412e4,jit,Untopiced,[SR] Add static runtime scopes to record function (#70944)
2a8b91548e,releng,Untopiced,Add `scripts` to OSS merge rules
84f1685397,linalg_frontend,Untopiced,Rewrite svd and linalg.svd as structured kernels (#69827)
8ff1a8fdca,linalg_frontend,Untopiced,Implement forward AD for linalg.svd and improve svd_backward (#70253)
e020414cb2,skip,Untopiced,Run the pr-label check on PR closed action and validate closed_by (#71917)
0a8b391936,releng,Untopiced,ci: Enable tests for iOS on GHA
9b53d3194c,distributed,Untopiced,Implement gather primitive for ProcessGroupNCCL (#66745)
6feba4bc7e,distributed,Untopiced,Implement scatter primitive for ProcessGroupNCCL (#70029)
5a14eca191,releng,Untopiced,Revert D33820822: [pytorch][PR] Run the pr-label check on PR closed action and validate closed_by
567c2bb8e9,fx,Untopiced,Support printing inplace operators in FX (#71887)
eeda31fa08,nn_frontend,Untopiced,"Added antialias flag to interpolate (CUDA, bilinear and bicubic) (#70930)"
0cae3c0481,Uncategorized,Untopiced,Improved error messages for `max_unpool{}d` operators (#67328)
e755a4f124,jit,Untopiced,Update the operator version check logic when generating models for testing upgraders (#71894)
e27271d05a,skip,Untopiced,Try setting pull_request checkout to head ref (#71734)
8551989bff,distributed,Untopiced,[c10d] Enable gather_object on nccl (#71623)
81d1ce05fd,Uncategorized,Untopiced,"Add complex support for Jiterator, port sinc to Jiterator (#71577)"
75cc2184e1,skip,Untopiced,Automated submodule update: FBGEMM (#65595)
b62780fc4f,Uncategorized,Untopiced,[warnings] Disable broken TORCH_CHECK (#71947)
dcc6aed52c,autograd_frontend,Untopiced,Implement derivatives for torch.remainder and torch.fmod wrt the second argument and update the docs (#69908)
e2011b29aa,autograd_frontend,Untopiced,Add OpInfo test to check that floating point inputs in OpInfos have requires_grad set to True (#69909)
a675770adc,python_frontend,Untopiced,Deactivate the tracking of gradients in sampling functions within OpInfos (#68522)
171cf153d2,Uncategorized,Untopiced,Make repeat_interleave respect the conj and neg bits. (#68523)
6cb128c8dd,Uncategorized,Untopiced,Generalize noncontiguous tests to several outputs (#67996)
1407939f69,linalg_frontend,Untopiced,Remove unnecessary non_contiguous and gradient tests from test_linalg (#68188)
b486797864,jit,Untopiced,[jit][edge] Make flatbuffer_serailzer print correct type strings. (#71935)
c85965600c,jit,Untopiced,Fix bug where frozen mod not used for OFI #68903 (#71436)
3b9f2e2cca,releng,Untopiced,[GHF] More verbose failures messages (#71941)
51ae9ccba4,autograd_frontend,Untopiced,Fix forward AD for cudnn batch norm (#71901)
f115a42362,Uncategorized,Untopiced,Revert D33805315: [pytorch][PR] Automated submodule update: FBGEMM
99a9929254,distributed,Untopiced,[Easy] Format DDP error (#71802)
a30b0cf52a,distributed,Untopiced,[FSDP] Add/refactor unit test for wrap (#71803)
d0ff1f0013,distributed,Untopiced,[FSDP] Backward prefetch in recursive call (#71804)
8548657ddb,Uncategorized,Untopiced,TransformedDistribution.icdf: Fix erroneous icdf ValueError (#71393)
7d613ab1d6,fx,Untopiced,Fix indentation typo in test_fx_experimental.py (#71885)
1aa2257cac,jit,Untopiced,Error message update: use proper name of custom c++ classes (#71922)
7bc5962329,fx,Untopiced,Trace asserts with fx by looking at byte code (#70960)
e849c8b0f2,mobile,Untopiced,Move bytecode generation to python (#71681)
0c2b1b8bcf,Uncategorized,Untopiced,Update docs for forward AD and make them public (#71643)
746e702104,releng,Untopiced,Revert D33827835: Try setting pull_request checkout to head ref
5bd19ba846,python_frontend,Untopiced,Expect test_fn_fwgrad_bwgrad to fail because forward AD is not implemented (#71944)
fb0e27d38a,Uncategorized,Untopiced,Add mechanism for functorch to error out on autograd.Function (#71866)
60997be85c,distributed,Untopiced,Replace LOG by LOG_EVERY_N to avoid log spamming (#71755)
e06eb286da,skip,Untopiced,Fix SVD error code handling for OpenBLAS 0.3.15+ and MKL 2022+ (#68812)
be2dc8f294,linalg_frontend,Untopiced,Sparse CSR CUDA: Add torch.baddbmm and torch.bmm (#68711)
e88c999da3,skip,Untopiced,[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`
9413c0cd3e,linalg_frontend,Untopiced,Revert D32626563: [pytorch][PR] Fix SVD error code handling for OpenBLAS 0.3.15+ and MKL 2022+
fa38e93fe9,Uncategorized,Untopiced,Add lightweight reparametrization for `_stateless` calls (#68969)
5735f2f875,Uncategorized,Untopiced,Make detach redispatch like a regular PyTorch operator (#71707)
de44a50f14,Uncategorized,Untopiced,index_backward: use out-of-place index_put if any input is subclass (#71779)
e58d5b718a,releng,Untopiced,"Remove code for using our own build cudnn image, use nvidia image (#71952)"
f499ab9cef,skip,Untopiced,Implement Tanh Gelu Approximation (#61439)
c5df294940,mobile,Untopiced,Fix bug in upgrader generation in mobile (#71578)
0c3bc426a8,Uncategorized,Untopiced,LTC move squeeze to master (#71677)
cb823d9f07,quantization,Untopiced,Revert D33744717: [pytorch][PR] Implement Tanh Gelu Approximation
765669e1b9,Uncategorized,Untopiced,Update docs for torch.real to indicate that it's supported for real tensors (#71962)
99bc978b78,jit,Untopiced,[JIT] Propagate requires_grad to autodiff subgraphs (#71666)
3d2d466fc0,quantization,Untopiced,[Quant] Fixed errors in test_embedding introduced by https://github.com/pytorch/pytorch/pull/69768 (#71387)
b2b63209e1,fx,Untopiced,make code simplify in get bufffers and parameters (#70399)
09e54ffec3,releng,Untopiced,.github: Ensure we're using correct build matrix (#72010)
63429bf4b3,nn_frontend,Untopiced,Removed JIT FC tweaks for interpolation options (#71937)
bc0e216d1f,jit,Untopiced,[jit][edge] Print correct type strings in code file for mobile models. (#71968)
65d3adc65d,Uncategorized,Untopiced,Add linspace test modules (#71850)
57a9b499dc,Uncategorized,Untopiced,torch/monitor: update pyi definitions (#71950)
4cd7819854,caffe2,Untopiced,[caffe2][torch] Remove unreferenced local variable e (#71856)
726cc39242,jit,Untopiced,Rename inplace variant of freeze_module (#71437)
2821574eea,caffe2,Untopiced,[caffe2] Fix compilation with fmt 8.x (#71966)
3e1e02595a,jit,Untopiced,Avoid unnecessary copy of ExecutionPlan in operator() (#71982)
8fa5cde3a9,distributed,Untopiced,Fix hooks (#71970)
bc9d1e709a,Uncategorized,Untopiced,[EASY] Adding virtual to the isUnionType op (#69554)
2017b404ec,skip,Untopiced,Fix SVD error code handling for OpenBLAS 0.3.15+ and MKL 2022+ (#68812)
d68c314b13,caffe2,Untopiced,[warnings][caffe2] Fix asserts yielding -Wstring-conversion warnings (#72013)
8ca7484ce7,Uncategorized,Untopiced,[FIX] Enable TORCH_CHECK again (#71971)
db370b7a1e,caffe2,Untopiced,[warnings][caffe2] Fix broken asserts (never trigger) (#72014)
87bbcf70f7,skip,Untopiced,Create torch.distributed.shard package. (#71742)
7a69752c27,Uncategorized,Untopiced,Make upgrader test model generation more robust (#72030)
815532d40c,Uncategorized,Untopiced,Unsqueeze ops to reduce the number of reshapes in we use in LTC (#72011)
af65634d1c,Uncategorized,Untopiced,Move generated keyword out of gen_mobile_upgraders.py (#71938)
784bd92340,mobile,Untopiced,Use upgrader_mobile.cpp as the reference for codegen unittest (#71930)
a18cfb790d,skip,Untopiced,Set correct device id on efficientzerotensors (#71611)
6208c2800e,Uncategorized,Untopiced,torch/monitor: merge Interval and FixedCount stats (#72009)
1e4aefaa2f,Uncategorized,Untopiced,Revert D33834916: Set correct device id on efficientzerotensors
72c972e1e1,Uncategorized,Untopiced,Fix bug in linspace model generation (#72027)
ca61292465,nn_frontend,Untopiced,Add append method for nn.Sequential (#71326)
23d03025dc,skip,Untopiced,Implement Tanh Gelu Approximation (#61439)
5045c18bd1,Uncategorized,Untopiced,Error if pocketfft is not found (#67909)
74c44ba9d6,quantization,Untopiced,Revert D33850228: [pytorch][PR] Implement Tanh Gelu Approximation
95d71ed212,skip,Untopiced,Run the pr-label check on PR closed action and validate closed_by (#71917)
bb6b501aa0,linalg_frontend,Untopiced,"Back out ""[pytorch][PR] Fix SVD error code handling for OpenBLAS 0.3.15+ and MKL 2022+"" (#72063)"
34494e6252,distributed,Untopiced,"Back out ""Create torch.distributed.shard package."" (#72062)"
dbd090d610,releng,Untopiced,.github: Change binary build workflow trigger (#71890)
689c218c36,caffe2,Untopiced,[caffe2] Rename c10d::detail::vformat to resolve conflict with fmt (#72039)
6714d039a1,fx,Untopiced,"[bug fix] for add_activation layer, mobilenetv2 is fixed (#71979)"
871e240e63,cuda,Untopiced,Improved error message for interpolation (#72066)
a1b4410964,Uncategorized,Untopiced,Add owners to custom test infra (#72080)
d46256bd7c,releng,Untopiced,[skip ci] Remove unused outdated .circleci bazel_definitions file (#71943)
d693739248,Uncategorized,Untopiced,CMake: Clean up unused definitions (#69216)
847dbb8684,Uncategorized,Untopiced,CMake: Clean up unused definitions (#69216)
082ff25f37,quantization,Untopiced,"[reland][bc-breaking][quant][be] Refactor fuser_method to include `is_qat` argument"" (#71956)"
184b78c4c1,fx,Untopiced,[acc_ops] Move slice_tensor to consider single dim at a time (#5906)
c62b515691,autograd_frontend,Untopiced,Make diag_embed a primitive w.r.t. autograd (#71750)
1f2751cdea,autograd_frontend,Untopiced,"Composite compliance for index_copy, index_fill, masked_scatter, masked_fill (#71751)"
25f5f2cd06,autograd_frontend,Untopiced,Composite compliance for index_put (#71765)
a83cf17807,Uncategorized,Untopiced,Composite compliance for gather_backward (#71766)
cf70466970,onnx,Untopiced,[ONNX] Improve scope inference in function extraction
a319bce58d,releng,Untopiced,Make sure we set GITHUB token in the header for pr-label GHA (#72085)
e784808bc6,releng,Untopiced,DOC: create 1.12 docs from a tag like v1.12.2rc1 (#71985)
bb456d2bf7,Uncategorized,Untopiced,Split cuda: list cpp files that go in _cu library explicitly (#69082)
c93d6f90c9,Uncategorized,Untopiced,"Revert #62143, the new CUDNN_RNN_ALGO_PERSIST_STATIC_SMALL_H algorithm (#72089)"
1cc824ef59,Uncategorized,Untopiced,Fix old GCC ABI check in CMake package config (#72081)
f20fa66f70,Uncategorized,Untopiced,"Revert ""[fix] max_pool1d: composite compliance (#70900)"" (#71992)"
58dabebcd7,Uncategorized,Untopiced,improve quantized error checking for structured kernels (#71928)
44e2b8da28,Uncategorized,Untopiced,Automated submodule update: FBGEMM (#72068)
a5e27c45dc,Uncategorized,Untopiced,Use new_empty in dropout (#72078)
7ea96a7293,quantization,Untopiced,[quant][fx] Don't assume bias is a keyword-argument (#71426)
e8d226cd9a,onnx,Untopiced,Remove some unnecessary python functional wrappers (#61608)
ba8d5f6f75,jit,Untopiced,"[JIT] FuseLinear pass now handles CallFunction(""linear"", ...) (#61646)"
7bb614fc71,Uncategorized,Untopiced,Simplify TensorImpl size check and fix error message (#72070)
4b789df68b,jit,Untopiced,[SR] Add BlockRunner and handle sub-blocks (#69834)
da0423aa0b,Uncategorized,Untopiced,[PyTorch] Use a better hash table in CUDACachingAllocator (#71667)
ca2ff12ea3,Uncategorized,Untopiced,[PyTorch] Remove call_once from CUDACachingAllocator (#71668)
4aade95029,Uncategorized,Untopiced,[PyTorch] Rework stat collection in CUDACachingAllocator (#71669)
1a30954f44,Uncategorized,Untopiced,CUDA TopK Optimization: use multiple block per slice (#71081)
702d375df5,skip,Untopiced,[pytorch] use cublas lt interface for bias fusion (#71200)
2c3ecb435e,Uncategorized,Untopiced,Automated submodule update: FBGEMM (#72116)
5024c1bc7b,dataloader_frontend,Untopiced,Make `get_file_pathnames_from_root` output order deterministic (#70435)
b28e696516,mobile,Untopiced,Update linspace and bump version nuymber to 8 (#71486)
c61be5fb22,fx,Untopiced,Add split_with_sizes converter (#71953)
4567d5ded4,Uncategorized,Untopiced,Upgrade oneDNN to v2.5.2 (#71546)
7e8217549f,nn_frontend,Untopiced,Added remove_duplicate parameter to `nn.Module` (#39)
e305248a33,Uncategorized,Untopiced,Add logspace test modules (#72052)
cf1833df70,jit,Untopiced,[WIP] add explicit dynamic fusion arg (#71173)
f1499d6c18,jit,Untopiced,Refactor PE so fusion specializations are configurable (#71650)
59a6375639,jit,Untopiced,[NNC] Add Tests for Dynamic Shape Fusion Change default fusion strategy (#71651)
27a4d39756,jit,Untopiced,NNC Dynamic Channels last fixes (#72032)
b44f724aef,jit,Untopiced,[nnc] Update cuda codegen to use llvm for thread and block extent computations (#72040)
a55ef69e68,jit,Untopiced,update default fusion strategy (#72038)
e0a0f37a11,jit,Untopiced,Add docs for fusion strategy (#72036)
be7ee92669,releng,Untopiced,Update process_commit.py
e118d6e59f,quantization,Untopiced,Add lowering path for LinearReLU module (#71427)
34e4418dfa,jit,Untopiced,[nnc] tensorexpr for quantized/aten::upsample_nearest2d (#71236)
9e8334e3ae,jit,Untopiced,"[tensorexpr][quant] Enable tensorexpr for quant,dequant (#71243)"
d0f397ae61,Uncategorized,Untopiced,Avoid unnecessary copy of padding/dilation vectors in check_shape_forward (#72019)
